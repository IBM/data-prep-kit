[project]
name = "data_prep_toolkit_spark"
version = "0.2.2.dev1"
keywords = ["data", "data preprocessing", "data preparation", "llm", "generative", "ai", "fine-tuning", "llmapps" ]
requires-python = ">=3.10,<3.13"
description = "Data Preparation Toolkit Library for Spark"
license = {text = "Apache-2.0"}
readme = {file = "README.md", content-type = "text/markdown"}
authors = [
    { name = "David Wood", email = "dawood@us.ibm.com" },
    { name = "Boris Lublinsky", email = "blublinsk@ibm.com" },
]
dependencies = [
    "data-prep-toolkit==0.2.2.dev1",	
    "pyspark>=3.5.2",
    "psutil>=6.0.0",
    "PyYAML>=6.0.2"
]

[project_urls]
Repository = "https://github.com/IBM/data-prep-kit"
Issues = "https://github.com/IBM/data-prep-kit/issues"
Documentation = "https://ibm.github.io/data-prep-kit/"
"Transform project" = "https://github.com/IBM/data-prep-kit/tree/dev/transforms/universal/noop"

[build-system]
requires = ["setuptools>=68.0.0", "wheel", "setuptools_scm[toml]>=7.1.0"]
build-backend = "setuptools.build_meta"

[project.optional-dependencies]
dev = [
    "twine",
    "pytest>=7.3.2",
    "pytest-dotenv>=0.5.2",
    "pytest-env>=1.0.0",
    "pre-commit>=3.3.2",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.10.0",
    "moto==5.0.5",
    "markupsafe==2.0.1",
]

[options]
package_dir = ["src","test"]

[options.packages.find]
where = ["src/data_processing_spark"]

[tool.pytest.ini_options]
# Currently we use low coverage since we have to run tests separately (see makefile)
#addopts = "--cov --cov-report term-missing --cov-fail-under 25"
markers = ["unit: unit tests", "integration: integration tests"]

[tool.coverage.run]
include = ["src/*"]
