# Code Quality 

Please see the set of
[transform project conventions](../../README.md)
for details on general project conventions, transform configuration,
testing and IDE set up.

## Summary
This module captures code specific metrics of input data.  It includs the metrics discussed or highlighted in CodeParrot and Starcoder data preprocessing pipeline. In the current implementation, the module includes the following metrics & reports each metrics in individual column:

* line specific metrics include mean & max line length
* character and token ratio - uses the input tokenizer to tokenize the input data & measure the ratio between the characters and tokens
* identifies the high occurence of the keywords "test " or "config" and tags them as config or test samples
* tags the samples as autogenerated if the sample contains keywords like `auto-generated`, `autogenerated` or `automatically generated`
* programming language specific identification, where:
    * if the input sample is `python` programming language and sample has no reference to constructs like def, class, it is highlighted as `has_no_keywords` 

This module adds the following fields into the output file:
<ul>
       <li>line_mean</li>
       <li>line_max</li>
       <li>total_num_lines</li>
       <li>avg_longest_lines</li>
       <li>alphanum_frac</li>
       <li>char_token_ratio</li>
       <li>autogenerated</li>
       <li>config_or_test</li>
       <li>has_no_keywords</li>
       <li>has_few_assignments</li>
       <li>is_xml</li>
       <li>is_html</li>
</ul>

It uses a tokenizer to collect metrics specific to token ratio.  It is designed to download the tokenizer from the [Huggingface](https://huggingface.co/) if the input tokenizer is not found in the local cache. By default, it uses [codeparrot/codeparrot](https://huggingface.co/codeparrot/codeparrot) tokenizer.

## Running

#### Running as pure python application

<pre>
% make venv
% source venv/bin/activate
(venv) % cd src
(venv) % python code_quality_transform.py
04:36:21 INFO - Running locally
04:36:21 INFO - Using local configuration with: input_folder - /root/codellm/repos/data-prep-lab/transforms/code/code_quality/test-data/input output_folder - /root/codellm/repos/data-prep-lab/transforms/code/code_quality/output
04:36:21 INFO - Not using data sets, checkpointing False, max files -1
04:36:21 INFO - number of workers 1 worker options {'num_cpus': 0.8}
04:36:21 INFO - pipeline id pipeline_id; number workers 1
04:36:21 INFO - job details {'job category': 'preprocessing', 'job name': 'code_quality', 'job type': 'ray', 'job id': 'job_id'}
04:36:21 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}
04:36:21 INFO - actor creation delay 0
2024-04-12 04:36:27,585	INFO worker.py:1715 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8265
(orchestrate pid=5960) 04:36:32 INFO - orchestrator started at 2024-04-12 04:36:32
(orchestrate pid=5960) 04:36:32 INFO - Number of files is 2, source profile {'max_file_size': 0.03258514404296875, 'min_file_size': 0.032202720642089844, 'total_file_size': 0.0647878646850586}
(orchestrate pid=5960) 04:36:32 INFO - Cluster resources: {'cpus': 8, 'gpus': 0, 'memory': 8.38815994374454, 'object_store': 4.1940799709409475}
(orchestrate pid=5960) 04:36:32 INFO - Number of workers - 1 with {'num_cpus': 0.8} each
(orchestrate pid=5960) 04:36:37 INFO - Completed 1 files in 0.08397076924641927 min
(orchestrate pid=5960) 04:36:37 INFO - Completed 1 files in 0.08398436307907105 min. Waiting for completion
(orchestrate pid=5960) 04:36:37 INFO - Completed processing in 0.08446235656738281 min
04:36:47 INFO - Completed execution in 0.4207143386205037 min, execution result 0
(venv) % deactivate
% ls ../output/
metadata.json  sample_1.parquet  sample_2.parquet
%
</pre>

## Launcher Command Line Options 

When running the transform with the Ray launcher (i.e. TransformLauncher),
the following command line arguments are available in addition to 
[the options provided by the launcher](../../../data-processing-lib/doc/launcher-options.md).

* "--contents_column_name" - input a column name which contains data to process. The default column name: `contents`
* "--language_column_name" - input a column name which contains programming language details. The default column name: `language`
* "--tokenizer" - input a tokenizer to convert the data into tokens. The default tokenizer is `codeparrot/codeparrot`
* "--hf_token" - input the Hugging Face auth token to download the tokenizer. This option is only required for the tokenizer's whose access is restricted in Hugging Face.

## Executing S3 examples

To execute S3 examples, please refer to this [document](../../../data-processing-lib/doc/using_s3_transformers.md) 
for setting up MinIO and mc prior to running the example
