{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Data Prep Kit <p>Data Prep Kit is a community project to democratize and accelerate unstructured data preparation for LLM app developers.  With the explosive growth of LLM-enabled use cases, developers are faced with the enormous challenge of preparing use case-specific unstructured data to fine-tune, instruct-tune the LLMs or to build RAG applications for LLMs. As the variety of use cases grow, so does the need to support:</p> <ul> <li>New ways of transforming the data to enhance the performance of the resulting LLMs for each specific use case.</li> <li>A large variety in the scale of data to be processed, from laptop-scale to datacenter-scale</li> <li>Support for different data modalities including language, code, vision, multimodal etc</li> </ul> <p>Data Prep Kit offers implementations of commonly needed data preparation steps, called modules or transforms, for both Code and Language modalities, with vision to extend to images, speech and multimodal data.  The goal is to offer high-level APIs for developers to quickly get started in working with their data, without needing expertise in the underlying runtimes and frameworks.</p> <p></p>"},{"location":"#table-of-contents","title":"\ud83d\udcdd Table of Contents","text":"<ul> <li>About</li> <li>Getting Started</li> <li>Scaling transforms from laptop to cluster</li> <li>Repository Use and Navigation</li> <li>How to Contribute</li> <li>Resources (papers, talks, presentations and tutorials)</li> <li>Citations</li> </ul>"},{"location":"#about","title":"\ud83d\udcd6 About","text":"<p>Data Prep Kit is a toolkit for streamlining data preparation for developers looking to build LLM-enabled applications via fine-tuning, RAG or instruction-tuning. Data Prep Kit contributes a set of modules that the developer can get started with to easily build data pipelines suitable for their use case. These modules have been tested while producing pre-training datasets for the Granite open source LLM models.</p> <p>The modules are built on common frameworks (for Spark and Ray), called the data processing library that allows the developers to build new custom modules that readily scale across a variety of runtimes.</p> <p>Features of the toolkit: </p> <ul> <li>It aims to accelerate unstructured data prep for the \"long tail\" of LLM use cases.</li> <li>It offers a growing set of module implementations across multiple runtimes, targeting laptop-scale to datacenter-scale processing.</li> <li>It provides a growing set of sample data processing pipelines that can be used for real enterprise use cases.</li> <li>It provides the Data processing library to enable contribution of new custom modules targeting new use cases.</li> <li>It uses Kubeflow Pipelines-based workflow automation.</li> </ul> <p>Data modalities supported today: Code and Natural Language.</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"#fastest-way-to-experience-data-prep-kit","title":"Fastest way to experience Data Prep Kit","text":"<p>With no setup necessary, let's use a Google Colab friendly notebook to try Data Prep Kit. This is a simple transform to extract content from PDF files: examples/notebooks/Run_your_first_transform_colab.ipynb  | . (Here are some tips for running Data Prep Kit transforms on Google Colab. For this simple example, these tips are either already taken care of, or are not needed.)</p>"},{"location":"#create-a-virtual-environment","title":"Create a Virtual Environment","text":"<p>To run on a local machine, follow these steps to quickly set up and deploy the Data Prep Kit in your virtual Python environment.</p> <pre><code>conda create -n data-prep-kit -y python=3.11\nconda activate data-prep-kit\npython --version\n</code></pre> <p>Check if the python version is 3.11. </p> <p>If you are using a linux system, install gcc using the below commands:</p> <pre><code>conda install gcc_linux-64\nconda install gxx_linux-64\n</code></pre> <p>Next, install the data prep toolkit library. This library installs both the python and ray versions of the transforms. For better management of dependencies, it is recommended to install the same tagged version of both the library and the transform. </p> <pre><code>pip3 install  'data-prep-toolkit[ray]==0.2.2.dev1'\npip3 install  'data-prep-toolkit-transforms[ray,all]==0.2.2.dev1'\npip3 install jupyterlab   ipykernel  ipywidgets\n\n## install custom kernel\npython -m ipykernel install --user --name=data-prep-kit --display-name \"dataprepkit\"\n</code></pre> <p>Test, your installation. If you are able to import these data-prep-kit libraries successfully in python, your installation has succeeded. </p> <pre><code>## start python interpreter\n$   python\n\n# import DPK libraries\n&gt;&gt;&gt; from data_processing_ray.runtime.ray import RayTransformLauncher\n&gt;&gt;&gt; from data_processing.runtime.pure_python import PythonTransformLauncher\n</code></pre> <p>If there are no errors, you are good to go!</p>"},{"location":"#run-your-first-transform-locally","title":"Run your first transform locally","text":"<p>Let's try the same simple transform to extract content from PDF files on a local machine. </p> <p>Local Notebook versions</p> <p>You can try either one or both of the following two versions: </p> <ul> <li>Option 1: Pure python notebook: examples/notebooks/Run_your_first_transform_python.ipynb - easiest to get started</li> <li>Option 2: Ray version: This one uses Ray framework for parallel execution while still allowing local processing - examples/notebooks/Run_your_first_transform_ray.ipynb</li> </ul> <p>To run the notebooks, launch jupyter from the same virtual environment you created using the command below. </p> <p><code>jupyter lab</code></p> <p>After opening the jupyter notebook, change the kernel to <code>dataprepkit</code>, so all libraries will be properly loaded.</p> <p>Explore more examples here.</p>"},{"location":"#run-your-first-data-prep-pipeline","title":"Run your first data prep pipeline","text":"<p>Now that you have run a single transform, the next step is to explore how to put these transforms together to run a data prep pipeline for an end to end use case like fine tuning model or building a RAG application. This notebook gives an example of how to build an end to end data prep pipeline for fine tuning for code LLMs. You can also explore how to build a RAG pipeline here.</p>"},{"location":"#current-list-of-transforms","title":"Current list of transforms","text":"<p>The matrix below shows the the combination of modules and supported runtimes. All the modules can be accessed here and can be combined to form data processing pipelines, as shown in the examples folder. </p> Modules Python-only Ray Spark KFP on Ray Data Ingestion Code (from zip) to Parquet PDF to Parquet HTML to Parquet Universal (Code &amp; Language) Exact dedup filter Fuzzy dedup filter Unique ID annotation Filter on annotations Profiler Resize HAP Tokenizer Language-only Language identification Document quality Document chunking for RAG Text encoder PII Annotator/Redactor Code-only Programming language annotation Code quality annotation Malware annotation Header cleanser Semantic file ordering License Select Annotation <p>Contributors are welcome to add new modules to expand to other data modalities as well as add runtime support for existing modules!</p>"},{"location":"#add-your-own-transform","title":"Add your own transform","text":"<p>At the core of the framework, is a data processing library, that provides a systematic way to implement the data processing modules. The library is python-based and enables the application of \"transforms\" to a one or more input data files to produce one or more output data files. We use the popular parquet format to store the data (code or language).  Every parquet file follows a set schema. A user can use one or more transforms (or modules) as discussed above to process their data.  A transform can follow one of the two patterns: annotator or filter.</p> <ul> <li> <p>Annotator An annotator transform adds information during the processing by adding one more columns to the parquet files. The annotator design also allows a user to verify the results of the processing before the actual filtering of the data.</p> </li> <li> <p>Filter A filter transform processes the data and outputs the transformed data, e.g., exact deduplication. A general purpose SQL-based filter transform enables a powerful mechanism for identifying columns and rows of interest for downstream processing.</p> </li> </ul> <p>For a new module to be added, a user can pick the right design based on the processing to be applied. More details here.</p> <p>One can leverage Python-based processing logic and the Data Processing Library to easily build and contribute new transforms. We have provided an example transform that can serve as a template to add new simple transforms. Follow the step by step tutorial to help you add your own new transform. </p> <p>For a deeper understanding of the library's architecture, its transforms, and available runtimes, we encourage the reader to consult the comprehensive overview document alongside dedicated sections on transforms and runtimes.</p> <p>Additionally, check out our video tutorial for a visual, example-driven guide on adding custom modules.</p>"},{"location":"#-from-laptop-to-cluster","title":"\ud83d\udcbb -&gt; \ud83d\udda5\ufe0f\u2601\ufe0f From laptop to cluster","text":"<p>Data-prep-kit provides the flexibility to transition your projects from proof-of-concept (PoC) stage to full-scale production mode, offering all the necessary tools to run your data transformations at high volume. In this section, we enable you how to run your transforms at scale and how to automate them. </p>"},{"location":"#scaling-of-transforms","title":"Scaling of Transforms","text":"<p>To enable processing of large data volumes leveraging multi-mode clusters, Ray  or Spark wrappers are provided, to readily scale out the Python implementations.</p> <p>A generalized workflow is shown here.</p>"},{"location":"#automation","title":"Automation","text":"<p>The toolkit also supports transform execution automation based on  Kubeflow pipelines (KFP), tested on a locally deployed Kind cluster and external OpenShift clusters. There is an  automation to create a Kind cluster and deploy all required components on it. The KFP implementation is based on the KubeRay Operator for creating and managing the Ray cluster and KubeRay API server to interact with the KubeRay operator. An additional framework along with several kfp components is used to simplify the pipeline implementation.</p> <p>A simple transform pipeline tutorial explains the pipeline creation and execution.  In addition, if you want to combine several transformers in a single pipeline, you can look at multi-steps pipeline </p> <p>When you finish working with the cluster, and want to clean up or destroy it. See the  clean up the cluster</p>"},{"location":"#run-your-first-transform-using-command-line-options","title":"Run your first transform using command line options","text":"<p>You can run transforms via docker image or using virtual environments. This document shows how to run a transform using virtual environment. You can follow this document to run using docker image. </p>"},{"location":"#citations","title":"Citations","text":"<p>If you use Data Prep Kit in your research, please cite our paper:</p> <pre><code>@misc{wood2024dataprepkitgettingdataready,\n      title={Data-Prep-Kit: getting your data ready for LLM application development}, \n      author={David Wood and Boris Lublinsky and Alexy Roytman and Shivdeep Singh \n      and Abdulhamid Adebayo and Revital Eres and Mohammad Nassar and Hima Patel \n      and Yousaf Shah and Constantin Adam and Petros Zerfos and Nirmit Desai \n      and Daiki Tsuzuku and Takuya Goto and Michele Dolfi and Saptha Surendran \n      and Paramesvaran Selvam and Sungeun An and Yuan Chi Chang and Dhiraj Joshi \n      and Hajar Emami-Gohari and Xuan-Hong Dang and Yan Koyfman and Shahrokh Daijavad},\n      year={2024},\n      eprint={2409.18164},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2409.18164}, \n}\n</code></pre>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"CONTRIBUTING","text":""},{"location":"CONTRIBUTING/#contributing-in-general","title":"Contributing In General","text":"<p>Our project welcomes external contributions. If you have an itch, please feel free to scratch it.</p> <p>To contribute code or documentation, please submit a pull request. You can get started with open issues with the label - good first issue.  Before embarking on a more ambitious contribution, please quickly get in touch with us via raising an issue.</p> <p>Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all!</p>"},{"location":"CONTRIBUTING/#proposing-new-features","title":"Proposing new features","text":"<p>If you would like to implement a new feature, please raise an issue,  before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base.</p>"},{"location":"CONTRIBUTING/#fixing-bugs","title":"Fixing bugs","text":"<p>If you would like to fix a bug, please raise an issue, before sending a pull request so it can be tracked.</p>"},{"location":"CONTRIBUTING/#merge-approval","title":"Merge approval","text":"<p>The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected.</p> <p>For a list of the maintainers, see the MAINTAINERS.md page.</p>"},{"location":"CONTRIBUTING/#legal","title":"Legal","text":"<p>Each source file must include a license header for the Apache Software License 2.0. Using the SPDX format is the simplest approach. e.g.</p> <pre><code>/*\nCopyright &lt;holder&gt; All Rights Reserved.\n\nSPDX-License-Identifier: Apache-2.0\n*/\n</code></pre> <p>We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions.</p> <p>We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message.</p> <p>Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>To include Signed-off-by message automatically, set the git config as, <pre><code>git config user.name \"FirstName LastName\"\ngit config user.email \"YourEmail@example.com\"\n</code></pre></p> <p>and include flag <code>-s | --sign-off</code> when you commit a change to your local git repository, for example</p> <pre><code>git commit -s -m \"your commit message\"\n</code></pre>"},{"location":"CONTRIBUTING/#transform-setup-and-testing","title":"Transform Setup and Testing","text":"<p>Please note the many useful options of the make command, as shown by using <code>make help</code>, that will take care of manual steps that would have been needed for tasks such as building, publishing, setting up or testing transforms in most directories.</p>"},{"location":"CONTRIBUTING/#coding-style-guidelines","title":"Coding style guidelines","text":"<p>Coding style as enforced by <code>pre-commit</code>.</p>"},{"location":"MAINTAINERS/","title":"MAINTAINERS","text":"<p>David Wood - dawood@us.ibm.com</p> <p>Boris Lublinsky - blublinsky@ibm.com</p> <p>Revital Eres - eres@il.ibm.com</p>"},{"location":"RELEASE/","title":"Release Management","text":""},{"location":"RELEASE/#overview","title":"Overview","text":"<p>Releases are created from the main repository branch using the version numbers, including an intermediate version suffix,  defined in <code>.make.versions</code>. The following points are important:</p> <ol> <li>In general, a common version number is used for all published pypi wheels and docker images.</li> <li><code>.make.versions</code> contains the version to be used when publishing the next release. </li> <li>Whenever <code>.make.versions</code> is changed, <code>make set-versions</code> should be run from the top of the repo.</li> <li>Corollary: <code>make set-versions</code> should ONLY be used from the top of the repo when <code>.make.versions</code> changes.</li> <li>The main branch always has the version suffix set to .dev\\&lt;N&gt;, which allows intermediate publishing from the main branch using version X.Y.Z.dev\\&lt;N&gt;.</li> <li>The <code>scripts/release-branch.sh</code> script automates creation of a new release branch and tag and version numbers in <code>.make.versions</code> </li> <li>Building and publishing is done manually, or soon via a git action, in the branch created by <code>scripts/release-branch.sh</code>. </li> <li>Wheels can only be published once to pypi for a given version.</li> <li>Transform and kfp images may be republished to the docker registry.</li> <li>Releases done via the <code>release-branch.sh</code> script will have their micro version number set to 0 (e.g., 1.2.0)</li> <li>Intermediate releases that bump the micro version may be done by individual transforms. This can mean that version X.Y.Z of a transform is equivalent to the X.Y+1.0 release.  The latter created when running the <code>release-branch.sh</code> script.</li> </ol>"},{"location":"RELEASE/#cutting-the-release","title":"Cutting the release","text":"<p>Creating the release involves</p> <ol> <li>Editing the <code>release-notes.md</code> to list major/minor changes and commit to the main branch.</li> <li>Creating a release branch and updating the main branch versions (using <code>release-branch.sh</code>).</li> <li>Creating a github release and tag from the release branch using the github web UI.</li> <li>Building and publishing pypi library wheels and docker registry image.</li> </ol> <p>Each is discussed below.</p>"},{"location":"RELEASE/#editing-release-notesmd","title":"Editing release-notes.md","text":"<p>Make a dummy release on github (see below) to get a listing of all commits. Use this to come up with the items. Commit this to the main branch so it is ready for including in the release branch.</p>"},{"location":"RELEASE/#creating-release-branch","title":"Creating release branch","text":"<p>The <code>scripts/release-branch.sh</code> is currently run manually to create the branch and tags as follows:</p> <ol> <li>Creates the <code>releases/vX.Y.Z</code> from the main branch where <code>X.Y.Z</code> are defined in .make.versions</li> <li>Creates the <code>pending-releases/vX.Y.Z</code> branch for PR'ing back into the <code>releases/vX.Y.Z</code> branch. </li> <li>In the new <code>pending-releases/vX.Y.Z</code> branch <ol> <li>Nulls out the version suffix in the new branch's <code>.make.version</code> file. </li> <li>Applies the unsuffixed versions to the artifacts published from the repo using <code>make set-versions</code>..</li> <li>Commits and pushes branch </li> </ol> </li> <li>Creates the <code>pending-version-change/vX.Y.Z</code> branch for PR'ing back into the main branch.<ul> <li>Note: this branch is named with the new release version (i.e. vX.Y.Z), however   the version in this branch is actually X.Y+1.0.dev0.</li> </ul> </li> <li>In the <code>pending-version-change/vX.Y.Z</code> branch<ol> <li>Increments the minor version (i.e. Z+1) and resets the suffix to <code>dev0</code> in <code>.make.versions</code>.</li> <li>Commits and pushes branch </li> </ol> </li> </ol> <p>To double-check the version that will be published from the release, <pre><code>git checkout pending-releases/vX.Y.Z \nmake show-version\n</code></pre> This will print for example, 1.2.3. </p> <p>To run the script from the top of the repo:</p> <pre><code>scripts/release-branch.sh\n</code></pre> <p>After running the script, you should 1. Create a pull request from branch <code>pending-releases/vX.Y.Z</code> into the <code>releases/vX.Y.Z</code> branch, and merge. 2. Use the github web UI to create a git release and tag of the <code>releases/vX.Y.Z</code> branch 3. Create a pull request from branch <code>pending-version-change/vX.Y.Z</code> into the main branch, and merge. </p>"},{"location":"RELEASE/#creating-the-github-release","title":"Creating the Github Release","text":"<p>After running the <code>release-branch.sh</code> script, to create tag <code>vX.Y.Z</code> and branch <code>releases/vX.Y.Z</code> and PRing/merging <code>vX.Y.Z</code> into <code>releases/vX.Y.Z</code>. 1. Go to the releases page.  1. Select <code>Draft a new release</code> 1. Select target branch <code>releases/vX.Y.Z</code> 1. Select <code>Choose a tag</code>, type in vX.Y.Z, click <code>Create tag</code> 1. Press <code>Generate release notes</code>  1. Add a title (e.g., Release X.Y.Z)  1. Add any additional relese notes. 1. Press <code>Publish release</code></p>"},{"location":"RELEASE/#building-and-publishing-wheels-and-images","title":"Building and Publishing Wheels and Images","text":"<p>After creating the release and tag on github: </p> <ol> <li>Switch to a release branch (e.g. releases/v1.2.3). </li> <li>Be sure you're at the top of the repository (<code>.../data-prep-kit</code>)</li> <li>Optionally, <code>make show-version</code> to see the version that will be published</li> <li>Running the following, either manually or in a git action<ol> <li><code>make build</code></li> <li><code>make publish</code>   (See credential requirements below)</li> </ol> </li> </ol> <p>For docker registry publishing, the following environment variables/credentials are needed:</p> <ul> <li>DPK_DOCKER_REGISTRY_USER - user used with the registry defined in DOCKER_HOST in <code>.make.defaults</code></li> <li>DPK_DOCKER_REGISTRY_KEY - key/password for docker registry user.</li> </ul> <p>To publish to pypi, the credentials in <code>~/.pypirc</code> file (let us know if there is a way to do this with environment variables). See pypi for details.</p>"},{"location":"release-notes/","title":"Data Prep Kit Release notes","text":""},{"location":"release-notes/#release-022-http-connector-module-10232024","title":"Release 0.2.2- HTTP Connector Module - 10/23/2024","text":""},{"location":"release-notes/#general","title":"General","text":"<ol> <li>Bug fixes across the repo</li> <li>Minor enhancements and experimentation with single packaging techniques using [extra]</li> <li>Decoupled the release process for each of the component so we can be more responsive to the needs of our stakeholders</li> <li>The minor digit for the release for all components is incremented and the patch digit is reset to 0 for all new releases of the data-prep-toolkit</li> <li>The patch digit for the release of any one component can be increased independently from other component patch number</li> </ol>"},{"location":"release-notes/#data-prep-toolkit-connector","title":"data-prep-toolkit-Connector","text":"<ol> <li>Released first version of the data-prep-toolkit-connector for crawling web sites and downloading HTML and PDF files for ingestion by the pipeline</li> </ol>"},{"location":"release-notes/#release-021-9242024","title":"Release 0.2.1 - 9/24/2024","text":""},{"location":"release-notes/#general_1","title":"General","text":"<ol> <li>Bug fixes across the repo</li> <li>Added AI Alliance RAG demo, tutorials and notebooks and tips for running on google colab</li> <li>Added new transforms and single package for transforms published to pypi</li> <li>Improved CI/CD with targeted workflow triggered on specific changes to specific modules</li> <li>New enhancements for cutting a release</li> </ol>"},{"location":"release-notes/#data-prep-toolkit-libraries-python-ray-spark","title":"data-prep-toolkit libraries (python, ray, spark)","text":"<ol> <li>Restructure the repository to distinguish/separate runtime libraries</li> <li>Split data-processing-lib/ray into python and ray</li> <li>Spark runtime</li> <li>Updated pyarrow version</li> <li>Define required transform() method as abstract to AbstractTableTransform</li> <li>Enables configuration of makefile to use src or pypi for data-prep-kit library dependencies </li> </ol>"},{"location":"release-notes/#kfp-workloads","title":"KFP Workloads","text":"<ol> <li>Add a configurable timeout before destroying the deployed Ray cluster.</li> </ol>"},{"location":"release-notes/#transforms","title":"Transforms","text":"<ol> <li>Added 7 new transdforms including: language identification, profiler, repo level ordering, doc quality, pdf2parquet, HTML2Parquet and PII Transform</li> <li>Added ededup python implementation and incremental ededup </li> <li>Added fuzzy floating point comparison</li> </ol>"},{"location":"release-notes/#release-020-6272024","title":"Release 0.2.0 - 6/27/2024","text":""},{"location":"release-notes/#general_2","title":"General","text":"<ol> <li>Many bug fixes across the repo, plus the following specifics.</li> <li>Enhanced CI/CD and makefile improvements  include definition of top-level targets (clean, set-verions, build, publish, test)</li> <li>Automation of release process branch/tag management</li> <li>Documentation improvements </li> </ol>"},{"location":"release-notes/#data-prep-toolkit-libraries-python-ray-spark_1","title":"data-prep-toolkit libraries (python, ray, spark)","text":"<ol> <li>Split libraries into 3 runtime-specific implementations</li> <li>Fix missing final count of processed and add percentages</li> <li>Improved fault tolerance in python and ray runtimes </li> <li>Report global DataAccess retry metric  </li> <li>Support for binary data transforms</li> <li>Updated to Ray version to 2.24</li> <li>Updated to PyArrow version 16.1.0</li> </ol>"},{"location":"release-notes/#kfp-workloads_1","title":"KFP Workloads","text":"<ol> <li>Add KFP V2 support </li> <li>Create a distinct (timestamped) execution.log file for each retry</li> <li>Support for multiple inputs/outputs</li> </ol>"},{"location":"release-notes/#transforms_1","title":"Transforms","text":"<ol> <li>Added language/lang_id - detects language in documents</li> <li>Added universal/profiler - counts works/tokens in documents</li> <li>Converted ingest2parquet tool to transform named code2parquet</li> <li>Split transforms, as appropriate, into python, ray and/or spark.</li> <li>Added spark implementations of filter, doc_id and noop transforms.</li> <li>Switch from using requirements.txt to pyproject.toml file for each transform runtime</li> <li>Repository restructured to move kfp workflow definitions to associated transform project directory</li> </ol>"},{"location":"release-notes/#release-011-5242024","title":"Release 0.1.1 - 5/24/2024","text":""},{"location":"release-notes/#release-010-5152024","title":"Release 0.1.0 - 5/15/2024","text":""},{"location":"release-notes/#release-010-5082024","title":"Release 0.1.0 - 5/08/2024","text":""},{"location":"resources/","title":"Data Prep Kit Resources","text":""},{"location":"resources/#papers","title":"\ud83d\udcc4 Papers","text":"<ol> <li>Data-Prep-Kit: getting your data ready for LLM application development</li> <li>Granite Code Models: A Family of Open Foundation Models for Code Intelligence</li> <li>Scaling Granite Code Models to 128K Context</li> </ol>"},{"location":"resources/#talks","title":"\ud83c\udfa4 Talks","text":"<ol> <li>\"Building Successful LLM Apps: The Power of high quality data\" - Video  |   Slides</li> <li>\"Hands on session for fine tuning LLMs\" - Video</li> <li>\"Build your own data preparation module using data-prep-kit\" - Video</li> </ol>"},{"location":"resources/#example-code","title":"Example Code","text":""},{"location":"resources/#tutorials-blogs","title":"Tutorials / Blogs","text":""},{"location":"resources/#workshops","title":"Workshops","text":"<ul> <li>2024-09-21: \"RAG with Data Prep Kit\" Workshop @ Mountain View, CA, USA - info</li> </ul>"},{"location":"data-connector-lib/","title":"DPK Connector","text":"<p>DPK Connector is a scalable and compliant web crawler developed for data acquisition towards LLM development. It is built on Scrapy. For more details read the documentation.</p>"},{"location":"data-connector-lib/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-connector-lib/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library <pre><code>make test build publish\n</code></pre></p> <p>To up the version number, edit the Makefile to change VERSION and rerun the above. This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-connector-lib/#how-to-use","title":"How to use","text":"<p>See the overview.</p>"},{"location":"data-connector-lib/doc/overview/","title":"DPK Connector Overview","text":"<p>The Data Prep Kit Connector (DPK Connector) is a Python library for scalable and compliant web crawling.</p> <p>Features: - Robots.txt compliant: The Connector follows allow/disallow lists and some extended directives such as <code>Crawl-delay</code> in robots.txt of websites. - Sitemap support: The Connector automatically parses sitemap urls from input and tries to find them from robots.txt. - User agent and headers customization: You can use your own user agent string and request headers. - Domain and path focus: You can limit domains and paths accessed by the library. - Mime type filters: You can restrict mime types which can be downloaded. - Parallel processing: Requests to websites are processed in parallel.</p>"},{"location":"data-connector-lib/doc/overview/#how-to-install","title":"How to install","text":""},{"location":"data-connector-lib/doc/overview/#from-pypi","title":"From PyPI","text":"<pre><code>pip install data-prep-connector\n</code></pre>"},{"location":"data-connector-lib/doc/overview/#from-github","title":"From Github","text":"<pre><code>pip install git+https://github.com/IBM/data-prep-kit.git@dev#subdirectory=data-connector-lib\n</code></pre>"},{"location":"data-connector-lib/doc/overview/#example-usage","title":"Example usage","text":"<pre><code>from dpk_connector import crawl, shutdown\n\n\ndef main():\n    \"\"\"\n    An example of running a crawl.\n    \"\"\"\n\n    def on_downloaded(url: str, body: bytes, headers: dict) -&gt; None:\n        \"\"\"\n        Callback function called when a page has been downloaded.\n        You have access to the request URL, response body and headers.\n        \"\"\"\n        print(f\"url: {url}, headers: {headers}, body: {body[:64]}\")\n\n    user_agent = \"Mozilla/5.0 (X11; Linux i686; rv:125.0) Gecko/20100101 Firefox/125.0\"\n\n    # Start crawling\n    crawl(\n        [\"https://crawler-test.com/\"],\n        on_downloaded,\n        user_agent=user_agent,\n        depth_limit=0,\n    )  # blocking call\n\n    # Shutdown all crawls\n    shutdown()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/","title":"Advanced Transform Tutorial","text":"<p>In this example, we implement an ededup transform that removes duplicate documents across all files. In this tutorial, we will show the following:</p> <ul> <li>How to write the <code>ededup</code> transform to generate the output table.</li> <li>How to define transform-specific metadata that can be associated   with each table transformation and aggregated across all transformations   in a single run of the transform.</li> <li>How to implement custom <code>TransformRuntime</code> to create supporting Ray objects and supplement   transform-specific metadata with the information about this statistics</li> <li>How to define command line arguments that can be used to configure   the operation of our noop transform.</li> </ul> <p>The complete task involves the following:</p> <ul> <li>EdedupTransform - class that implements the specific transformation</li> <li>EdedupRuntime - class that implements custom TransformRuntime to create supporting Ray objects and enhance job output   statistics</li> <li>EdedupTableTransformConfiguration - class that provides configuration for the   EdedupTransform and EdedupRuntime, including transform runtime class and the command line arguments used to   configure them.</li> <li>main() - simple creation and use of the TransformLauncher.</li> </ul> <p>(Currently, the complete code for the noop transform used for this tutorial can be found in the ededup transform directory.</p> <p>Finally, we show to use the command line to run the transform in a local ray cluster</p>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/#hashfilter","title":"HashFilter","text":"<p>One of the basic components of exact dedup implementation is a cache of hashes. That is why we will start from implementing this support actor. The implementation is fairly straight forward and can be found here</p>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/#ededuptransform","title":"EdedupTransform","text":"<p>First, let's define the transform class.  To do this we extend the base abstract/interface class AbstractTableTransform, which requires definition of the following:</p> <ul> <li>an initializer (i.e. <code>init()</code>) that accepts a dictionary of configuration   data.  For this example, the configuration data will only be defined by   command line arguments (defined below).</li> <li>the <code>transform()</code> method itself that takes an input table and produces an output   table and any associated metadata for that table transformation.</li> </ul> <p>Other methods such as <code>flush()</code> need not be overridden/redefined for this example.</p> <p>We start with the simple definition of the class, its initializer and the imports required by subsequent code:</p> <p><pre><code>from argparse import ArgumentParser, Namespace\nfrom typing import Any\n\nimport pyarrow as pa\nimport ray\nfrom data_processing_ray.data_access import DataAccessFactoryBase\nfrom data_processing_ray.runtime.ray import (\n  DefaultRayTransformRuntime,\n  RayTransformLauncher,\n  RayUtils,\n)\nfrom data_processing_ray.runtime.ray.runtime_configuration import (\n  RayTransformRuntimeConfiguration,\n)\nfrom data_processing.transform import AbstractTableTransform, TransformConfiguration\nfrom data_processing.utils import GB, CLIArgumentProvider, TransformUtils, get_logger\nfrom ray.actor import ActorHandle\n\n\nclass EdedupTransform(AbstractTableTransform):\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n    self.doc_column = config.get(\"doc_column\", \"\")\n    self.hashes = config.get(\"hashes\", [])\n</code></pre> The <code>EdedupTransform</code> class extends the <code>AbstractTableTransform</code>, which defines the required methods.</p> <p>For purposes of the tutorial and to simulate a more complex processing job, our initializer allows our transform to be configurable with document column name and a list of hash actors during the call to <code>transform()</code>. Configuration is provided by the framework in a dictionary provided to the initializer. Below we will cover how <code>doc_column</code> and <code>hashes</code> arguments are made available to the initializer.</p> <p>Next we define the <code>transform()</code> method itself, which includes the addition of some metadata.</p> <p><pre><code>    def transform(self, table: pa.Table) -&gt; tuple[list[pa.Table], dict[str, Any]]:\n\n\n  if not TransformUtils.validate_columns(table=table, required=[self.doc_column]):\n    return [], {}\n# Inner variables\nhashes = set()\nunique = []\nhd = {}\n# Compute unique hashes for the table\nfor text in table[self.doc_column]:\n  # Compute doc hash\n  h = TransformUtils.str_to_hash(TransformUtils.normalize_string(str(text)))\n  if h not in hashes:  # Processing this hash for the first time\n    hashes.add(h)  # Remember it locally\n    hd[h] = str(text)\n    if len(hd) &gt;= REQUEST_LEN:  # time to check remotely\n      unique = unique + self._process_cached_hashes(hd=hd)\n      hd = {}\nif len(hd) &gt; 0:  # Process remaining hashes\n  unique = unique + self._process_cached_hashes(hd=hd)\n\n# Remove duplicates\nunique_set = set(unique)\nmask = [False] * table.num_rows\nindex = 0\nfor text in table[self.doc_column]:\n  str_text = str(text)\n  if str_text in unique_set:\n    mask[index] = True\n    unique_set.remove(str_text)\n  index += 1\n# Create output table\nout_table = table.filter(mask)\n# report statistics\nstats = {\"source_documents\": table.num_rows, \"result_documents\": out_table.num_rows}\nreturn [out_table], stats\n</code></pre> The single input to this method is the in-memory pyarrow table to be transformed. The return of this function is a list of tables and optional metadata.  In this case of simple 1:1 table conversion the list will contain a single table, result of removing duplicates from input table.</p> <p>The metadata is a free-form dictionary of keys with numeric values that will be aggregated by the framework and reported as aggregated job statistics metadata. If there is no metadata then simply return an empty dictionary.</p>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/#ededupruntime","title":"EdedupRuntime","text":"<p>First, let's define the transform runtime class.  To do this we extend the base abstract/interface class DefaultTableTransformRuntime, which requires definition of the following:</p> <ul> <li>an initializer (i.e. <code>init()</code>) that accepts a dictionary of configuration   data.  For this example, the configuration data will only be defined by   command line arguments (defined below).</li> <li>the <code>get_transform_config()</code> method that takes <code>data_access_factory</code>, <code>statistics actor</code>, and   <code>list of files to process</code> and produces a dictionary of parameters used by transform.</li> <li>the <code>compute_execution_stats()</code> method that takes take a dictionary of metadata, enhances it and   produces an enhanced metadata dictionary.</li> </ul> <p>We start with the simple definition of the class and its initializer</p> <p><pre><code>class EdedupRuntime(DefaultTableTransformRuntime):\n\n    def __init__(self, params: dict[str, Any]):\n        super().__init__(params)\n        self.filters = []\n</code></pre> Next we define the <code>get_transform_config()</code> method, which, in this case, creates supporting Ray Actors and adds their handles to the transform parameters</p> <p><pre><code>    def get_transform_config(\n        self, data_access_factory: DataAccessFactory, statistics: ActorHandle, files: list[str]\n) -&gt; dict[str, Any]:\n\n\nself.aggregators = RayUtils.create_actors(\n  clazz=HashFilter,\n  params={},\n  actor_options={\"num_cpus\": self.params.get(\"hash_cpu\", 0.5)},\n  n_actors=self.params.get(\"num_hashes\", 1),\n)\nreturn {\"hashes\": self.aggregators} | self.params\n</code></pre> Inputs to this method includes a set of parameters, that moght not be needed for this transformer, but rather a superset of all parameters that can be used by different implementations of transform runtime ( see for example fuzzy dedup, etc). The return of this function is a dictionary information for transformer initialization. In this implementation we add additional parameters to the input dictionary, but in general, it can be a completely new dictionary build here</p> <p>Finally we define the <code>compute_execution_stats()</code> method, which which enhances metadata collected by statistics class</p> <p><pre><code>    def compute_execution_stats(self, stats: dict[str, Any]) -&gt; dict[str, Any]:\n\n\n# Get filters stats\nsum_hash = 0\nsum_hash_mem = 0\nremote_replies = [f.get_size.remote() for f in self.aggregators]\nwhile remote_replies:\n  # Wait for replies\n  ready, not_ready = ray.wait(remote_replies)\n  for r in ready:\n    h_size, h_memory = ray.get(r)\n    sum_hash = sum_hash + h_size\n    sum_hash_mem = sum_hash_mem + h_memory\n  remote_replies = not_ready\ndedup_prst = 100 * (1.0 - stats.get(\"result_documents\", 1) / stats.get(\"source_documents\", 1))\nreturn {\"number of hashes\": sum_hash, \"hash memory, GB\": sum_hash_mem, \"de duplication %\": dedup_prst} | stats\n</code></pre> Input to this method is a dictionary of metadata collected by statistics object. It then enhances it by information collected by hash actors and custom computations based on statistics data.</p>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/#ededuptabletransformconfiguration","title":"EdedupTableTransformConfiguration","text":"<p>The final class we need to implement is <code>EdedupRayTransformConfiguration</code> class that provides configuration for  running our transform. Although we provide only Ray-based implementation, Ray-based configuration relies on Python-based configuration that we need to define first. So we first need to define <code>EdedupTableTransformConfiguration</code> class,  defining the following:</p> <ul> <li>The short name for the transform</li> <li>The class implementing the transform - in our case EdedupTransform</li> <li>The transform runtime class be used - in our case EdedupRuntime</li> <li>Command line argument support.</li> </ul> <p>First we define the class and its initializer,</p> <pre><code>short_name = \"ededup\"\ncli_prefix = f\"{short_name}_\"\n\nclass EdedupTableTransformConfiguration(TransformConfiguration):\n  def __init__(self):\n    super().__init__(\n      name=short_name,\n      transform_class=EdedupTransform,\n    )\n</code></pre> <p>The initializer extends the DefaultTableTransformConfiguration which provides simple capture of our configuration data and enables picklability through the network. It also adds a <code>params</code> field that will be used below to hold the transform's configuration data (used in <code>EdedupRuntime.init()</code> above).</p> <p>Next, we provide two methods that define and capture the command line configuration that is specific to the <code>EdedupTransform</code>, in this case the number of seconds to sleep during transformation. First we define the method establishes the command line arguments. This method is given a global argument parser to which the <code>EdedupTransform</code> arguments are added. It is good practice to include a common prefix to all transform-specific options (i.e. pii, lang, etc). In our case we will use <code>noop_</code>.</p> <p><pre><code>    def add_input_params(self, parser: ArgumentParser) -&gt; None:\n      parser.add_argument(f\"--{cli_prefix}hash_cpu\", type=float, default=0.5, help=\"number of CPUs per hash\")\n      parser.add_argument(f\"--{cli_prefix}num_hashes\", type=int, default=0, help=\"number of hash actors to use\")\n      parser.add_argument(f\"--{cli_prefix}doc_column\", type=str, default=\"contents\", help=\"key for accessing data\")\n</code></pre> Next we implement a method that is called after the framework has parsed the CLI args and which allows us to capture the <code>EdedupTransform</code>-specific arguments and optionally validate them.</p> <p><pre><code>    def apply_input_params(self, args: Namespace) -&gt; bool:\n      captured = CLIArgumentProvider.capture_parameters(args, cli_prefix, False)\n      self.params = self.params | captured\n      if self.params[\"num_hashes\"] &lt;= 0:\n        logger.info(f\"Number of hashes should be greater then zero, provided {args.num_hashes}\")\n        return False\n      logger.info(f\"exact dedup params are {self.params}\")\n      return True\n</code></pre> Now we can implement <code>EdedupRayTransformConfiguration</code>with the following code <pre><code>class EdedupRayTransformConfiguration(RayTransformConfiguration):\n    def __init__(self):\n        super().__init__(transform_config=EdedupTableTransformConfiguration(), runtime_class=EdedupRuntime)\n</code></pre></p>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/#main","title":"main()","text":"<p>Next, we show how to launch the framework with the <code>EdedupTransform</code> using the framework's <code>TransformLauncher</code> class.</p> <pre><code>if __name__ == \"__main__\":\n  launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n  launcher.launch()\n</code></pre> <p>The launcher requires only an instance of DefaultTableTransformConfiguration (our <code>EdedupRayTransformConfiguration</code> class). A single method <code>launch()</code> is then invoked to run the transform in a Ray cluster.</p>"},{"location":"data-processing-lib/doc/advanced-transform-tutorial/#running","title":"Running","text":"<p>Note: You will need to run the setup commands in the <code>README</code> before running the following examples.</p> <p>Assuming the above <code>main()</code> is placed in <code>ededup_transform.py</code> we can run the transform on local data  as follows:</p> <p><pre><code>make run-cli-sample\n</code></pre> See the launcher options for a complete list of transform-independent command line options.</p>"},{"location":"data-processing-lib/doc/architecture/","title":"Data Processing Architecture","text":"<p>In this section we cover the high-level architecture, some of the core components.  </p> <p>Transform implementation and examples are provided in the tutorial.</p>"},{"location":"data-processing-lib/doc/architecture/#architecture","title":"Architecture","text":"<p>The architecture is a \"standard\" implementation of Embarrassingly parallel to process many input files in parallel using a distribute network of RayWorkers.</p> <p></p> <p>The architecture includes the following core components: </p> <ul> <li> <p>RayLauncher accepts and validates   CLI parameters to establish the Ray Orchestrator with the proper configuration.  It uses the following components, all of which can/do define CLI configuration parameters.:</p> <ul> <li>Transform Orchestrator Configuration is responsible   for defining and validating infrastructure parameters   (e.g., number of workers, memory and cpu, local or remote cluster, etc.). This class has very simple state  (several dictionaries) and is fully pickleable. As a result framework uses its instance as a  parameter in remote functions/actors invocation.</li> <li>DataAccessFactory - provides the   configuration for the type of DataAccess to use when reading/writing the input/output data for   the transforms.  Similar to Transform Orchestrator Configuration, this is a pickleable   instance that is passed between Launcher, Orchestrator and Workers.</li> <li>TransformConfiguration - defines specifics   of the transform implementation including transform implementation class, its short name, any transform-   specific CLI parameters, and an optional TransformRuntime class, discussed below. </li> </ul> <p>After all parameters are validated, the ray cluster is started and the DataAccessFactory, TransformOrchestratorConfiguraiton and TransformConfiguration are given to the Ray Orchestrator, via Ray remote() method invocation. The Launcher waits for the Ray Orchestrator to complete.</p> </li> <li> <p>documents with Ray Orchestrator is responsible for overall management of   the data processing job. It creates the actors, determines the set of input data and distributes the    references to the data files to be processed by the workers. More specifically, it performs the following:</p> </li> <li> <p>Uses the DataAccess instance created by the DataAccessFactory to determine the set of the files    to be processed.  </p> </li> <li>uses the TransformConfiguration to create the TransformRuntime instance </li> <li>Uses the TransformRuntime to optionally apply additional configuration (ray object storage, etc) for the configuration   and operation of the Transform.</li> <li>uses the TransformOrchestratorConfiguration to determine the set of RayWorkers to create   to execute transformers in parallel, providing the following to each worker:<ul> <li>Ray worker configuration</li> <li>DataAccessFactory </li> <li>Transform class and its TransformConfiguration containing the CLI parameters and any TransformRuntime additions.</li> </ul> </li> <li>in a load-balanced, round-robin fashion, distributes the names of the input files to the workers for them to transform/process.</li> </ul> <p>Additionally, to provide monitoring of long-running transforms, the orchestrator is instrumented with    custom metrics, that are exported to localhost:8080 (this is the endpoint that    Prometheus would be configured to scrape).   Once all data is processed, the orchestrator will collect execution statistics (from the statistics actor)    and build and save it in the form of execution metadata (<code>metadata.json</code>). Finally, it will return the execution    result to the Launcher.</p> <ul> <li> <p>Ray worker is responsible for  reading files (as PyArrow Tables) assigned by the orchestrator, applying the transform to the input table and writing out the  resulting table(s).  Metadata produced by each table transformation is aggregated into Transform Statistics (below).</p> </li> <li> <p>Transform Statistics is a general  purpose data collector actor aggregating the numeric metadata from different places of  the framework (especially metadata produced by the transform). These statistics are reported as metadata (<code>metadata.json</code>) by the orchestrator upon completion.</p> </li> </ul>"},{"location":"data-processing-lib/doc/architecture/#core-components","title":"Core Components","text":"<p>Some of the core components used by the architecture are definfed here:</p> <ul> <li>CLIProvider - provides a general purpose   mechanism for defining, validating and sharing CLI parameters.    It is used by the DataAccessFactor and Transform Configuration (below).</li> <li>Data Access is an abstraction layer for different data access supported by the framework. The main components   of this layer are:</li> <li>Data Access is the basic interface for the data access, and enables the identification of    input files to process, associated output files, checkpointing and general file reading/writing.     Currently, the framework implements several concrete implementations of the Data Access, including     local data support and     s3. Additional Data Access implementations can be added as required.</li> <li>Data Access Factory is an implementation of the      factory design pattern for creation     of the data access instances. Data Access Factory, as a CLIProvider,  enables the definition of CLI      parameters that configure the instance of Data Access to be created. Data Access factory has very simple state      (several dictionaries) and is fully pickleable. The framework uses Data Access Factory instance as a      parameter in remote functions/actors invocations.</li> </ul>"},{"location":"data-processing-lib/doc/architecture/#transforms","title":"Transforms","text":"<p>A brief discussion of the Transform components are provided here. For a more complete discussion, see the tutorials.</p> <ul> <li>Transform - defines the methods required of any transform implementation - <code>transform()</code> and <code>flush()</code> - and provides the bulk of any transform implementation convert one Table to 0 or more new Tables.   In general, this is not tied to the above Ray infrastructure  and so can usually be used independent of Ray. </li> <li>TransformConfiguration - this is the bootstrap   class provided to the Launcher that enables the instantiation of the Transform and the TransformRuntime within   the architecture.  It is a CLIProvider, which allows it to define transform-specific CLI configuration   that is made available to the Transform's initializer.</li> </ul>"},{"location":"data-processing-lib/doc/data-access-factory/","title":"Data Access Factory","text":""},{"location":"data-processing-lib/doc/data-access-factory/#introduction","title":"Introduction","text":"<p>Data Access Factory(DAF) provides a mechanism to create  DataAccess  implementations that support the processing of input data files and the expected destination of the processed files. The <code>DataAccessFactory</code> is most often configured using command line arguments to specify the type of <code>DataAccess</code> instance to create (see <code>--data_*</code> options here. Currently,  it supports DataAccessLocal and  DataAccessS3 implementations.</p> <p>You can use DAF and the resulting DataAccess implementation in your transform logic to read and write extra file(s), for example, write log or metadata files.</p> <p>This document explains how to initialize and use DAF to write a file using a <code>DataAccess</code> instance. </p>"},{"location":"data-processing-lib/doc/data-access-factory/#data-access","title":"Data Access","text":"<p>Each Data Access implementation supports the notion of processing a set of input files to produce a set of output files, generally in a 1:1 mapping,  although this is not strictly required. With this in mind, the following function is provided:  * Input file identification by      * input folder      * sub-directory selection (aka data sets))     * file extension     * files extensions to checkpoint     * maximum count     * random sampling * Output file identification (for a given input) * Checkpointing  - determines the set of input files that need processing  (i.e. which do not have corresponding output files). In the case of parquet files, where inputs and outputs are parquet this comparison is fairly simple. In the case of binary files it is a little bit more involved as input and output files may have different extensions. in this case you need to specify both <code>files extensions</code> and <code>files extensions to checkpoint</code> * Reading and writing of files.</p> <p>Each transform runtime uses a DataAccessFactory to create a DataAccess instance which is then used to identify and process the target input data. Transforms may use this the runtime instance or can use their own DataAccessFactory. This might be needed if reading or writing other files to/from other locations.</p>"},{"location":"data-processing-lib/doc/data-access-factory/#creating-daf-instance","title":"Creating DAF instance","text":"<p><pre><code>from data_processing.data_access import DataAccessFactory\ndaf = DataAccessFactory(\"myprefix_\", False)\n</code></pre> The first parameter <code>cli_arg_prefix</code> is prefix used to look for parameter names  starting with prefix <code>myprefix_</code>. Generally the prefix used is specific to the transform.</p>"},{"location":"data-processing-lib/doc/data-access-factory/#preparing-and-setting-parameters","title":"Preparing and setting parameters","text":"<p><pre><code>from argparse import Namespace\n\ns3_cred = {\n    \"access_key\": \"XXXX\",\n    \"secret_key\": \"XXX\",\n    \"url\": \"https://s3.XXX\",\n}\n\ns3_conf={\n    'input_folder': '&lt;COS Location of input&gt;', \n    'output_folder': 'cos-optimal-llm-pile/somekey'\n}\n\nargs = Namespace(\n    myprefix_s3_cred=s3_cred,\n    myprefix_s3_config=s3_conf,\n)\nassert daf.apply_input_params(args)\n</code></pre> <code>apply_input_params</code> will extract and use parameters from <code>args</code> with  prefix <code>myprefix_</code>(which is <code>myprefix_s3_cred</code> and <code>myprefix_s3_config</code> in this example).</p> <p>The above is equivalent to passing the following on the command line to a runtime launcher <pre><code>... --myprefix_s3_cred '{ \"access_key\": \"XXXX\", \"secret_key\": \"XXX\", \"url\": \"https:/s3.XXX\" }'\\\n    --myprefix_s3_config '{ \"input_folder\": \"&lt;COS Location of input&gt;\", \"cos-optimal-llm-pile/somekey\" }'\n</code></pre></p>"},{"location":"data-processing-lib/doc/data-access-factory/#create-dataaccess-and-write-file","title":"Create DataAccess and write file","text":"<pre><code>data_access = daf.create_data_access()\ndata_access.save_file(f\"data/report.log\", \"success\")\n</code></pre> <p>Call to <code>create_data_access</code> will create the <code>DataAccess</code> instance (<code>DataAccessS3</code> in this case) . <code>save_file</code> will write a new file at <code>data/report.log</code> with content <code>success</code>.</p> <p>When writing a transform, the <code>DataAccessFactory</code> is generally created in the transform's configuration class and passed to the transform's initializer by the runtime.  See this section on accessing external resources for details.</p>"},{"location":"data-processing-lib/doc/overview/","title":"Data Processing Overview","text":"<p>The Data Processing Framework is python-based and enables the  application of \"transforms\" to  a one or more input data files  to produce one or more output data files. Various runtimes are available to execute the transforms using a common shared methodology and mechanism to configure input and output across either local or S3-base storage.</p> <p>The framework allows simple 1:1 transformation of (parquet) files, but also enables more complex transformations requiring coordination among transforming nodes. This might include operations such as de-duplication, merging, and splitting. The framework uses a plug-in model for the primary functions.  The core transformation-specific classes/interfaces are as follows:</p> <ul> <li>AbstractBinaryTransform -  a simple, easily-implemented interface allowing the definition transforms of arbitrary data as a byte array. Additionally table transform interface is provided allowing definition of transforms operating on  pyarrow tables.</li> <li>TransformConfiguration - defines the transform short name, its implementation class,  and command line configuration parameters.</li> </ul> <p>In support of running a transform over a set of input data in a runtime, the following class/interfaces are provided:</p> <ul> <li>AbstractTransformLauncher - is the central runtime interfacee expected to be implemented by each runtime (python ray, spark, etc.) to apply a transform to a set of data.   It is configured with a <code>TransformRuntimeConfiguration</code> and a <code>DataAccessFactory</code> instance (see below).</li> <li>DataAccessFactory - is used to configure the input and output data files to be processed and creates the <code>DataAccess</code> instance (see below) according to the CLI parameters.</li> <li>TransformRuntimeConfiguration - captures   the <code>TransformConfiguration</code> and runtime-specific configuration.</li> <li>DataAccess - is   the interface defining data i/o methods and selection.  Implementations for local   and S3 storage are provided.</li> </ul> <p></p> <p>To learn more consider the following:</p> <ul> <li>Transforms</li> <li>Transform Exceptions</li> <li>Transform Runtimes</li> <li>Transform Examples</li> <li>Simplified transform APIs</li> <li>Data Access Factory</li> <li>Testing Transforms</li> <li>Utilities</li> <li>Architecture Deep Dive</li> <li>Transform project root readme</li> </ul>"},{"location":"data-processing-lib/doc/python-launcher-options/","title":"Pure Python Launcher Command Line Options","text":"<p>A number of command line options are available when launching a transform as a Python class.  </p> <p>The following is a current --help output (a work in progress) for  the <code>NOOPTransform</code> (note the --noop_sleep_sec option):</p> <pre><code>usage: noop_python_runtime.py [-h] [--noop_sleep_sec NOOP_SLEEP_SEC] [--noop_pwd NOOP_PWD] [--data_s3_cred DATA_S3_CRED] [--data_s3_config DATA_S3_CONFIG] [--data_local_config DATA_LOCAL_CONFIG] [--data_max_files DATA_MAX_FILES]\n                              [--data_checkpointing DATA_CHECKPOINTING] [--data_data_sets DATA_DATA_SETS] [--data_files_to_use DATA_FILES_TO_USE] [--data_num_samples DATA_NUM_SAMPLES] [--runtime_pipeline_id RUNTIME_PIPELINE_ID]\n                              [--runtime_job_id RUNTIME_JOB_ID] [--runtime_code_location RUNTIME_CODE_LOCATION]\n\nDriver for noop processing\n\noptions:\n  -h, --help            show this help message and exit\n  --noop_sleep_sec NOOP_SLEEP_SEC\n                        Sleep actor for a number of seconds while processing the data frame, before writing the file to COS\n  --noop_pwd NOOP_PWD   A dummy password which should be filtered out of the metadata\n  --data_s3_cred DATA_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access.\n                        access_key: access key help text\n                        secret_key: secret key help text\n                        url: optional s3 url\n                        region: optional s3 region\n                        Example: { 'access_key': 'access', 'secret_key': 'secret', \n                        'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud', \n                        'region': 'us-east-1' }\n  --data_s3_config DATA_S3_CONFIG\n                        AST string containing input/output paths.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': 's3-path/your-input-bucket', \n                        'output_folder': 's3-path/your-output-bucket' }\n  --data_local_config DATA_LOCAL_CONFIG\n                        ast string containing input/output folders using local fs.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': './input', 'output_folder': '/tmp/output' }\n  --data_max_files DATA_MAX_FILES\n                        Max amount of files to process\n  --data_checkpointing DATA_CHECKPOINTING\n                        checkpointing flag\n  --data_data_sets DATA_DATA_SETS\n                        List of sub-directories of input directory to use for input. For example, ['dir1', 'dir2']\n  --data_files_to_use DATA_FILES_TO_USE\n                        list of file extensions to choose for input.\n  --data_num_samples DATA_NUM_SAMPLES\n                        number of random input files to process\n  --runtime_num_processors RUNTIME_NUM_PROCESSORS\n                        size of multiprocessing pool\n  --runtime_pipeline_id RUNTIME_PIPELINE_ID\n                        pipeline id\n  --runtime_job_id RUNTIME_JOB_ID\n                        job id\n  --runtime_code_location RUNTIME_CODE_LOCATION\n                        AST string containing code location\n                        github: Github repository URL.\n                        commit_hash: github commit hash\n                        path: Path within the repository\n                        Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324', \n                        'path': 'transforms/universal/code' }\n</code></pre>"},{"location":"data-processing-lib/doc/python-runtime/","title":"Python runtime","text":""},{"location":"data-processing-lib/doc/python-runtime/#python-runtime","title":"Python Runtime","text":"<p>The python runtime provides a simple mechanism to run a transform on a set of input data to produce a set of output data, all within a single python execution environment. We currently support two  options of Python execution: * Sequential execution - all files are processed sequentially * Usage of the Python multiprocessing pool.  In this case execution start user-defined number of processors which allows to parallelize data processing</p> <p><code>Note</code> some of transformers, for example, exact dedup do not support multi processing Python runtime, as they rely on a shared classes, which are not supported by this runtime</p> <p>To support multiprocessing pool based runtime, Python execution introduced an additional  parameter: * <code>runtime_num_processors</code> defines the number of processors to use for execution. If this number is greater then 0, multiprocessing pool runtime is used with number of processor equal to  <code>num_processors</code>. Default number of processors is 0.</p> <p>Usage of this parameter allows user to choose the type of Python execution runtime and configure parallelism in the case of multiprocessing pool.</p> <p>A <code>PythonTransformLauncher</code> class is provided that enables the running of the transform.  For example,</p> <p><pre><code>launcher = PythonTransformLauncher(YourTransformConfiguration())\nlauncher.launch()\n</code></pre> The <code>YourTransformConfiguration</code> class configures your transform. More details can be found in the transform tutorial.</p>"},{"location":"data-processing-lib/doc/ray-launcher-options/","title":"Ray Launcher Command Line Options","text":"<p>A number of command line options are available when launching a transform.  </p> <p>The following is a current --help output (a work in progress) for  the <code>NOOPTransform</code> (note the --noop_sleep_sec and --noop_pwd options):</p> <pre><code>usage: noop_transform.py [-h] [--run_locally RUN_LOCALLY] [--noop_sleep_sec NOOP_SLEEP_SEC] [--noop_pwd NOOP_PWD] [--data_s3_cred DATA_S3_CRED] [--data_s3_config DATA_S3_CONFIG] [--data_local_config DATA_LOCAL_CONFIG]\n                         [--data_max_files DATA_MAX_FILES] [--data_checkpointing DATA_CHECKPOINTING] [--data_data_sets DATA_DATA_SETS] [--data_files_to_use DATA_FILES_TO_USE] [--data_num_samples DATA_NUM_SAMPLES]\n                         [--runtime_num_workers RUNTIME_NUM_WORKERS] [--runtime_worker_options RUNTIME_WORKER_OPTIONS] [--runtime_creation_delay RUNTIME_CREATION_DELAY] [--runtime_pipeline_id RUNTIME_PIPELINE_ID]\n                         [--runtime_job_id RUNTIME_JOB_ID] [--runtime_code_location RUNTIME_CODE_LOCATION]\n\nDriver for noop processing\n\noptions:\n  -h, --help            show this help message and exit\n  --run_locally RUN_LOCALLY\n                        running ray local flag\n  --noop_sleep_sec NOOP_SLEEP_SEC\n                        Sleep actor for a number of seconds while processing the data frame, before writing the file to COS\n  --noop_pwd NOOP_PWD   A dummy password which should be filtered out of the metadata\n  --data_s3_cred DATA_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access.\n                        access_key: access key help text\n                        secret_key: secret key help text\n                        url: optional s3 url\n                        region: optional s3 region\n                        Example: { 'access_key': 'access', 'secret_key': 'secret', \n                        'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud', \n                        'region': 'us-east-1' }\n  --data_s3_config DATA_S3_CONFIG\n                        AST string containing input/output paths.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': 's3-path/your-input-bucket', \n                        'output_folder': 's3-path/your-output-bucket' }\n  --data_local_config DATA_LOCAL_CONFIG\n                        ast string containing input/output folders using local fs.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': './input', 'output_folder': '/tmp/output' }\n  --data_max_files DATA_MAX_FILES\n                        Max amount of files to process\n  --data_checkpointing DATA_CHECKPOINTING\n                        checkpointing flag\n  --data_data_sets DATA_DATA_SETS\n                        List of sub-directories of input directory to use for input. For example, ['dir1', 'dir2']\n  --data_files_to_use DATA_FILES_TO_USE\n                        list of file extensions to choose for input.\n  --data_num_samples DATA_NUM_SAMPLES\n                        number of random input files to process\n  --runtime_num_workers RUNTIME_NUM_WORKERS\n                        number of workers\n  --runtime_worker_options RUNTIME_WORKER_OPTIONS\n                        AST string defining worker resource requirements.\n                        num_cpus: Required number of CPUs.\n                        num_gpus: Required number of GPUs\n                        resources: The complete list can be found at\n                                   https://docs.ray.io/en/latest/ray-core/api/doc/ray.remote_function.RemoteFunction.options.html#ray.remote_function.RemoteFunction.options\n                                   and contains accelerator_type, memory, name, num_cpus, num_gpus, object_store_memory, placement_group,\n                                   placement_group_bundle_index, placement_group_capture_child_tasks, resources, runtime_env,\n                                   scheduling_strategy, _metadata, concurrency_groups, lifetime, max_concurrency, max_restarts,\n                                   max_task_retries, max_pending_calls, namespace, get_if_exists\n                        Example: { 'num_cpus': '8', 'num_gpus': '1', \n                        'resources': '{\"special_hardware\": 1, \"custom_label\": 1}' }\n  --runtime_creation_delay RUNTIME_CREATION_DELAY\n                        delay between actor' creation\n  --runtime_pipeline_id RUNTIME_PIPELINE_ID\n                        pipeline id\n  --runtime_job_id RUNTIME_JOB_ID\n                        job id\n  --runtime_code_location RUNTIME_CODE_LOCATION\n                        AST string containing code location\n                        github: Github repository URL.\n                        commit_hash: github commit hash\n                        path: Path within the repository\n                        Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324', \n                        'path': 'transforms/universal/code' }\n</code></pre>"},{"location":"data-processing-lib/doc/ray-runtime/","title":"Ray Runtime","text":"<p>The Ray runtime provides the ability to run in either a local or Kubernetes cluster, and includes the following set of components:</p> <ul> <li>RayTransformLauncher - this is a  class generally used to implement <code>main()</code> that makes use of a <code>TransformConfiguration</code> to  start the Ray runtime and execute the transform over the specified set of input files. The RayTransformLauncher is created using a <code>RayTransformConfiguration</code> instance.</li> <li>RayTransformConfiguration - this  class extends transform's base TransformConfiguration implementation to add an optional  <code>TranformRuntime</code> (see next) class to be used by the transform implementation.</li> <li>TransformRuntime -  this provides the ability for the transform implementor to create additional Ray resources  and include them in the configuration used to create a transform (see, for example, </li> <li>fuzzy dedup Many transforms will not need additional resources and can use the DefaultRayTransformRuntime. <code>TransformRuntime</code> also provide the ability to supplement the statics collected by Statistics (see below).</li> </ul> <p>Roughly speaking the following steps are completed to establish transforms in the RayWorkers</p> <ol> <li>Launcher parses the CLI parameters using an ArgumentParser configured with its own CLI parameters  along with those of the Transform Configuration, </li> <li>Launcher passes the Transform Configuration and CLI parameters to the RayOrchestrator</li> <li>RayOrchestrator creates the Transform Runtime using the Transform Configuration and its CLI parameter values</li> <li>Transform Runtime creates transform initialization/configuration including the CLI parameters, and any Ray components need by the transform.</li> <li>RayWorker is started with configuration from the Transform Runtime.</li> <li>RayWorker creates the Transform using the configuration provided by the Transform Runtime.</li> <li>Statistics is used to collect the statistics submitted by the individual transform, that  is used for building execution metadata.</li> </ol> <p></p>"},{"location":"data-processing-lib/doc/ray-runtime/#ray-transform-launcher","title":"Ray Transform Launcher","text":"<p>The RayTransformLauncher uses the Transform Configuration and provides a single method, <code>launch()</code>, that kicks off the Ray environment and transform execution coordinated  by orchestrator. For example, <pre><code>launcher = RayTransformLauncher(YourTransformConfiguration())\nlauncher.launch()\n</code></pre> Note that the launcher defines some additional CLI parameters that are used to control the operation of the  orchestrator and workers and  data access.  Things such as data access configuration, number of workers, worker resources, etc. Discussion of these options is beyond the scope of this document  (see Launcher Options for a list of available options.)</p>"},{"location":"data-processing-lib/doc/ray-runtime/#transform-configuration","title":"Transform Configuration","text":"<p>In general, a transform should be able to run in both the python and Ray runtimes. As such we first define the python-only transform configuration, which will then be used by the Ray-runtime-specific transform configuration.  The python transform configuration implements TransformConfiguration and defines with transform-specific name, and implementation  and class. In addition, it is responsible for providing transform-specific methods to define and capture optional command line arguments. <pre><code>class YourTransformConfiguration(TransformConfiguration):\n\n    def __init__(self):\n        super().__init__(name=\"YourTransform\", transform_class=YourTransform)\n        self.params = {}\n\n    def add_input_params(self, parser: ArgumentParser) -&gt; None:\n        ...\n    def apply_input_params(self, args: Namespace) -&gt; bool:\n        ...\n</code></pre> Next we define the Ray-runtime specific transform configuration as an extension of the RayTransformConfiguration and uses the <code>YourTransformConfiguration</code> above. <pre><code>class YourTransformConfiguration(RayTransformConfiguration):\n    def __init__(self):\n        super().__init__(YourTransformConfiguration(),\n                         runtime_class=YourTransformRuntime)\n</code></pre> This class provides the ability to create the instance of <code>YourTransformRuntime</code> class (see below) as needed by the Ray runtime.  Note, that not all transforms will require a <code>runtime_class</code> and can omit this parameter to default to an acceptable runtime class. Details are covered in the advanced transform tutorial.</p>"},{"location":"data-processing-lib/doc/ray-runtime/#transform-runtime","title":"Transform Runtime","text":"<p>The  DefaultRayTransformRuntime class is provided and will be  sufficient for many use cases, especially 1:1 table transformation. However, some transforms will require use of the Ray environment, for example, to create additional workers, establish a shared memory object, etc. Of course, these transforms will generally not run outside of a Ray environment. </p> <pre><code>class DefaultRayTransformRuntime:\n\n    def __init__(self, params: dict[str, Any]):\n        ...\n\n    def get_transform_config(\n        self, data_access_factory: DataAccessFactory, statistics: ActorHandle, files: list[str]\n    ) -&gt; dict[str, Any]:\n        ...\n\n    def compute_execution_stats(self, stats: dict[str, Any]) -&gt; dict[str, Any]:\n        ...\n</code></pre> <p>The RayOrchestrator initializes the instance with the CLI parameters provided by the Transform Configurations <code>get_input_params()</code> method.</p> <p>The <code>get_transform_config()</code> method is used by the RayOrchestrator to create the parameters used to initialize the Transform in the RayWorker.  This is where additional Ray components would be added to the environment  and references added to them, as needed, in the returned dictionary of configuration data that will initialize the transform. For those transforms that don't need this support, the default implementation simpy returns the CLI parameters used to initialize the runtime instance.</p> <p>The <code>computed_execution_stats()</code> provides an opportunity to augment the statistics collected and aggregated by the TransformStatistics actor. It is called by the RayOrchestrator after all files have been processed.</p>"},{"location":"data-processing-lib/doc/simplest-transform-tutorial/","title":"Simplest Transform Tutorial","text":"<p>In this example, we implement a noop  transform that takes no action on the input datum and returns it unmodified - a no operation (noop). This effectively enables a copy of a directory tree of files to an output directory. This is functionally not too powerful, but allows us to focus on the minimum requirements for a transform. </p> <p>NOTE: What follows is a discussion of pyarrow Table transform that will run in either the Ray or Python runtimes. Mapping the tutorial to byte arrays would use the  AbstractBinaryTransform instead of <code>AbstractTableTransform</code> (a sub-class of the former). Mapping the tutorial to a Spark runtime would use  AbstractSparkTransform instead of <code>AbstractTableTransform</code> and use <code>DataFrame</code> instead of pyarrow Table as the <code>DATA</code> type.  In addition, the  SparkTransformLauncher would be used in place of the <code>RayTransformLauncher</code> and <code>PythonTransformLauncher</code> shown below.</p> <p>That said, we will show the following:</p> <ul> <li>How to write the noop transform to generate the output table.</li> <li>How to define transform-specific metadata that can be associated   with each table transformation and aggregated across all transformations   in a single run of the transform.</li> <li>How to define command line arguments that can be used to configure   the operation of our noop transform.</li> </ul> <p>We will not be showing the following: * The creation of a custom <code>TransformRuntime</code> that would enable more global   state and/or coordination among the transforms running in other Ray actors.   This will be covered in an advanced tutorial.</p> <p>The complete task involves the following: * <code>noop_main.py</code> - a empty file to start writing code as described below  * <code>NOOPTransform</code> - class that implements the specific transformation * <code>NOOPTableTransformConfiguration</code> - class that provides configuration for the   <code>NOOPTransform</code>, specifically the command line arguments used to configure it. * <code>main()</code> - simple creation and use of the <code>TransformLauncher</code>.</p> <p>(Currently, the complete code for the noop transform used for this tutorial can be found in the noop transform directory.</p> <p>Finally, we show how to use the command line to run the transform in a local ray cluster.</p> <p>Note: You will need to run the setup commands in the <code>README</code> before running the following examples.</p>"},{"location":"data-processing-lib/doc/simplest-transform-tutorial/#nooptransform","title":"<code>NOOPTransform</code>","text":"<p>First, let's define the transform class.  To do this we extend the base abstract/interface class <code>AbstractTableTransform</code>, which requires definition of the following:</p> <ul> <li>an initializer (i.e. <code>init()</code>) that accepts a dictionary of configuration   data.  For this example, the configuration data will only be defined by   command line arguments (defined below).</li> <li>the <code>transform()</code> method itself that takes an input table and produces an output   table with any associated metadata for that table transformation.</li> </ul> <p>Other methods such as <code>flush()</code> need not be overridden/redefined for this simple example.</p> <p>We start with the simple definition of the class, its initializer and the imports required by subsequent code:</p> <p><pre><code>import time\nfrom argparse import ArgumentParser, Namespace\nfrom typing import Any\n\nimport pyarrow as pa\nfrom data_processing_ray.runtime.ray import RayTransformLauncher\nfrom data_processing_ray.runtime.ray.runtime_configuration import (\n  RayTransformRuntimeConfiguration,\n)\nfrom data_processing.transform import AbstractTableTransform, TransformConfiguration\nfrom data_processing.utils import CLIArgumentProvider, get_logger\n\n\nclass NOOPTransform(AbstractTableTransform):\n\n  def __init__(self, config: dict[str, Any]):\n    self.sleep = config.get(\"sleep\", 1)\n</code></pre> The <code>NOOPTransform</code> class extends the <code>AbstractTableTransform</code>, which defines the required methods.</p> <p>For purposes of the tutorial and to simulate a more complex processing job, our initializer allows our transform to be configurable with an amount of seconds to sleep/delay during the call to <code>transform()</code>. Configuration is provided by the framework in a dictionary provided to the initializer. Below we will cover how this <code>sleep</code> argument is made available to the initializer.</p> <p>Note that in more complex transforms that might, for example, load a Hugging Face or other model, or perform other deep initializations, these can be done in the initializer.</p> <p>Next we define the <code>transform()</code> method itself, which includes the addition of some almost trivial metadata.</p> <p><pre><code>    def transform(self, table: pa.Table, file_name: str = None) -&gt; tuple[list[pa.Table], dict[str, Any]]:\n        if self.sleep is not None:\n            time.sleep(self.sleep)\n        # Add some sample metadata.\n        metadata = {\"nfiles\": 1, \"nrows\": len(table)}\n        return [table], metadata\n</code></pre> The single input to this method is the in-memory pyarrow table to be transformed. The return value of this method is a list of tables and optional metadata.  In this case, we are doing a simple 1:1 table conversion, so the list will contain a single table, the input table. The metadata is a free-form dictionary of keys with numeric values that will be aggregated by the framework and reported as aggregated job statistics metadata. If there is no metadata then simply return an empty dictionary.</p>"},{"location":"data-processing-lib/doc/simplest-transform-tutorial/#nooptransformconfiguration","title":"<code>NOOPTransformConfiguration</code>","text":"<p>Next we define the <code>NOOPTransformConfiguration</code> class and its initializer that defines the following:</p> <ul> <li>The short name for the transform</li> <li>The class implementing the transform - in our case <code>NOOPTransform</code></li> <li>Command line argument support.</li> </ul> <p>We also define the <code>NOOPRayTransformationConfiguration</code> so we can run the transform in the Ray runtime as well.  This adds allows the option to add a transform-specific Ray runtime class allowing more complex distributed memory and data sharing operations. The NOOP transform will not make use of this so is a simple extension.:</p> <p>First we define the pure python transform configuration  class and its initializer,</p> <pre><code>short_name = \"noop\"\ncli_prefix = f\"{short_name}_\"\nsleep_key = \"sleep_sec\"\npwd_key = \"pwd\"\nsleep_cli_param = f\"{cli_prefix}{sleep_key}\"\npwd_cli_param = f\"{cli_prefix}{pwd_key}\"\n\n\nclass NOOPTransformConfiguration(TransformConfiguration):\n    def __init__(self):\n        super().__init__(\n            name=short_name,\n            transform_class=NOOPTransform,\n            remove_from_metadata=[pwd_key],\n        )\n</code></pre> <p>The initializer extends the <code>TransformConfiguration</code> that provides simple capture of our configuration data and enables the ability to pickle through the network. It also adds a <code>params</code> field that will be used below to hold the transform's configuration data (used in <code>NOOPTransform.init()</code> above).</p> <p>Next, we provide two methods that define and capture the command line configuration that is specific to the <code>NOOPTransform</code>, in this case the parameters are the number of seconds to sleep during transformation and an example command line parameter, <code>pwd</code> (\"password\"), option holding sensitive data that we don't want reported in the job metadata produced by the Ray orchestrator.</p> <p>The first method establishes the command line arguments. It is given a global argument parser to which the <code>NOOPTransform</code> arguments are added. It is a good practice to include a common prefix to all transform-specific options (i.e. pii, lang, etc). In our case we will use <code>noop_</code>.</p> <p><pre><code>    def add_input_params(self, parser: ArgumentParser) -&gt; None:\n        parser.add_argument(\n            f\"--{sleep_cli_param}\",\n            type=int,\n            default=1,\n            help=\"Sleep actor for a number of seconds while processing the data frame, before writing the file to COS\",\n        )\n        parser.add_argument(\n            f\"--{pwd_cli_param}\",\n            type=str,\n            default=\"nothing\",\n            help=\"A dummy password which should be filtered out of the metadata\",\n        )\n</code></pre> Next we implement a method that is called after the CLI args are parsed (usually by one of the runtimes) and which allows us to capture the <code>NOOPTransform</code>-specific arguments. </p> <pre><code>    def apply_input_params(self, args: Namespace) -&gt; bool:\n        captured = CLIArgumentProvider.capture_parameters(args, cli_prefix, False)\n        if captured.get(sleep_key) &lt; 0:\n            print(f\"Parameter noop_sleep_sec should be non-negative. you specified {args.noop_sleep_sec}\")\n            return False\n        self.params = captured\n        return True\n</code></pre>"},{"location":"data-processing-lib/doc/simplest-transform-tutorial/#runtime-launching","title":"Runtime Launching","text":"<p>To run the transform on a set of input data, we use one of the runtimes, each described below.</p>"},{"location":"data-processing-lib/doc/simplest-transform-tutorial/#python-runtime","title":"Python Runtime","text":"<p>To run in the python runtime, we need to create the instance of <code>PythonTransformLauncher</code> using the <code>NOOPTransformConfiguration</code>, and launch it as follows:</p> <pre><code>from data_processing.runtime.pure_python import PythonTransformLauncher\nif __name__ == \"__main__\":\n    launcher = PythonTransformLauncher(runtime_config=NOOPTransformConfiguration())\n    launcher.launch()\n</code></pre> <p>Assuming the above <code>main</code> code is placed in <code>noop_main.py</code> we can run the transform on some test data. We'll use data in the repo for the noop transform and create a temporary directory to hold the output: <pre><code>export DPK_REPOROOT=...\nexport NOOP_INPUT=$DPK_REPOROOT/transforms/universal/noop/python/test-data/input\n</code></pre> To run <pre><code>python noop_main.py --noop_sleep_sec 2 \\\n  --data_local_config \"{'input_folder': '\"$NOOP_INPUT\"', 'output_folder': '/tmp/noop-output'}\"\n</code></pre> See the python launcher options for a complete list of transform-independent command line options.</p>"},{"location":"data-processing-lib/doc/simplest-transform-tutorial/#ray-runtime","title":"Ray Runtime","text":"<p>To run in the Ray runtime, instead of creating the <code>PythonTransformLauncher</code> we use the <code>RayTransformLauncher</code>. as follows: <pre><code>class NOOPRayTransformConfiguration(RayTransformRuntimeConfiguration):\n    def __init__(self):\n        super().__init__(transform_config=NOOPTransformConfiguration())\n\nfrom data_processing_ray.runtime.ray import RayTransformLauncher\nif __name__ == \"__main__\":\n    launcher = RayTransformLauncher(runtime_config=NOOPRayTransformConfiguration())\n    launcher.launch()\n</code></pre> We can run this with the same command as for the python runtime but to run in local Ray add the <code>--run_locally True</code> option. <pre><code>python noop_main.py --noop_sleep_sec 2 \\\n  --data_local_config \"{'input_folder': '\"$NOOP_INPUT\"', 'output_folder': '/tmp/noop-output'}\" --run_locally True\n</code></pre> which will start local ray instance ( ray should be pre installed). See the ray launcher options for a complete list of transform-independent command line options.</p>"},{"location":"data-processing-lib/doc/simplified_transform_apis/","title":"Simplified APIs for invoking transforms","text":"<p>Current transform invocation, requires defining transform parameters, passing them to <code>sys.argv</code> and  then invoking launcher (runtime specific) with transform/runtime specific configuration (see for example NOOP Python invocation, NOOP Ray invocation and NOOP Spark invocation). </p> <p>Simplified APIs, described here make invocations a little simpler by eliminating the need for the boilerplate code. Currently we provide 2 APIs for simplified transform invocation: * execute_python_transform * execute_ray_transform</p> <p>Both APIs look the same, defining runtime in the API name and accept the following parameters: * transform name * transforms configuration object (see below) * input_folder containing data to be processed, currently can be local file system or S3 compatible * output_folder defining where execution results will be placed, currently can be local file system or S3 compatible * S3 configuration, required only for input/output folders in S3 * transform params - a dictionary of transform specific parameters</p> <p>APIs returns <code>True</code> if transform execution succeeds or <code>False</code> otherwise</p> <p>APIs implementation is leveraging TransformsConfiguration class which manages configurations of all existing transforms. By default transforms information is loaded from this json file, but can be overwritten by the user.</p> <p>Additionally configuration provides the method for listing existing (known) transforms.</p> <p>Finally, as configurator knows  about all existing transforms (and their dependencies) it checks whether transform is code is installed locally and install it if it is not (using  PipInstaller). If transforms are installed, they are removed after transform execution is complete.</p> <p>An example of the APIs usage can be found in this notebook.</p> <p>Creation and usage of configuration: </p> <pre><code>from data_processing.utils import TransformsConfiguration\n\nt_configuration = TransformsConfiguration()\ntransforms = t_configuration.get_available_transforms()\n</code></pre> <p>Invoking Python transform:</p> <pre><code>code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n\nruntime_python_params = {\n    \"runtime_pipeline_id\": \"pipeline_id\",\n    \"runtime_job_id\": \"job_id\",\n    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n}\ninput_folder = os.path.abspath(zip_input_folder)\noutput_folder =  os.path.abspath(parquet_data_output)\nsupported_languages_file = os.path.abspath(\"../../../transforms/code/code2parquet/python/test-data/languages/lang_extensions.json\")\n\ningest_config = {\n    \"data_files_to_use\": ast.literal_eval(\"['.zip']\"),\n    \"code2parquet_supported_langs_file\": supported_languages_file,\n    \"code2parquet_detect_programming_lang\": True,\n}\n\nexecute_python_transform(\n    configuration = t_configuration,\n    name=\"code2parquet\",\n    input_folder=input_folder,\n    output_folder=output_folder,\n    params=runtime_python_params | ingest_config\n)    \n</code></pre> <p>In the fragment above, we first define Python runtime parameters. Most of them are defaulted, so their use is optional (only if we need to overwrite them). Then we define input/output folder and location of the support file. We also define code to parquet specific parameters and finally invoke the transform itself. Note here, that <code>runtime_python_params</code> can be defined once and then reused across several Python transform invocation.</p> <p>Invoking Python transform:</p> <pre><code>runtime_ray_params = {\n    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n    \"runtime_num_workers\": 3,\n    \"runtime_pipeline_id\": \"pipeline_id\",\n    \"runtime_job_id\": \"job_id\",\n    \"runtime_creation_delay\": 0,\n    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n}\n\nededup_config = {\n    \"ededup_hash_cpu\": 0.5,\n    \"ededup_num_hashes\": 2,\n    \"ededup_doc_column\": \"contents\",\n}\n\nexecute_ray_transform(\n    configuration = t_configuration,\n    name=\"ededup\",\n    input_folder=input_folder,\n    output_folder=output_folder,\n    params=runtime_ray_params | ededup_config\n)    \n</code></pre> <p>In the fragment above, we first define Ray runtime parameters. Most of them are defaulted, so their use is optional (only if we need to overwrite them). Then we define input/output folder and ededup specific parameters and finally  invoke the transform itself. Note here, that <code>runtime_ray_params</code> can be defined once and then reused across several Python transform invocation.</p>"},{"location":"data-processing-lib/doc/spark-launcher-options/","title":"Spark Launcher Command Line Options","text":"<p>A number of command line options are available when launching a transform using Spark.  </p> <p>The following is a current --help output (a work in progress) for  the <code>NOOPTransform</code> (note the --noop_sleep_sec and --noop_pwd options):</p> <pre><code>usage: noop_python_runtime.py [-h] [--noop_sleep_sec NOOP_SLEEP_SEC] [--noop_pwd NOOP_PWD] [--data_s3_cred DATA_S3_CRED] [--data_s3_config DATA_S3_CONFIG] [--data_local_config DATA_LOCAL_CONFIG] [--data_max_files DATA_MAX_FILES]\n                              [--data_checkpointing DATA_CHECKPOINTING] [--data_data_sets DATA_DATA_SETS] [--data_files_to_use DATA_FILES_TO_USE] [--data_num_samples DATA_NUM_SAMPLES] [--runtime_pipeline_id RUNTIME_PIPELINE_ID]\n                              [--runtime_job_id RUNTIME_JOB_ID] [--runtime_code_location RUNTIME_CODE_LOCATION]\n\nDriver for noop processing\n\noptions:\n  -h, --help            show this help message and exit\n  --noop_sleep_sec NOOP_SLEEP_SEC\n                        Sleep actor for a number of seconds while processing the data frame, before writing the file to COS\n  --noop_pwd NOOP_PWD   A dummy password which should be filtered out of the metadata\n  --data_s3_cred DATA_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access.\n                        access_key: access key help text\n                        secret_key: secret key help text\n                        url: optional s3 url\n                        region: optional s3 region\n                        Example: { 'access_key': 'access', 'secret_key': 'secret', \n                        'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud', \n                        'region': 'us-east-1' }\n  --data_s3_config DATA_S3_CONFIG\n                        AST string containing input/output paths.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': 's3-path/your-input-bucket', \n                        'output_folder': 's3-path/your-output-bucket' }\n  --data_local_config DATA_LOCAL_CONFIG\n                        ast string containing input/output folders using local fs.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': './input', 'output_folder': '/tmp/output' }\n  --data_max_files DATA_MAX_FILES\n                        Max amount of files to process\n  --data_checkpointing DATA_CHECKPOINTING\n                        checkpointing flag\n  --data_data_sets DATA_DATA_SETS\n                        List of sub-directories of input directory to use for input. For example, ['dir1', 'dir2']\n  --data_files_to_use DATA_FILES_TO_USE\n                        list of file extensions to choose for input.\n  --data_num_samples DATA_NUM_SAMPLES\n                        number of random input files to process\n  --runtime_pipeline_id RUNTIME_PIPELINE_ID\n                        pipeline id\n  --runtime_job_id RUNTIME_JOB_ID\n                        job id\n  --runtime_code_location RUNTIME_CODE_LOCATION\n                        AST string containing code location\n                        github: Github repository URL.\n                        commit_hash: github commit hash\n                        path: Path within the repository\n                        Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324', \n                        'path': 'transforms/universal/code' }\n</code></pre>"},{"location":"data-processing-lib/doc/spark-runtime/","title":"Spark Framework","text":"<p>The Spark runtime implementation is roughly based on the ideas from  here, here and here.  Spark itself is basically used for execution parallelization, but all data access is based on the framework's data access, thus preserving all the implemented features. At  the start of the execution, the list of files to process is obtained (using data access framework) and then split between Spark workers for reading actual data, its transformation and writing it back. The implementation is based on Spark RDD (For comparison of the three Apache Spark APIs:  RDDs, DataFrames, and Datasets see this  Databricks blog post) As defined by Databricks: <pre><code>RDD was the primary user-facing API in Spark since its inception. At the core, an RDD is an \nimmutable distributed collection of elements of your data, partitioned across nodes in your \ncluster that can be operated in parallel with a low-level API that offers transformations \nand actions.\n</code></pre> This APIs fits perfectly into what we are implementing. It allows us to fully leverage our  existing DataAccess APIs thus preserving all of the investments into flexible, reliable data  access. Additionally RDDs flexible low-level control allows us to work on partition level,  thus limiting the amount of initialization and set up. Note that in our approach transform's processing is based on either binary or parquet data,  not Spark DataFrames or DataSet. We are not currently supporting supporting these Spark APIs,  as they are not well mapped into what we are implementing.</p> <p>In our implementation we are using  pyspark.SparkContext.parallelize for running multiple transforms in parallel. We allow 2 options for specifying the number of partitions, determining  how many partitions the RDD should be divided into. See here for the explanation of this parameter: * If you specify a positive value of the parameter, Spark will attempt to evenly   distribute the data from seq into that many partitions. For example, if you have   a collection of 100 elements and you specify numSlices as 4, Spark will try   to create 4 partitions with approximately 25 elements in each partition.  * If you don\u2019t specify this parameter, Spark will use a default value, which is   typically determined based on the cluster configuration or the available resources   (number of workers).</p>"},{"location":"data-processing-lib/doc/spark-runtime/#transforms","title":"Transforms","text":"<ul> <li>SparkTransformRuntimeConfiguration   allows to configure transform to use PySpark. In addition to its base class   TransformRuntimeConfiguration features,   this class includes <code>get_bcast_params()</code> method to get very large configuration settings. Before starting the   transform execution, the Spark runtime will broadcast these settings to all the workers.</li> </ul>"},{"location":"data-processing-lib/doc/spark-runtime/#runtime","title":"Runtime","text":"<p>Spark runtime extends the base framework with the following set of components: * SparkTransformExecutionConfiguration   allows to configure Spark execution * SparkTransformFileProcessor extends   AbstractTransformFileProcessor to work on   PySpark * SparkTransformLauncher allows   to launch PySpark runtime and execute a transform * orchestrate function orchestrates Spark   based execution</p>"},{"location":"data-processing-lib/doc/testing-e2e-transform/","title":"Testing End-to-End Transform operation","text":"<p>WIP - Points to discuss</p> <ol> <li>Reading input files and writing output files. </li> <li>Testing of the transform runtime and use of ray components in the transform</li> </ol>"},{"location":"data-processing-lib/doc/transform-exceptions/","title":"Exceptions","text":"<p>A transform may find that it needs to signal error conditions. For example, if a referenced model could not be loaded or a given input data (e.g., pyarrow Table) does not have the expected format (.e.g, columns). In general, it should identify such conditions by raising an exception.  With this in mind, there are two types of exceptions:</p> <ol> <li>Those that would not allow any data to be processed (e.g. model loading problem).</li> <li>Those that would not allow a specific datum to be processed (e.g. missing column).</li> </ol> <p>In the first situation the transform should throw an  unrecoverable exception, which will cause the runtime to terminate processing of all data.  Note: any exception thrown from <code>init</code> method of transform will cause runtime to  terminate processing</p> <p>In the second situation (identified in the <code>transform()</code> or <code>flush()</code> methods), the transform should throw an exception from the associated method. This will cause only the error-causing datum to be ignored and not written out, but allow continued processing of tables by the transform. In both cases, the runtime will log the exception as an error.</p>"},{"location":"data-processing-lib/doc/transform-external-resources/","title":"Support for external resources","text":"<p>Often when implementing a transform, the transform will require loading its own resources  (e.g. models, configuration, etc.) to complete its job.  For example, the Blocklist transform loads a list of domains to block.  Resources can be loaded from either S3 or local storage or a  custom location defined by the transform (i.e. hugging face, etc). In addition to actually loading the resource(s), the transform needs to define the configuration that  defines the location of the domain list. </p> <p>In the next sections we cover the following:</p> <ol> <li>How to define the transform-specific resource location(s) as command line arguments</li> <li>How to load the transform-specific resources, either or both of:<ol> <li>During transform initialization - this is useful for testing outside of ray, and optionally</li> <li>During transform configuration in the Ray runtime.  This may not be feasible if a resource       is not picklable.</li> </ol> </li> </ol>"},{"location":"data-processing-lib/doc/transform-external-resources/#defining-transform-specific-resource-locations","title":"Defining Transform-specific Resource Locations","text":"<p>Each transform has a configuration class that defines the command line options with which the transform can be configured.  In the example below, the  DataAccessFactory  is used in the configuration to add transform-specific arguments that allow a <code>DataAccessFactory</code> to be initialized specifically for the transform.  The initialized <code>DataAcessFactory</code> is then made available to the transform's initializer to enable it to read from transform-specific location.  Note that you may choose not to use the DataAccessFactory and might have your own mechanism for loading a  resource (for example, to load a hugging face model).  In this case you will define CLI arguments that allow you to configure where the resources is located. The implementation using DataAccessFactory looks as follows (the code here is from block listing):</p> <p><pre><code>class BlockListTransformConfiguration(DefaultTransformConfiguration):\n    ...\n    def add_input_params(self, parser: argparse.ArgumentParser) -&gt; None:\n        \"\"\"\n        Add Transform-specific arguments to the given parser.\n        This will be included in a dictionary used to initialize the BlockListTransform.\n        By convention a common prefix should be used for all mutator-specific CLI args\n        (e.g, noop_, pii_, etc.)\n        \"\"\"\n\n        ...\n\n       # The DataAccess created by the DataAccessFactory below will use this CLI arg value \n       parser.add_argument(\n            f\"--{blocked_domain_list_path_key}\",\n            type=str,\n            required=False,\n            default=blocked_domain_list_path_default,\n            help=\"S3/COS URL or local folder (file or directory) that points to the list of block listed domains.\"\n        )  \n        # Create the DataAccessFactor to use CLI args with the given blocklist prefix.\n        self.daf = DataAccessFactory(f\"{arg_prefix}_\")\n        # Add the DataAccessFactory parameters to the transform's configuration parameters.\n        self.daf.add_input_params(parser)\n</code></pre> We are creating the <code>DataAccessFactory</code> using a transform-specific prefix to define the transform-specific command line options to configure the  transform's factory instance. In this case, all the transform's DataAccessFactory parameters are prepended with  <code>blocklist_</code>, (<code>arg_prefix</code>).  For example <code>blocklist_s3_cred</code>.</p> <p>After configuring the command line argument parser above,  The BlocklistConfiguration <code>apply_input_params()</code> is implemented to capture all  <code>blocklist_</code> prefixed parameters and apply the arguments  to the DataAccessFactory. In addition, it adds the factory to the parameters that will be made available to the transform. In this way, the transform initializer will receive the DataAccessFactory created and initialized by the configuration instance.</p> <p><pre><code>    def apply_input_params(self, args: argparse.Namespace) -&gt; bool:\n        # Capture the args that are specific to this transform\n        ...\n\n        # Add the DataAccessFactory to the transform's configuration parameters.\n        self.params[block_data_factory_key] = self.daf\n        # mark this parameter to be removed\n        self.remove_from_metadata.append(block_data_factory_key)\n        # Validate and populate the transform's DataAccessFactory \n        return self.daf.apply_input_params(args)\n</code></pre> Note here, that as daf can contain secrets we do not want him to show up in the execution metadata, we add its key to the <code>self.remove_from_metadata</code> array. All the keys contained in this array will be removed from metadata. This  can also be very usefull for any keys cantaining sensitive information, for example, secrets, passwords, etc.</p> <p>The above code can be run in a non-ray main() as follows:  <pre><code>if __name__ == \"__main__\":\n    parser = ArgumentParser()\n    bltc = BlockListTransformConfiguration()\n    bltc.add_input_params(parser)\n    args = parser.parse()\n    config = bltc.apply_input_params(args) \n    transform = BlockListTransform(config)\n    ...\n</code></pre></p>"},{"location":"data-processing-lib/doc/transform-external-resources/#when-and-where-to-load-the-additional-resources","title":"When and Where to Load the Additional Resources","text":"<p>With a DataAccessFactory established, it can be used in either the transform's Runtime class when running in Ray, or in the transform's initializer to load the resource(s). These two approaches have the following considerations:</p> <ul> <li>Loading in transform itself: <ul> <li>Advantages<ul> <li>enables debugging without the need for a remote debugger to attach to the Ray worker.</li> <li>simplifies local testing, especially if a transform itself can be tested locally.</li> <li>can load any resource regardless of its picklability (irrelevant for data, but relevant for models).</li> </ul> </li> <li>Disadvantages<ul> <li>can create additional load on external resources, for example S3 or external web site.</li> </ul> </li> </ul> </li> <li>Loading in the Ray runtime, storing it plasma (Ray object storage), delivering it to the  transform via pointer:<ul> <li>Advantages<ul> <li>minimises load on external resources, for example S3 or external web site</li> </ul> </li> <li>Disadvantages<ul> <li>can be problematic if the resource/model is not picklable</li> <li>makes it slightly more complex for testing as loading is done in a process separate from the launcher</li> </ul> </li> </ul> </li> </ul> <p>With the above in mind, we recommend at least loading the resource(s) in the transform's initializer. This will ease debugging.  If load is an issue and the resource is picklable, then ALSO implement loading in the transform's Runtime. Next we show how to load resources using both approaches.</p>"},{"location":"data-processing-lib/doc/transform-external-resources/#loading-in-the-transform-initializer","title":"Loading in the Transform Initializer","text":"<p>If you decide to implement resource loading in the transform itself, you can do this in the init method of the transform class.  Let's look at the implementation, based on block listing) example. The code below demonstrates loading of data.</p> <p><pre><code>    # Get the DataAccessFactory we created above in the configuration\n    daf = config.get(block_data_factory_key)\n    if daf is None:\n        raise RuntimeError(f\"Missing configuration value for key {block_data_factory_key}\")\n    data_access = daf.create_data_access()\n    url = config.get(blocked_domain_list_path_key)\n    if url is None:\n        raise RuntimeError(f\"Missing configuration value for key {blocked_domain_list_path_key}\")\n    domain_list = get_domain_list(url, data_access)\n</code></pre> Note that alternatively,  if you are downloading data/models from the same source as the data itself, you can use <code>data_access</code> key to get the DataAccess object used to read/write the data.</p>"},{"location":"data-processing-lib/doc/transform-external-resources/#loading-in-the-transform-runtime","title":"Loading in the Transform Runtime","text":"<p>If you decide to implement resource loading in the transform runtime,  you must implement a custom transform runtime class, in particular, to  implement the <code>get_transform_config()</code> method that produces the configuration for the transform an in this example, load the domain list and store a Ray reference in the configuration.  Let's look at the implementation, based on the block listing) transform.  First define the initializer() which must accept a dictionary of parameters as generally will be defined by the configuration and its CLI parameters.</p> <p><pre><code>class BlockListRuntime(DefaultTableTransformRuntime):\n\n    def __init__(self, params: dict[str, Any]):\n        super().__init__(params)\n</code></pre> Once this is done, you need to at least implement <code>get_transform_config</code> method, which is called by the Ray orchestrator to establish the transform configuration parameters passed to the Ray Worker that then creates the transform instance using the provided  configuation parameters.  In short, these are the parameters that will be used to configure your transform in the Ray worker.</p> <p><pre><code>    def get_transform_config(\n        self, data_access_factory: DataAccessFactory, statistics: ActorHandle, files: list[str]\n    ) -&gt; dict[str, Any]:\n        # create the list of blocked domains by reading the files at the conf_url location\n        url = self.params.get(blocked_domain_list_path_key, None)\n        if url is None:\n            raise RuntimeError(f\"Missing configuration key {blocked_domain_list_path_key}\")\n        blocklist_data_access_factory = self.params.get(block_data_factory_key, None)\n        if blocklist_data_access_factory is None:\n            raise RuntimeError(f\"Missing configuration key {block_data_factory_key}\")\n        # Load domain list \n        domain_list = get_domain_list(url, blocklist_data_access_factory.create_data_access())\n        # Store it in Ray object storage\n        domain_refs = ray.put(domain_list)\n        # Put the reference in the configuration that the transform initializer will use.\n        return {domain_refs_key: domain_refs} | self.params\n</code></pre> In the implementation above, we do something very similiar to what was done in the transform initializer, except that here we store the loaded resource (i.e. the domain list) in Ray global memory and place the key for the stored object in the configuration.  This way the transform initializer can first look for this key and if found, avoid loading the domain list itself.</p> <p>Alternatively, if you are downloading resources from the same source as  the data itself, you can use input parameter <code>data_access_factory</code> to create data access  and download everything that you need using it.</p> <p>Finally, and as mentioned above, the transform's initializer looks to see if the key to the domain list is present and uses it instead of loading the domain list itself.</p> <pre><code>        runtime_provided_domain_refs = config.get(domain_refs_key, None)\n        if runtime_provided_domain_refs is None:\n            # this is useful during local debugging and testing without Ray\n            url = config.get(blocked_domain_list_path_key, None)\n            if url is None:\n                raise RuntimeError(f\"Missing configuration value for key {annotation_column_name_key}\")\n            daf = config.get(block_data_factory_key, None)\n            if url is None:\n                raise RuntimeError(f\"Missing configuration value for key {block_data_factory_key}\")\n            data_access = daf.create_data_access()\n            domain_list = get_domain_list(url, data_access)\n        else:\n            # This is recommended for production approach. In this case domain list is build by the\n            # runtime once, loaded to the object store and can be accessed by actors without additional reads\n            try:\n                domain_list = ray.get(runtime_provided_domain_refs)\n            except Exception as e:\n                logger.info(f\"Exception loading list of block listed domains from ray object storage {e}\")\n                raise RuntimeError(f\"exception loading from object storage for key {runtime_provided_domain_refs}\")\n</code></pre>"},{"location":"data-processing-lib/doc/transform-external-resources/#conclusion","title":"Conclusion","text":"<p>Generally, although both resource loading approaches can be used, we recommend  always implementing loading in the transform initializer, and if desired, loading in the transform  runtime if feasible (picklable, etc) or desirable for other reasons such as network bandwidth.</p>"},{"location":"data-processing-lib/doc/transform-runtimes/","title":"Transform Runtimes","text":"<p>Runtimes provide a mechanism to run a transform over a set of input files to produce a set of output files.  Each runtime is started using a launcher. The available runtimes are as follows: </p> <ul> <li>Python runtime - provides single process running of a transform.</li> <li>Ray runtime - provides running transforms across multiple Ray workers to   achieve highly scalable processing.</li> <li>Spark runtime - provides running spark-based transforms in a spark cluster.</li> </ul>"},{"location":"data-processing-lib/doc/transform-s3-testing/","title":"Testing transforms with S3","text":"<p>For testing transforms with S3 we are using Minio, which can be installed on Linux, macOS and Windows. Here we are assuming Mac usage, refer to documentation above for other platforms.</p>"},{"location":"data-processing-lib/doc/transform-s3-testing/#installing-minio","title":"Installing Minio","text":"<p>The simplest way to install Minio on Mac is using Homebrew. Use the following command:</p> <pre><code>brew install minio/stable/minio\n</code></pre> <p>In addition to the Minio server install the latest stable MinIO cli using</p> <p><pre><code>brew install minio/stable/mc\n</code></pre> Now you can start Minio server using the following command:</p> <pre><code>minio server start\n</code></pre> <p>When it starts you can connect to the server UI using the following address: <code>http://localhost:9000</code> The default user name/password is <code>minioadmin|minioadmin</code></p>"},{"location":"data-processing-lib/doc/transform-s3-testing/#populating-minio-with-testing-data","title":"Populating Minio with testing data","text":"<p>Populating Minio server with test data can be done using <code>mc</code>. First configure mc to work with the local Minio server:</p> <pre><code>mc alias set local http://127.0.0.1:9000 minioadmin minioadmin\n</code></pre> <p>This set an alias <code>local</code> to 'mc' connected to the local Minio server instance. Now we can use our mc instance to populate server using a set of commands provided by <code>mc</code>.</p> <p>First test the connection to the newly added MinIO deployment using the <code>mc admin info</code> command:</p> <pre><code>mc admin info local\n</code></pre> <p>To copy the data to Minio, you first need to create a bucket:</p> <pre><code>mc mb local/test\n</code></pre> <p>Once the bucket is created, you can copy files (assuming you are in the transforms directory), using:</p> <pre><code>mc cp --recursive tools/ingest2parquet/test-data/input/ local/test/ingest2parquet/input\nmc cp --recursive code/code_quality/test-data/input/ local/test/code_quality/input\nmc cp --recursive code/proglang_select/test-data/input/ local/test/proglang_select/input\nmc cp --recursive code/proglang_select/test-data/languages/ local/test/proglang_select/languages\nmc cp --recursive code/malware/test-data/input/ local/test/malware/input\n\nmc cp --recursive language/doc_quality/test-data/input/ local/test/doc_quality/input\nmc cp --recursive language/lang_id/ray/test-data/input/ local/test/lang_id/input\n\nmc cp --recursive universal/blocklist/test-data/input/ local/test/blocklist/input\nmc cp --recursive universal/blocklist/test-data/domains/ local/test/blocklist/domains\nmc cp --recursive universal/doc_id/test-data/input/ local/test/doc_id/input\nmc cp --recursive universal/ededup/test-data/input/ local/test/ededup/input\nmc cp --recursive universal/fdedup/test-data/input/ local/test/fdedup/input\nmc cp --recursive universal/filter/test-data/input/ local/test/filter/input\nmc cp --recursive universal/noop/test-data/input/ local/test/noop/input\nmc cp --recursive universal/resize/test-data/input/ local/test/resize/input\nmc cp --recursive universal/tokenization/test-data/ds01/input/ local/test/tokenization/ds01/input\nmc cp --recursive universal/tokenization/test-data/ds02/input/ local/test/tokenization/ds02/input\n</code></pre> <p>Note, that once the data is copied, Minio is storing it on the local file system, so you do not need to copy it again after cluster restart</p>"},{"location":"data-processing-lib/doc/transform-s3-testing/#creating-access-and-secret-key-for-minio-access","title":"Creating access and secret key for Minio access","text":"<p>The last thing is to add Minio access and secret keys for accessing it. The following command:</p> <pre><code>mc admin user svcacct add --access-key \"localminioaccesskey\" --secret-key \"localminiosecretkey\" local minioadmin\n</code></pre> <p>creates both access and secret key for usage by the applications</p>"},{"location":"data-processing-lib/doc/transform-standalone-testing/","title":"Testing Transforms","text":"<p>A test framework is provided as part of the library in the <code>data_processing_test</code> package. Transform testing makes use of an <code>AbstractTransformTest</code> super-class that defines  the generic tests that should be common to all transforms. Initially this means testing the transformation of one or more in-memory  input tables to a one or more output tables and one or more metadata dictionaries.</p> <p>The <code>AbstractTransformTest</code> class defines the <code>test_*(...)</code> methods and makes use  of pytest fixtures to define test cases (i.e. inputs) applied to the test method(s). Each <code>test_*(...)</code> method (currently only one) has an associated abstract <code>get_test_*_fixtures()</code> method that must be implemented by the specific transform implementation test to define the various sets of inputs tested. This approach allows the definition of new generic transform tests that existing transform implementation tests will easily leverage.</p> <p>The first (currently only test) is a the <code>test_transform()</code> method that takes the following inputs:</p> <ul> <li>the transform implementation being tested, properly configured with the configuration dictionary for the associated test data.</li> <li>a list of N (1 or more) input tables to be processed with the transform's <code>transform(Table)</code> method.</li> <li>The expected list of accumulated tables across the N calls to  <code>transform(Table)</code> and the single finalizing call to the transform's <code>flush()</code> method. In the case where the <code>transform()</code> returns an empty list, no associated expected Table  should be included in this list. </li> <li>The expected list of accumulated metadata dictionaries across the N calls to <code>transform(Table)</code>   and the single finalizing call to the transform's <code>flush()</code> method.  This list should be of length N+1 for the N calls to <code>transform(Table)</code> plus the finalizing call to <code>flush()</code>.</li> </ul> <p>As an example, consider the <code>NOOPTransformTest</code> developed as ane example of the testing framework.</p> <p><pre><code>from typing import Tuple\n\nimport pyarrow as pa\nfrom data_processing.test_support import AbstractTransformTest\nfrom noop_transform import NOOPTransform\n\n# Define the test input and expected outputs\ntable = pa.Table.from_pydict({\"name\": pa.array([\"Tom\"]), \"age\": pa.array([23])})\nexpected_table = table  # We're a noop after all.\nexpected_metadata_list = [{\"nfiles\": 1, \"nrows\": 1}, {}]  # transform() result  # flush() result\n\n\nclass TestNOOPTransform(AbstractTransformTest):\n\n  # Define the method that provides the test fixtures to the test from the super class.\n  def get_test_transform_fixtures(self) -&gt; list[Tuple]:\n    fixtures = [\n      (NOOPTransform({\"sleep\": 0}), [table], [expected_table], expected_metadata_list),\n      (NOOPTransform({\"sleep\": 1}), [table], [expected_table], expected_metadata_list),\n    ]\n    return fixtures\n</code></pre> In the above we use the <code>NOOPTransform</code> to process the single input <code>table</code>, to produce the expected table <code>expected_table</code> and list of metadata in <code>expected_metadata_list</code>,  The NOOPTransform has no configuration that effects the transformation of input to output. However, in general this will not be the case and a transform may have different configurations and associated test data.  For example, a transform might be configurable to use different models and perhaps as a result have different results. </p> <p>Once the test class is defined you may run the test from your IDE or from the command line...  <pre><code>% cd .../data-prep-kit/transforms/universal/noop/src\n% make venv\n% source venv/bin/activate\n(venv)% export PYTHONPATH=.../data-prep-kit/transforms/universal/noop/src\n(venv)% pytest test/test_noop.py \n================================================================================ test session starts ================================================================================\nplatform darwin -- Python 3.10.11, pytest-8.0.2, pluggy-1.4.0\nrootdir: /Users/dawood/git/data-prep-kit/transforms/universal/noop\nplugins: cov-4.1.0\ncollected 2 items                                                                                                                                                                   \n\ntest/test_noop.py ..                                                                                                                                                          [100%]\n\n================================================================================= 2 passed in 0.83s =================================================================================\n(venv) % \n</code></pre> Finally, the tests should be runnable with the <code>Makefile</code>  as follows: <pre><code>$ make test\nsource venv/bin/activate;           \\\n    export PYTHONPATH=../src:.:$PYTHONPATH; \\\n    cd test; pytest . \n========================================================================================== test session starts ==========================================================================================\nplatform darwin -- Python 3.10.11, pytest-8.0.2, pluggy-1.4.0\nrootdir: /Users/dawood/git/data-prep-kit/transforms/universal/noop/test\ncollected 3 items                                                                                                                                                                                       \n\ntest_noop.py ..                                                                                                                                                                                   [ 66%]\ntest_noop_launch.py .                                                                                                                                                                             [100%]\n\n========================================================================================== 3 passed in 17.15s ===========================================================================================\n$\n</code></pre> Note that the venv was activated for you as part of running the test.</p>"},{"location":"data-processing-lib/doc/transform-testing/","title":"Transform Testing","text":"<p>Once a transform has been built, testing can be enabled with the provided testing framework:</p> <ul> <li>Standalone Transform Testing - shows how to test a transform independent of the runtime.</li> <li>End-to-End Testing - shows how to test the transform running in a runtime.</li> <li>Testing Transforms with S3 -shows how to set up <code>minio</code> to test transforms with the S3 interfaces.</li> </ul>"},{"location":"data-processing-lib/doc/transform-tutorial-examples/","title":"Tutorial Examples","text":"<p>With these basic concepts in mind, we start with a simple example and  progress to more complex transforms.  Before getting started  you may want to consult the  transform project root readme documentation.</p> <ul> <li>Simplest transform -  Here we will take a simple example to show the basics of creating a simple transform that takes a single input Table, and produces a single Table.</li> <li>External resources transform - shows how to load additional resources (models, configuration, etc) for a transform.</li> <li>Advanced transform</li> </ul>"},{"location":"data-processing-lib/doc/transformer-utilities/","title":"Transform Utilities","text":"<p>A class TransformUtils provides several methods that simplify  transformer's implementation. Currently, it includes the following methods:</p> <ul> <li><code>deep_get_size</code> is the method to get the complete size of the Python object based on   https://www.askpython.com/python/built-in-methods/variables-memory-size-in-python   It supports Python structures: list, tuple and set</li> <li><code>normalize_string</code> normalizes string, converting it to lowercase and removing spaces, punctuation and CR</li> <li><code>str_to_hash</code>convert string to 259 bit hash</li> <li><code>str_to_int</code> getting an integer representing string by calculating string's hash</li> <li><code>validate_columns</code> check whether required columns exist in the table</li> <li><code>add_column</code> adds column to the table avoiding duplicates. If the column with the given name already exists it will  be removed before it is added</li> <li><code>validate_path</code> cleans up s3 path - Removes white spaces from the input/output paths   removes schema prefix (s3://, http:// https://), if exists   adds the \"/\" character at the end, if it doesn't exist   removes URL encoding</li> </ul> <p>It also contains two variables:</p> <ul> <li><code>RANDOM_SEED</code> number that is used for methods that require seed</li> <li><code>LOCAL_TO_DISK</code> rough local size to size on disk/S3</li> </ul> <p>This class should be extended with additional methods, generally useful across multiple transformers and documentation  should be added here </p>"},{"location":"data-processing-lib/doc/transforms/","title":"Transforms","text":"<p>Transform is a basic integration unit of DPK that can be executed in any of the supported by the DPK  runtimes (Python, Ray and Spark). All transforms  are derived from the  AbstractTransform class. Theis class provides no functionality and is used as just a marker that a given class implements transform. There are currently two types of transforms defined in DPK:</p> <ul> <li>AbstractBinaryTransform which is a base  class for all data transforms. Data transforms convert a file of data producing zero or more data files  and metadata. A specific class of the binary transform is  AbstractTableTransform that consumes and produces data files containing pyarrow tables</li> <li>AbstractFolderTransform which is a base class consuming a folder (that can contain an arbitrary set of files, that need to be processed together) and proces zero or more data files and metadata.</li> </ul> <p>In the discussion that follows, we'll focus on the transformation of pyarrow Tables using the <code>AbstractTableTransform</code> class (see below), supported by Ray Spark and Python runtimes.</p>"},{"location":"data-processing-lib/doc/transforms/#abstracttabletransform-class","title":"AbstractTableTransform class","text":"<p>AbstractTableTransform  is expected to be extended when implementing a transform of pyarrow Tables. In general, when possible a transform should be independent of the runtime in which it runs, and the mechanism used to define its configuration (e.g., the <code>TransformConfiguration</code> class below, or other mechanism). That said, some transforms may require facilities provided by the runtime (shared memory, distribution, etc.), but as a starting point, think of the transform as an independent operator.</p> <p>The following methods are defined:</p> <ul> <li><code>__init__(self, config:dict)</code> - an initializer through which the transform can be created  with implementation-specific configuration.  For example, the location of a model, maximum number of rows in a table, column(s) to use, etc.   Error checking of configuration should be done here.</li> <li><code>transform(self, table:pyarrow.Table) -&gt; tuple(list[pyarrow.Table], dict)</code> - this method is responsible for the actual transformation of a given table to zero or more output tables, and optional  metadata regarding the transformation applied.  Zero tables might be returned when merging tables across calls to <code>transform()</code> and more than 1 table might be returned when splitting tables by size or other criteria.</li> <li>output tables list - the RayWork handles the various number of returned tables as follows: <ul> <li>0 - no file will be written out and the input file name will not be used in the output directory.</li> <li>1 - one parquet file will be written to the output directory with </li> <li>N - N parquet files are written to the output with <code>_&lt;index&gt;</code> appended to the base file name</li> </ul> </li> <li>dict - is a dictionary of transform-specific data keyed to numeric values.  A statistics component will          accumulate/add dictionaries across all calls to transform across all calls to all transforms running          in a given runtime (see below). As an example, a          transform might wish to track the number of instances of PII entities detected and might return           this as <code>{ \"entities\" : 1234 }</code>.</li> <li><code>flush() -&gt; tuple(list[pyarrow.Table], dict)</code> - this is provided for transforms that make use of buffering (e.g. to resize the tables) across calls  to <code>transform()</code> and need to be flushed of all buffered data at the end of processing of input tables. The return values are handled the same waa as the return values for <code>transform()</code>.  Since most transforms will likely not need this feature, a default implementation is provided to return an empty list and empty dictionary.</li> </ul>"},{"location":"data-processing-lib/doc/transforms/#transformconfiguration-class","title":"TransformConfiguration class","text":"<p>The TransformConfiguration serves as an interface and must be implemented by the any <code>AbstractTableTransform</code> implementation to enable running within and runtime or from a command line to capture  transform configuration.  It provides the following configuration:</p> <ul> <li>the transform class to be used,</li> <li>command line arguments used to initialize the Transform Runtime and generally, the Transform.</li> <li>Transform Runtime class to use</li> <li>transform short name </li> </ul> <p>It is expected that transforms are initialized with a fixed name, the class of its corresponding <code>AbstractTableTransform</code> implementation and optionally the configuration keys that should not be exposed as metadata for a run. To support command line configuration, the <code>TransformConfiguration</code> extends the CLIArgumentProvider class. The set of methods of interest are</p> <ul> <li><code>__init__(self, name:str, transform_class:type[AbstractTableTransform], list[str]:remove_from_metadata )</code> - sets the required fields</li> <li><code>add_input_params(self, parser:ArgumentParser)</code> - adds transform-specific command line options that will be made available in the dictionary provided to the transform's initializer.</li> <li><code>apply_input_params(self, args: argparse.Namespace)</code> - verifies  and captures the relevant transform parameters.</li> <li><code>get_input_params(self ) -&gt; dict[str,Anny]</code> - returns the dictionary of configuration values that should be used to initialize the transform.</li> </ul>"},{"location":"data-processing-lib/python/","title":"Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently parquet files are supported - and running them in a ray cluster. Data files may be stored in the local file system or  COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/python/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/python/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre></p> <p>To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-processing-lib/ray/","title":"Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently parquet files are supported - and running them in a ray cluster. Data files may be stored in the local file system or  COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/ray/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>cd data-processing-lib/ray/\nmake venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/ray/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre> To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-processing-lib/spark/","title":"Spark Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently any binary and parquet files are supported - and running them in a Spark cluster. Data files may be stored in the local file system or COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/spark/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/spark/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre> To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the automatically updated <code>pyproject.toml</code> file.</p>"},{"location":"doc/data-processing/","title":"Data Processing Using Multiple Transforms","text":"<p>The transformation framework is designed to operate on arbitrary arrays of bytes with a specialization for columnar data, generally contained in parquet files and read as pyarrow tables.</p> <p>In general, transforms can be written to process any type of binary data, to be interpreted by the transform.</p> <p>The table-specific transforms are written to process a table to, for example:</p> <ul> <li>Remove duplicates or non supported documents, for example, exact and fuzzy dedup, or language selection</li> <li>Transform files, for example coalesce or split files or add/remove some of the columns</li> <li>Annotate the tables to add additional data such as document quality score, language, etc.</li> <li>Derive statistics on data in one or more columns </li> <li>Filter the table to remove or edit rows and/or columns, for example to remove rows from blocked domains.</li> </ul> <p>The table is generally expected to have something like the following minimal set of columns :</p> <ul> <li>URL source of the document (can be use for domain block listing)</li> <li>Document id</li> <li>Contents of the actual document to be used for LLM training</li> </ul> <p>The following might be an example sequence of transforms (applied to tables, though the model is equivalent for binary data transformations)</p> <p></p> <p>The ordering of the transforms can change depending on the requirements on the training data. </p> <p>Each transform is generally run on a set of input files to produce a set of output files. The transforms are generally sequenced/stitched together,  each accepting the completed set of output files from a previous transform in the sequence. </p> <p></p> <p>Transforms are generally executed in one of the available  \"runtimes\" with scalability provided by the Ray and Spark runtimes.</p>"},{"location":"doc/google-colab/","title":"Running on Google Colab","text":"<p>Google Colab is free hosted Jupyter environment.</p> <p>Here are some tips for running data prep kit application code on Google Colab.</p>"},{"location":"doc/google-colab/#how-to-determine-if-we-are-running-on-colab","title":"How to Determine if We are Running on Colab?","text":"<p>Use one of these code snippets</p> <pre><code>import os\n\nif os.getenv(\"COLAB_RELEASE_TAG\"):\n   print(\"Running in Colab\")\n   RUNNING_IN_COLAB = True\nelse:\n   print(\"NOT in Colab\")\n   RUNNING_IN_COLAB = False\n</code></pre>"},{"location":"doc/google-colab/#installing-dependencies","title":"Installing Dependencies","text":"<p>We need to install data prep kit and libraries in Colab</p> <pre><code>if RUNNING_IN_COLAB:\n    ! pip install  --default-timeout=100  data-prep-toolkit-transforms-ray==0.2.1.dev3\n</code></pre>"},{"location":"doc/google-colab/#downloading-data-files-on-colab","title":"Downloading Data Files on Colab","text":"<pre><code>if RUNNING_IN_COLAB:\n    !mkdir -p 'input'\n    !wget -O 'input/1.pdf'  'remote_file_url'\n</code></pre>"},{"location":"doc/google-colab/#ray-runtime-settings","title":"Ray Runtime Settings","text":"<p>These are some recommended settings for running RAY based notebooks.  You can use these as starting points and tweak for your application</p> <pre><code>if RUNNING_IN_COLAB:\n  RAY_RUNTIME_WORKERS = 2\n  RAY_NUM_CPUS =  0.3\n  RAY_MEMORY_GB = 2  # GB\n</code></pre> <p>It is recommended to set cpu per worker (RAY_NUM_CPUS) to a low number.  Otherwise Ray jobs seem to hang and will not complete.</p>"},{"location":"doc/google-colab/#fuzzy-dedupe-settings","title":"Fuzzy Dedupe Settings","text":"<p>Start with the following settings before launching fuzzy dedupe job.</p> <p>Here is the infrastructure section for fuzzy dedupe.  Again we recommend to keep CPU share low.</p> <pre><code>    # infrastructure\n    \"fdedup_bucket_cpu\": 0.3,\n    \"fdedup_doc_cpu\": 0.3,\n    \"fdedup_mhash_cpu\": 0.3,\n    \"fdedup_num_doc_actors\": 1,\n    \"fdedup_num_bucket_actors\": 1,\n    \"fdedup_num_minhash_actors\": 1,\n    \"fdedup_num_preprocessors\": 1,\n</code></pre> <p>Here is full code for completeness</p> <pre><code>local_conf = {\n    \"input_folder\": input_folder,\n    \"output_folder\": output_folder,\n}\nworker_options = {\"num_cpus\" : RAY_NUM_CPUS}\n\nparams = {\n    # where to run\n    \"run_locally\": True,\n    # Data access. Only required parameters are specified\n    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n    # Orchestration parameters\n    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n    \"runtime_num_workers\": RAY_RUNTIME_WORKERS,\n    # columns used\n    \"fdedup_doc_column\": \"contents\",\n    \"fdedup_id_column\": \"int_id_column\",\n    \"fdedup_cluster_column\": \"hash_column\",\n    # infrastructure\n    \"fdedup_bucket_cpu\": 0.3,\n    \"fdedup_doc_cpu\": 0.3,\n    \"fdedup_mhash_cpu\": 0.3,\n    \"fdedup_num_doc_actors\": 1,\n    \"fdedup_num_bucket_actors\": 1,\n    \"fdedup_num_minhash_actors\": 1,\n    \"fdedup_num_preprocessors\": 1,\n    # fuzzy parameters\n    \"fdedup_num_permutations\": 64,\n    \"fdedup_threshold\": 0.7,\n    \"fdedup_shingles_size\": 5,\n    \"fdedup_delimiters\": \" \"\n}\n\n# Pass commandline params\nsys.argv = ParamsUtils.dict_to_req(d=params)\n\nlauncher = RayTransformLauncher(FdedupRayTransformConfiguration())\nreturn_code = launcher.launch()\n</code></pre>"},{"location":"doc/mac/","title":"Mac","text":""},{"location":"doc/mac/#execution-on-laptops-with-apple-silicon-cpu","title":"\u2757 Execution on laptops with Apple Silicon (CPU)","text":"<p>Starting with certain models introduced in late 2020, Apple began the transition from Intel processors to Apple silicon in Mac computers. These CPUs have ARM architecture and are incompatible with Intel processors. </p>"},{"location":"doc/mac/#transforms","title":"Transforms","text":"<p>Developing transforms for either the python or Ray runtimes, without KubeFlow pipelines (KFP), should have no issues on Apple silicon Macs, or other platforms for that matter. Therefore, to the extent the supported versions of python are used, transforms can be developed that will run on Apple silicon Macs. </p>"},{"location":"doc/mac/#virtualization-considerations","title":"Virtualization Considerations","text":"<p>Desktops such as Docker Desktop, Podman desktop and Rancher desktop use different virtualization and emulation techniques, (qemu, Apple Virtualization framework) to allow the execution of containers based on images compiled for Intel silicon. However, emulation significantly impacts performance, and there are additional restrictions, such as Virtual Machine RAM size.</p> <p>On the other hand, executing a Kind Kubernetes cluster with KubeFlow pipelines (KFP) and local data storage (Minio) requires a significant amount of memory. For this initial Data Prep Kit release, we do not recommend local (Kind) execution on Mac computers with Apple silicon. Instead, we suggest using a real Kubernetes cluster or a Linux virtual machine with an Intel CPU.</p>"},{"location":"doc/mac/#memory-considerations","title":"Memory Considerations","text":"<p>To verify that running transforms through KFP does not leak memory and also get an idea on the required Podman VM memory size configuration, a few tests were devised and run, as summarized here.</p> <p>Note: the current release does not support building cross-platform images, therefore, please do not build images  on the Apple silicon. </p>"},{"location":"doc/memory/","title":"Memory","text":""},{"location":"doc/memory/#memory-and-endurance-considerations","title":"Memory and Endurance Considerations","text":"<p>A test was devised with a set of 1483 files on a Mac with 32GB memory and 4CPU cores. Traceback library was used to check for memory leak.  10 iterations were run and the memory usage was observed, which peaked around 4 GB. There were no obvious signs of a memory leak. </p> <p>Another set of tests was done with the 1483 files on a podman VM with different memory configurations. The results are shown below. It seems that it needed around 4GB of available memory to run successfully for all 1483 files.</p> CPU Cores Total Memory Memory Used by Ray Transform Files Processed Successfully 4 8GB 4.2GB NOOP 1483 4 6GB 3GB NOOP 910 4 4GB 2GB NOOP 504"},{"location":"doc/repo/","title":"Repository Structure and Use","text":"<p>Here we discuss the structure, use and approach to code management in the repo.</p>"},{"location":"doc/repo/#setup","title":"Setup","text":"<p>There are various entry points that you can choose based on the use case. Each entry point has its pre-requirements and setup steps. The common part of are:</p>"},{"location":"doc/repo/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or 3.11  -Docker/Podman</li> </ul> <p>Two important development tools will also be installed using the steps below: - pre-commit - twine </p>"},{"location":"doc/repo/#installation-steps","title":"Installation Steps","text":"<p><pre><code>pip install pre-commit\npip install twine\n...\ngit clone git@github.com:IBM/data-prep-kit.git\ncd data-prep-kit\npre-commit install\n</code></pre> Please note that there are further installation steps  for running the transforms in general, as documented  here  and on a local Kind cluster or on an existing Kubernetes  cluster, as documented here.</p>"},{"location":"doc/repo/#repository-structure","title":"Repository structure","text":"<ul> <li> <p>data_processing_lib - provides the core transform framework and library  supporting data transformations in 3 runtimes</p> <ul> <li>python </li> <li>ray</li> <li>spark</li> </ul> </li> <li> <p>transform</p> <ul> <li>universal<ul> <li>noop </li> <li>python </li> <li>ray</li> <li>spark </li> <li>kfp_ray</li> <li>...</li> </ul> </li> <li>code<ul> <li>code_quality<ul> <li>ray</li> <li>kfp_ray</li> </ul> </li> <li>...</li> </ul> </li> <li>language<ul> <li>...</li> </ul> </li> </ul> </li> <li>kfp - Kubeflow pipeline support<ul> <li>kfp_support_lib - Data Preparation Kit Library. KFP support</li> <li>kfp_ray_components - Kubflow pipeline components used in the pipelines</li> </ul> </li> <li>scripts</li> </ul>"},{"location":"doc/repo/#build-and-makefiles","title":"Build and Makefiles","text":"<p>Makefiles are used for operations performed across all projects in the directory tree. There are two types of users envisioned to use the make files.  </p> <ul> <li>adminstrators - perform git actions and release management </li> <li>developers - work with core libraries and transforms</li> </ul> <p>Each directory has access to a <code>make help</code> target that will show all available targets.</p>"},{"location":"doc/repo/#administrators","title":"Administrators","text":"<p>Generally, administrators will issue make commands from the top of the repository to, for example publish a new release.  The top level make file provides a set of targets that  are executed recursively, which as a result are expected to be implementd by sub-directories.  These and their semantics are expected to be implemented, as appropriate, in the sub-directories are as follows:</p> <ul> <li>clean - Restore the directory to as close to initial repository clone state as possible. </li> <li>build - Build all components contained in a given sub-directory. This might include pypi distributions, images, etc.</li> <li>test -  Test all components contained in a given sub-directory. </li> <li>publish - Publish any components in sub-directory.  This might include things published to pypi or the docker registry.</li> <li>set-versions - apply the DPK_VERSION to all published components. </li> </ul> <p>Sub-directories are free to define these as empty/no-op targets, but generally are required to define them unless a parent directory does not recurse into the directory.</p>"},{"location":"doc/repo/#developers","title":"Developers","text":"<p>Generally, developers will be working in a python project directory (e.g., data-processing-lib/python, transforms/universal/filter, etc.)  and can issue the administrator's make targets (e g., build, test, etc) or others that might be defined locally (e.g., venv, test-image, test-src in transform projects). Key targets are as follows:</p> <ul> <li>venv -  creates the virtual environment from either a pyproject.toml or requirements.txt file.</li> <li>publish - publish libraries or docker images as appropriate. This is generally only used during release generation.</li> </ul> <p>If working with an IDE, one generally makes the venv, then configures the IDE to  reference the venv, src and test directories.</p> <p>Transform projects generally include these transform project-specific targets for convenience, which are triggered with the the <code>test</code> target.</p> <ul> <li>test-src - test python tests in the test directory</li> <li>test-image - build and test the docker image for the transform</li> </ul> <p>Please also consult transform project conventions for  additional considerations when developing transforms.</p>"},{"location":"doc/repo/#transforms-and-kfp","title":"Transforms and KFP","text":"<p>The kfp_ray directories in the transform projects provide  <code>workflow-</code> targets and are dedicated to handling the  Kubeflow Pipelines  workflows for the specified transforms.</p> <p>```</p>"},{"location":"doc/quick-start/new-transform-inside/","title":"Creating a New Transform in this Repository","text":"<p>An easy way could be to replicate noop transform, change class names as per your needs and add your business logic in noop_transform.py  Please see tutorials for more details </p>"},{"location":"doc/quick-start/new-transform-outside/","title":"Creating a New Transform","text":"<p>Here we show how to create a new transform outside of this repository using the Data Prep Kit libraries published on pypi.org. </p> <p>Before proceeding, you should review the  architectural overview for the details on  transforms and  runtimes. In addition, the  simple transform tutorial will be useful.</p> <p>Depending on the target runtime for the transform, you will need one of the following dependencies:</p> <ul> <li><code>data-prep-toolkit</code> - base transform and python runtime framework.</li> <li><code>data-prep-toolkit-ray</code> - ray extensions for ray runtime. Depends on <code>data-prep-toolkit</code>.</li> <li><code>data-prep-toolkit-spark</code> - spark extensions for spark runtime. Depends on <code>data-prep-toolkit</code>.</li> </ul> <p>Note the use of <code>-toolkit</code> instead of <code>-kit</code> due to name collisions on pypi.org.</p> <p>For this quickstart, we will implement a simple transform that adds a column  containing a string provided on the command line.</p>"},{"location":"doc/quick-start/new-transform-outside/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>To start, create a <code>requirements.txt</code> file containing the following <pre><code>data-prep-toolkit==0.2.0\n</code></pre> then <pre><code>python -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre></p>"},{"location":"doc/quick-start/new-transform-outside/#create-python-transform","title":"Create python transform","text":"<p>We create the following classes 1. <code>HelloTransform</code> - implements the table transformation 2. <code>HelloTransformConfiguration</code> - defines CLI arguments to configure HelloTransform</p> <pre><code>from typing import Any\nfrom data_processing.transform import AbstractTableTransform\nimport pyarrow as pa\nfrom data_processing.utils import TransformUtils\n\nclass HelloTransform(AbstractTableTransform):\n    '''\n    Extends the base table transform class to implement the configurable transform() method.\n    '''\n    def __init__(self, config:dict):\n        self.who = config.get(\"who\", \"World\")\n        self.column_name = config.get(\"column_name\", \"greeting\")\n\n    def transform(self, table: pa.Table, file_name: str = None) -&gt; tuple[list[pa.Table], dict[str, Any]]:\n        # Create a new column with each row holding the who value\n        new_column = [\"Hello \" + self.who + \"!\"] * table.num_rows\n        # Append the column to create a new table with the configured name.\n        table = TransformUtils.add_column(table=table, name=self.column_name, content=new_column)\n        return [table], {}\n\nimport argparse\nfrom data_processing.transform import  TransformConfiguration\n\nclass HelloTransformConfiguration(TransformConfiguration):\n    '''\n    Extends the configuration class to add command line configuration parameters for this transfomr\n    '''\n    def __init__(self):\n        super().__init__(\n            name=\"hello\",\n            transform_class=HelloTransform,\n        )\n\n    def add_input_params(self, parser: argparse.ArgumentParser) -&gt; None:\n        # Define the command line arguments used by this transform\n        # We use the ac_ CLI prefix to help distinguish from other runtime CLI parameters\n        parser.add_argument(\n            \"--ac_who\",\n            type=str, required=False, default=\"World\",\n            help=\"Who to say hello to.\"\n        )\n\n        parser.add_argument(\n            \"--ac_column_name\",\n            type=str, required=False, default=\"greeting\", help=\"Name of column to add\"\n        )\n\n    def apply_input_params(self, args: argparse.Namespace) -&gt; bool:\n        dargs = vars(args)\n        # Select this transform's command line arguments from all those\n        # provided on the command line.\n        self.params = {\n            \"who\": dargs.get(\"ac_who\",None),\n            \"column_name\": dargs.get(\"ac_column_name\",None)\n        }\n        return True\n</code></pre>"},{"location":"doc/quick-start/new-transform-outside/#python-runtime","title":"Python Runtime","text":"<p>To run the transform in the pure python runtime, we create  1. <code>HelloPythonTransformConfiguration</code> class - defines configuration for the python runtime 2. <code>main</code> - launches the python runtime with our transform.</p> <pre><code>from data_processing.runtime.pure_python import PythonTransformRuntimeConfiguration, PythonTransformLauncher\nfrom hello_transform import HelloTransformConfiguration\n\nclass HelloPythonConfiguration(PythonTransformRuntimeConfiguration):\n    '''\n    Configures the python runtime to use the Hello transform\n    '''\n    def __init__(self):\n        super().__init__(transform_config=HelloTransformConfiguration())\n\n\nif __name__ == \"__main__\":\n    # Create the runtime launcher to use the HelloTransform\n    launcher = PythonTransformLauncher(HelloPythonConfiguration())\n    launcher.launch()\n</code></pre>"},{"location":"doc/quick-start/new-transform-outside/#running","title":"Running","text":"<p>In the following, <code>parquet-tools</code> will be helpful.  Install with  <pre><code>% source venv/bin/activate\n(venv) % pip install parquet-tools\n</code></pre> We will the transform a single parquet file in a directory named  <code>input</code>. The directory may contain more than one parquet file,  in which case they will all be processed. We can examine the input as follows: <pre><code>% source venv/bin/activate\n(venv) % ls input\ntest1.parquet\n(venv) % parquet-tools show input/test1.parquet\n+-----------------------------------+\n| title                             |\n|-----------------------------------|\n| https://poker                     |\n| https://poker.fr                  |\n| https://poker.foo.bar             |\n| https://abc.efg.com               |\n| http://asdf.qwer.com/welcome.htm  |\n| http://aasdf.qwer.com/welcome.htm |\n| https://zxcv.xxx/index.asp        |\n+-----------------------------------+\n</code></pre> Next we run the transform to  1. read file(s) from the <code>input</code> directory  2. write transformed file(s) to the <code>output</code> directory 3. add a column named <code>hello</code> containing <code>Hello David!</code>, using the <code>--ac_column_name</code> and <code>--ac_who</code> CLI parameters.  <pre><code>(venv) % mkdir output\n(venv) % python hello_transform.py  --data_local_config \\\n    \"{ 'input_folder': 'input', 'output_folder': 'output'}\" \\\n    --ac_who David \\\n    --ac_column_name hello\n15:06:59 INFO - pipeline id pipeline_id\n15:06:59 INFO - job details {'job category': 'preprocessing', 'job name': 'add_column', 'job type': 'pure python', 'job id': 'job_id'}\n15:06:59 INFO - code location None\n15:06:59 INFO - data factory data_ is using local data access: input_folder - input output_folder - output\n15:06:59 INFO - data factory data_ max_files -1, n_sample -1\n15:06:59 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n15:06:59 INFO - orchestrator add_column started at 2024-07-10 15:06:59\n15:06:59 INFO - Number of files is 1, source profile {'max_file_size': 0.0007181167602539062, 'min_file_size': 0.0007181167602539062, 'total_file_size': 0.0007181167602539062}\n15:07:06 INFO - Completed 1 files (100.0%) in 0.12749731938044231 min\n15:07:06 INFO - done flushing in 7.867813110351562e-06 sec\n15:07:06 INFO - Completed execution in 0.1275073528289795 min, execution result 0\n(venv) % ls output\nmetadata.json   test1.parquet\n(venv) % parquet-tools show output/test1.parquet \n+-----------------------------------+--------------+\n| title                             | hello        |\n|-----------------------------------+--------------|\n| https://poker                     | Hello David! |\n| https://poker.fr                  | Hello David! |\n| https://poker.foo.bar             | Hello David! |\n| https://abc.efg.com               | Hello David! |\n| http://asdf.qwer.com/welcome.htm  | Hello David! |\n| http://aasdf.qwer.com/welcome.htm | Hello David! |\n| https://zxcv.xxx/index.asp        | Hello David! |\n+-----------------------------------+--------------+\n</code></pre></p>"},{"location":"doc/quick-start/quick-start/","title":"Quick Start for Data Prep Kit","text":"<p>Here we provided short examples of various uses of the Data Prep Kit. Most users who want to jump right in can use standard pip install to deploy the data-prep-kit and the python or ray transforms to their virtual python environment. </p> <p>When setting up a virtual environment it is recommended to use python3.11 as in the example below using conda. </p> <p>setup a virtual environment (example using conda)</p> <pre><code>conda create -n data-prep-kit-1 -y python=3.11\n</code></pre> <p>Install the gcc/g++ that is required while building fastext:</p> <pre><code>conda install gcc_linux-64\nconda install gxx_linux-64\n</code></pre> <p>activate the new conda environment</p> <pre><code>conda activate data-prep-kit-1\n</code></pre> <p>make sure env is switched to data-prep-kit-1 and Check python version.</p> <pre><code>python --version\nThe command above should say: 3.11\n</code></pre> <p>install jupyter lab</p> <pre><code>pip3 install jupyterlab\n</code></pre> <p>then</p> <p>Deploy the latest release of the data prep toolkit library</p> <pre><code>pip3 install data-prep-toolkit\n</code></pre> <p>or </p> <p>Deploy the latest releases of the data prep toolkit library and all python transforms</p> <pre><code>pip3 install data-prep-toolkit-transforms\n</code></pre> <p>or </p> <p>Deploy the latest releases of the data prep toolkit library, all python transforms and all ray transforms</p> <pre><code>pip3 install data-prep-toolkit-transforms[ray]\n</code></pre>"},{"location":"doc/quick-start/quick-start/#running-transforms","title":"Running transforms","text":"<ul> <li>Notebooks<ul> <li>Example data processing pipelines - Use these to quickly process your data. A notebook structure allows a user to select/de-select transforms and change the order of processing as desired. </li> </ul> </li> <li>Command line  <ul> <li>Using a docker image - runs a transform in a docker transform image </li> <li>Using a virtual environment - runs a transform on the local host </li> </ul> </li> </ul>"},{"location":"doc/quick-start/quick-start/#creating-transforms","title":"Creating transforms","text":"<ul> <li>Outside of the repository - shows how to use pypi dependencies to create a transform independent of this repository.</li> <li>Adding to this repository - shows how to add a transform to this repository, including repository conventions and ci/cd. </li> </ul>"},{"location":"doc/quick-start/run-transform-image/","title":"Running a Transform Image","text":"<p>Here we address a simple use case of applying a single transform to a  set of parquet files. We'll use one of the pre-built docker images from the repository to process the data. Additionally, what follows uses the  python runtime image, but the examples below should also work for the ray or spark runtime images (<code>-ray</code> or <code>-spark</code> image name suffix instead of <code>-python</code>).</p>"},{"location":"doc/quick-start/run-transform-image/#getting-an-image","title":"Getting an image","text":"<p>You may build the transform locally in the repository, for example, <pre><code>cd transforms/universal/noop/python\nmake image\ndocker images | grep noop\n</code></pre> produces <pre><code>quay.io/dataprep1/data-prep-kit/noop-python                     latest   aac55fa3d82d  5 seconds ago  355 MB\n</code></pre> Or, you can use the pre-built images (latest, or 0.2.1 or later tags)  on quay.io found at https://quay.io/user/dataprep1.</p>"},{"location":"doc/quick-start/run-transform-image/#local-data-python-runtime","title":"Local Data - Python Runtime","text":"<p>To use an image to process local data we will mount the host input and output directories into the image.  Any mount point can be used, but we will use <code>/input</code> and <code>/output</code>.</p> <p>To process data in the <code>/home/me/input</code> directory and write it to the <code>/home/me/output</code> directory, we mount these directories into the image at the above mount points. So for example, using the locally built <code>noop</code> transform:</p> <p><pre><code>docker run  --rm \n    -v /home/me/input:/input \\\n    -v /home/me/output:/output \\\n    noop-python:latest  \\\n    python noop_transform_python.py \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/input', \\\n        'output_folder' : '/output' \\\n        }\"\n</code></pre> To run the quay.io located transform instead, substitute  <code>quay.io/dataprep1/data-prep-kit/noop-python:latest</code> for <code>noop-python:latest</code>, as follows: <pre><code>docker run  --rm \n    -v /home/me/input:/input \\\n    -v /home/me/output:/output \\\n    quay.io/dataprep1/data-prep-kit/noop-python:latest  \\\n    python noop_transform_python.py \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/input', \\\n        'output_folder' : '/output' \\\n        }\"\n</code></pre></p>"},{"location":"doc/quick-start/run-transform-image/#local-data-ray-runtime","title":"Local Data - Ray Runtime","text":"<p>To use the ray runtime, we must  1. Switch to using the ray-based image <code>noop-ray:latest</code> 2. Use the ray runtime python main() defined in <code>noop_transform_ray.py</code></p> <p>For example, using the quay.io image <pre><code>docker run  --rm \n    -v /home/me/input:/input \\\n    -v /home/me/output:/output \\\n    quay.io/dataprep1/data-prep-kit/noop-ray:latest     \\\n    python noop_transform_ray.py \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/input', \\\n        'output_folder' : '/output' \\\n        }\"\n</code></pre> This is functionally equivalent to the python-runtime, but additional configuration can be provided (see the  ray launcher args) for details.</p>"},{"location":"doc/quick-start/run-transform-image/#s3-located-data-python-runtime","title":"S3-located Data - Python Runtime","text":"<p>When processing data located in S3 buckets, one can use the same image and specify different <code>--data_s3_*</code> configuration as follows: </p> <pre><code>docker run  --rm \n    noop-python:latest  \\\n    python noop_transform_python.py \\\n    --data_s3_cred \"{ \\\n        'access_key'  : '...', \\\n        'secret_key' : '...', \\\n        'url' : '...', \\\n        }\"  \\\n    --data_s3_config \"{ \\\n        'input_folder'  : '...', \\\n        'output_folder' : '...', \\\n        }\"  \n</code></pre>"},{"location":"doc/quick-start/run-transform-venv/","title":"Running a Transform in a Virtual Environment","text":"<p>Here we address a simple use case of applying a single transform to a  set of parquet files.  We'll use the noop transform as an example, but in general, this process will work for any of the transforms contained in the repository. Additionally, what follows uses the  python runtime (e.g., noop/python directory),  but the examples below should also work for the ray (noop/ray directory) or spark  (noop/spark directory) runtimes.</p>"},{"location":"doc/quick-start/run-transform-venv/#creating-the-virtual-environment","title":"Creating the Virtual Environment","text":"<p>Each transform project contains a Makefile that will assist in building the virtual environment in a directory named <code>venv</code>. To create the virtual environment for the <code>noop</code> transform:</p> <p><pre><code>cd transforms/univeral/noop/python\nmake venv \n</code></pre> Note, if needed, you can override the default <code>python</code> command used  in <code>make venv</code> above, with for example:</p> <pre><code>make PYTHON=python3.10 venv\n</code></pre>"},{"location":"doc/quick-start/run-transform-venv/#local-data","title":"Local Data","text":"<p>To process data in the <code>/home/me/input</code> directory and write it to the <code>/home/me/output</code> directory, activate the virtual environment and then call the transform referencing these directories. So for example, using the <code>noop</code> transform  to read parquet files from <code>/home/me/input</code>:</p> <pre><code>cd transforms/univeral/noop/python\nsource venv/bin/activate\npython src/noop_transform_python.py \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/home/me/input', \\\n        'output_folder' : '/home/me/output' \\\n        }\"\ndeactivate\n</code></pre>"},{"location":"doc/quick-start/run-transform-venv/#s3-located-data","title":"S3-located Data","text":"<p>When processing data located in S3 buckets, one can use the same  approach and specify different <code>--data_s3_*</code> configuration as follows: </p> <pre><code>cd transforms/univerals/noop/python\nsource venv/bin/activate\npython src/noop_transform_python.py \\\n    --data_s3_cred \"{ \\\n        'access_key'  : '...', \\\n        'secret_key' : '...', \\\n        'url' : '...', \\\n        }\"  \\\n    --data_s3_config \"{ \\\n        'input_folder'  : '...', \\\n        'output_folder' : '...', \\\n        }\"  \n</code></pre>"},{"location":"examples/notebooks/archive/","title":"Data Prep Kit Examples","text":"<ul> <li>Code </li> <li>Language - coming soon.</li> </ul>"},{"location":"examples/notebooks/archive/code/","title":"Index","text":""},{"location":"examples/notebooks/archive/code/#demo-notebook","title":"Demo Notebook","text":"<p>We can launch a jupyter notebook to try out the library or transforms or to build a usecase.</p>"},{"location":"examples/notebooks/archive/code/#setup-simple-environment-with-jupyter-notebook","title":"Setup simple environment with Jupyter notebook","text":"<ol> <li>Prepare virtual env</li> </ol> <pre><code>python3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install jupyter\ndeactivate\n</code></pre> <ol> <li>After jupyter dependancies are installed, we can launch the jupyter notebook</li> </ol> <pre><code>source venv/bin/activate\njupyter notebook\n</code></pre> <p>It will launch browser with the jupyter notebook where we can try out the transforms.</p>"},{"location":"examples/notebooks/archive/code/#demo-api-notebook","title":"Demo API notebook","text":"<p>Prepare virtualenv</p> <p><code>make venv</code></p> <p>Activate virtualenv</p> <p><code>. ./venv/bin/activate</code></p> <p>Run notebook</p> <p><code>jupyter notebook</code></p>"},{"location":"examples/notebooks/archive/code_developers/","title":"Index","text":""},{"location":"examples/notebooks/archive/code_developers/#demo-notebook","title":"Demo Notebook","text":"<p>We can launch a jupyter notebook to try out the library or transforms or to build a usecase.</p>"},{"location":"examples/notebooks/archive/code_developers/#one-time-step","title":"One time Step","text":"<ol> <li>Run this step install the dependancies. </li> </ol> <p>Prepare virtualenv</p> <p><code>make venv</code></p>"},{"location":"examples/notebooks/archive/code_developers/#launch","title":"Launch","text":"<ol> <li>After dependancies are installed, we can launch the jupyter notebook</li> </ol> <p><code>make jupyter</code></p> <p>It will launch browser with the jupyter notebook where we can try out the transforms.</p>"},{"location":"examples/notebooks/archive/language/","title":"Index","text":""},{"location":"examples/notebooks/archive/language/#demo-notebook","title":"Demo Notebook","text":"<p>We can launch a jupyter notebook to try out the library or transforms or to build a usecase.</p>"},{"location":"examples/notebooks/archive/language/#one-time-step","title":"One time Step","text":"<ol> <li>Run this step install the dependancies. </li> </ol> <p>Prepare virtualenv</p> <p><code>make venv</code></p>"},{"location":"examples/notebooks/archive/language/#launch","title":"Launch","text":"<ol> <li>After dependancies are installed, we can launch the jupyter notebook</li> </ol> <p><code>make jupyter</code></p> <p>It will launch browser with the jupyter notebook where we can try out the transforms.</p>"},{"location":"examples/notebooks/fine%20tuning/language/","title":"Index","text":""},{"location":"examples/notebooks/fine%20tuning/language/#demo-notebook","title":"Demo Notebook","text":"<p>We can launch a jupyter notebook to try out the library or transforms or to build a usecase.</p>"},{"location":"examples/notebooks/fine%20tuning/language/#one-time-step","title":"One time Step","text":"<ol> <li>Run this step install the dependancies. </li> </ol> <p>Prepare virtualenv</p> <p><code>make venv</code></p>"},{"location":"examples/notebooks/fine%20tuning/language/#launch","title":"Launch","text":"<ol> <li>After dependancies are installed, we can launch the jupyter notebook</li> </ol> <p><code>make jupyter</code></p> <p>It will launch browser with the jupyter notebook where we can try out the transforms.</p>"},{"location":"examples/notebooks/intro/","title":"Data Prep Kit Introduction","text":"<p>This is an example featuring some of the features of data prep kit.</p>"},{"location":"examples/notebooks/intro/#running-the-code","title":"Running the code","text":"<p>The code can be run on either </p> <ol> <li>Google colab: very easy to run; no local setup needed.</li> <li>On your local Python environment.  Here is a quick guide.  You can  find instructions for latest version here</li> </ol> <pre><code>conda create -n data-prep-kit -y python=3.11\nconda activate data-prep-kit\n\n# install the following in 'data-prep-kit' environment\npip3 install data-prep-toolkit==0.2.1\npip3 install data-prep-toolkit-transforms==0.2.1\npip3 install data-prep-toolkit-transforms-ray==0.2.1\npip3 install jupyterlab   ipykernel  ipywidgets\n\n## install custom kernel\n## Important: Use this kernel when running example notebooks!\npython -m ipykernel install --user --name=data-prep-kit --display-name \"dataprepkit\"\n\n# start jupyter and run the notebooks with this jupyter\njupyter lab\n</code></pre>"},{"location":"examples/notebooks/intro/#intro","title":"Intro","text":"<p>This notebook will demonstrate processing PDFs</p> <p><code>PDFs ---&gt; text ---&gt; chunks ---&gt;   exact dedupe ---&gt; fuzzy dedupe ---&gt; embeddings</code></p> <p>python version  \u00a0   |   \u00a0  ray version</p>"},{"location":"examples/notebooks/intro/input/solar-system/earth/","title":"Earth","text":""},{"location":"examples/notebooks/intro/input/solar-system/earth/#solar-system","title":"Solar System","text":"<p>Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.</p> <p>For more details about our Solar system see Chapter 1.</p>"},{"location":"examples/notebooks/intro/input/solar-system/earth/#earth_1","title":"Earth","text":"<p>Earth is the third planet from the Sun.  It's our home planet.  Earth is the only place we know of with life.</p> <p>Basic facts about Earth:</p> <ul> <li>Distance from the Sun: Average of 149.6 million kilometers (93 million miles)</li> <li>Rotation Period: 24 hours (one day)</li> <li>Moons: One moon, called Luna or simply \"the Moon\".</li> </ul>"},{"location":"examples/notebooks/intro/input/solar-system/mars/","title":"Mars","text":""},{"location":"examples/notebooks/intro/input/solar-system/mars/#solar-system","title":"Solar System","text":"<p>Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.</p> <p>For more details about the Solar system see Chapter 1.</p>"},{"location":"examples/notebooks/intro/input/solar-system/mars/#mars_1","title":"Mars","text":"<p>Mars, the fourth planet from the Sun, is a cold, desert world with a thin atmosphere composed primarily of carbon dioxide.  Its reddish hue comes from iron oxide, or rust, prevalent on its surface. </p> <p>Basic facts about Mars:</p> <ul> <li>Distance from the Sun: Average of 228 million kilometers (142 million miles)</li> <li>Rotation Period: 24.6 hours (one Martian day - called a \"sol\")</li> <li>Moons: Two small moons, Phobos and Deimos.</li> </ul>"},{"location":"examples/notebooks/rag/","title":"RAG with Data Prep Kit","text":"<p>This folder has examples of RAG applications with data prep kit (DPK).</p>"},{"location":"examples/notebooks/rag/#step-0-getting-started","title":"Step-0: Getting Started","text":"<pre><code>## Clone this repo\ngit  clone   https://github.com/IBM/data-prep-kit\n\ncd data-prep-kit\n## All commands from here on out, assume you are in project root directory\n</code></pre>"},{"location":"examples/notebooks/rag/#step-1-setup-python-environment","title":"Step-1: Setup Python Environment","text":"<p>setup-python-dev-env.md</p> <p>Make sure Jupyter is running after this step.  We will use this Jupyter instance to run the notebooks in next steps.</p>"},{"location":"examples/notebooks/rag/#rag-workflow","title":"RAG Workflow","text":"<p>Here is the overall work flow.  For details see RAG-explained</p> <p></p>"},{"location":"examples/notebooks/rag/#step-2-process-input-documents-rag-stage-1-2-3","title":"Step-2: Process Input Documents (RAG stage 1, 2 &amp; 3)","text":"<p>This code uses DPK to </p> <ul> <li>Extract text from PDFs (RAG stage-1)</li> <li>Performs de-dupes (RAG stage-1)</li> <li>split the documents into chunks (RAG stage-2)</li> <li>vectorize the chunks (RAG stage-3)</li> </ul> <p>Here is the code: </p> <ul> <li>Python version: rag_1A_dpk_process_python.ipynb</li> <li>Ray version: rag_1A_dpk_process_ray.ipynb</li> </ul>"},{"location":"examples/notebooks/rag/#step-3-load-data-into-vector-database-rag-stage-4","title":"Step-3: Load data into vector database  (RAG stage 4)","text":"<p>Our vector database is Milvus</p> <p>Run the code: rag_1B_load_data_into_milvus.ipynb</p> <p>Be sure to shutdown the notebook before proceeding to the next step</p>"},{"location":"examples/notebooks/rag/#step-4-perform-vector-search-rag-stage-5-6","title":"Step-4: Perform vector search (RAG stage 5 &amp; 6)","text":"<p>Let's do a few searches on our data.</p> <p>Code: rag_1C_vector_search.ipynb</p> <p>Be sure to shutdown the notebook before proceeding to the next step</p>"},{"location":"examples/notebooks/rag/#step-5-query-the-documents-using-llm-rag-steps-5-6-7-8-9","title":"Step-5: Query the documents using LLM (RAG steps 5, 6, 7, 8 &amp; 9)","text":"<p>We will use Llama as our LLM running on Replicate service.</p>"},{"location":"examples/notebooks/rag/#51-create-an-env-file","title":"5.1 - Create an <code>.env</code> file","text":"<p>To use replicate service, we will need a replicate API token.</p> <p>You can get one from Replicate</p> <p>Create an <code>.env</code> file (notice the dot in the file name in this directory with content like this</p> <pre><code>REPLICATE_API_TOKEN=your REPLICATE token goes here\n</code></pre>"},{"location":"examples/notebooks/rag/#52-run-the-query-code","title":"5.2 - Run the query code","text":"<p>Code: rag_1D_query_llama_replicate.ipynb</p>"},{"location":"examples/notebooks/rag/#step-6-illama-index","title":"Step 6: Illama Index","text":"<p>For comparision, we can use Llama-index framework to process PDFs and query</p>"},{"location":"examples/notebooks/rag/#step-61-process-documents-and-save-the-index-into-vector-db","title":"Step 6.1 - Process documents and save the index into vector DB","text":"<p>Code: rag_2A_llamaindex_process.ipynb</p> <p>Be sure to shutdown the notebook before proceeding to the next step</p>"},{"location":"examples/notebooks/rag/#step-62-query-documents-with-llm","title":"Step 6.2 - Query documents with LLM","text":"<p>code: rag_2B_llamaindex_query.ipynb</p>"},{"location":"examples/notebooks/rag/#tips-close-the-notebook-kernels-to-release-the-dblock","title":"Tips: Close the notebook kernels, to release the db.lock","text":"<p>When using embedded database, the program maintains a lock on the database file.  If the lock is active, other notebooks may not be able to access the same database.</p> <p>Here is how to close kernels:</p> <ul> <li>In vscode:  Restart the kernel. This will end the current kernel session and release the db.lock</li> <li>In Jupyter : Go to  <code>File --&gt; Close and shutdown notebook</code>.  This will end the current kernel session and release the db.lock</li> </ul>"},{"location":"examples/notebooks/rag/RAG-explained/","title":"RAG explained","text":""},{"location":"examples/notebooks/rag/RAG-explained/#rag-process-explained","title":"RAG Process Explained","text":"<p>RAG conists of two phases</p> <ul> <li>1 - Import phase:  Preparing and indexing the data  (the top row in the diagram)</li> <li>2 - Querying phase: query the data prepared in phase 1  (bottom row in the  diagram)</li> </ul> <p></p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-1-ingest-cleanup-documents","title":"Step 1 (Ingest): Cleanup documents","text":"<p>Remove markups, perform de-duplication ..etc</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-2-ingest-split-into-chunks","title":"Step 2 (Ingest): Split into chunks","text":"<p>Split the documents into manageable chunks or segments. There are various chunking stratergies.  Documents can be split into pages or paragraphs or sections.  The right chunking strategy depends on the document types being processed</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-3-ingest-vectorize-calculate-embeddings","title":"Step 3 (Ingest): Vectorize / Calculate Embeddings","text":"<p>In order to make text searchable, we need to 'vectorize' them.  This is done by using embedding models.  We will feature a variety of embedding models, open source ones and API based ones.</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-4-ingest-saving-data-into-vector-database","title":"Step 4 (Ingest). Saving Data into Vector Database","text":"<p>In order to effectivly retrieve relevant documents, we use Milvus - a very popular open source, vector database.</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-5-query-vectorize-question","title":"Step 5 (Query). Vectorize Question","text":"<p>When user asks a question, we are going to vectorize the question so we can fetch documents that may have the answer question.</p> <p>For example, if a user asks a financial question ('how much was the revenue in 2022') the answer is most likely in financial documents, not in employee handbooks.</p> <p>So we want to retrieve the relevant documents first.</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-6-query-vector-search","title":"Step 6 (Query): Vector Search","text":"<p>We send the 'vectorized query' to vector database to retrieve the relevant documents.</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-7-query-retrieve-relevant-documents","title":"Step 7 (Query): Retrieve Relevant Documents","text":"<p>Vector database looks at our query (in vectorized form), searches through the documents and returns the documents matching the query.</p> <p>This is an important step, because it cuts down the 'search space'.  For example, if have 1000 pdf documents, and only a small number of documents might contain the answer, it returns the relevant documents.</p> <p>The search has to be accurate, as these are the documents sent to LLM as 'context'.  LLM will look through these documents searching for the answer to our question</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-8-query-send-relevant-documents-and-query-llm","title":"Step 8 (Query): Send relevant documents and query LLM","text":"<p>We send the relevant documents (returned in the above step by Vector DB) and our query to LLM.</p> <p>LLMs can be accessed via API or we can run one locally.</p>"},{"location":"examples/notebooks/rag/RAG-explained/#step-9-query-answer-from-llm","title":"Step 9 (Query): Answer from LLM","text":"<p>Now we get to see the answer provided by LLM \ud83d\udc4f</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/","title":"Setup a Local Python Dev Environment","text":"<p>We can use </p> <ul> <li>Option-A: Use Anaconda environment</li> <li>Option-B: Use python virtual env</li> </ul> <p>Just follow one.  (A) is recommended!</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#option-a-recommended-anaconda-python-environment","title":"Option A (Recommended): Anaconda Python environment","text":"<p>You can install Anaconda by following the guide here.</p> <p>We will create an environment for this workshop with all the required libraries installed.</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#a-1-setup-a-conda-env","title":"A-1: Setup a conda env","text":"<pre><code>conda create -n data-prep-kit-1 -y python=3.11\n</code></pre> <p>activate the new conda environment</p> <pre><code>conda activate data-prep-kit-1\n</code></pre> <p>Make sure env is swithced to data-prep-kit-1</p> <p>Check python version</p> <pre><code>python --version\n</code></pre> <p>should say : 3.11</p> <p>Note: If you are on a linux system install these too</p> <pre><code>conda install gcc_linux-64\n\nconda install gxx_linux-64\n</code></pre>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#a-2-install-dependencies","title":"A-2: Install dependencies","text":"<pre><code>cd examples/notebooks/rag\n</code></pre> <pre><code>pip  install  -r requirements.txt\n</code></pre> <p>If any issues see troubleshooting tips</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#a-3-start-jupyter","title":"A-3: Start Jupyter","text":"<p><code>jupyter lab</code></p> <p>This will usually open a browser window/tab.  We will use this to run the notebooks</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#option-b-python-virtual-env","title":"Option B: Python virtual env","text":""},{"location":"examples/notebooks/rag/setup-python-dev-env/#b-1-have-python-version-311","title":"B-1: Have python version 3.11","text":"<pre><code>## Check python version\npython --version\n# should say : 3.11\n</code></pre>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#b-2-create-a-venv","title":"B-2: Create a venv","text":"<pre><code>cd examples/notebooks/rag\n\n\npython -m venv venv\n\n## activate venv\nsource ./venv/bin/activate\n\n## Install requirements\npip install -r requirements.txt\n</code></pre> <p>If any issues see troubleshooting tips</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#b-3-launch-jupyter","title":"B-3: Launch Jupyter","text":"<p><code>./venv/bin/jupyter lab</code></p> <p>This will usually open a browser window/tab.  We will use this to run the notebooks</p> <p>Note:: Make sure to run <code>./venv/bin/jupyter lab</code>, so it can load installed dependencies correctly.</p>"},{"location":"examples/notebooks/rag/setup-python-dev-env/#troubleshooting-tips","title":"Troubleshooting Tips","text":""},{"location":"examples/notebooks/rag/setup-python-dev-env/#fasttext-compile-issue-with-gccg-compiler-version-13","title":"fasttext compile issue with GCC/G++ compiler version 13","text":"<p><code>pip install</code> may fail because one of the python dependencies, <code>fasttext==0.9.2</code> compiles with GCC/G++ version 11, not version 13.</p> <p>Here is how to fix this error:</p> <pre><code>## These instructions are for Ubuntu 22.04 and later\n\nsudo apt update\n\n## install GCC/G++ compilers version 11 \nsudo apt install -y gcc-11  g++-11\n\n## Verify installation\ngcc-11  --version\ng++-11  --version\n# should say 11\n\n## Set the compiler before doing pip install\nCC=gcc-11  pip install -r requirements.txt \n</code></pre>"},{"location":"kfp/","title":"Automation with Kubeflow Pipelines","text":""},{"location":"kfp/#map-betweens-transforms-and-kfp-pipelines","title":"Map betweens transforms and KFP pipelines","text":"Transform KFP pipeline language/lang_id lang_id_wf.py language/html2parquet html2parquet_wf.py code/malware malware_wf.py code/code2parquet code2parquet_wf.py code/code_quality code_quality_wf.py code/proglang_select proglang_select_wf.py code/license_select license_select_wf.py universal/doc_id doc_id_wf.py universal/ededup ededup_wf.py universal/fdedup fdedup_wf.py universal/filtering filter_wf.py universal/noop noop_wf.py universal/profiler profiler_wf.py universal/tokenization tokenization_wf.py universal/hap hap_wf.py"},{"location":"kfp/#set-up-and-working-steps","title":"Set up and working steps","text":"<ul> <li>Set up a Kubernetes clusters for KFP execution</li> <li>Simple Transform pipeline tutorial</li> <li>Execution several transformers</li> <li>Clean up the cluster</li> </ul>"},{"location":"kfp/RELEASE/","title":"Release Process","text":"<p>This document describes the release process for the following components:</p> <ul> <li> <p><code>kfp_support_lib</code> Python packages in <code>kfp_support_lib</code> directory.</p> </li> <li> <p><code>kfp-data-processing</code> docker image built based on the Docker file in <code>kfp_ray_components</code> directory.</p> </li> <li> <p>kubeflow pipelines in <code>transform_workflows</code> directory. For example the one that is generated from <code>transform_workflows/universal/noop/noop_wf.py</code> file.</p> </li> </ul>"},{"location":"kfp/RELEASE/#1-update-makeversions-file","title":"1. Update <code>.make.versions</code> file","text":"<p>The .make.versions file specifies the target versions for the building components, as well as the  desired versions for the dependencies.  The KFP package build uses the following variables from the file: - RELEASE_VERSION_SUFFIX - the common suffix for all building components - DPK_LIB_KFP_VERSION - the version of <code>kfp_v1_workflow_support</code> - DPK_LIB_KFP_VERSION_v2 - the version of <code>kfp_v2_workflow_support</code> - DPK_LIB_KFP_SHARED - the version of <code>kfp_shared_workflow_support</code> - KFP_DOCKER_VERSION - the docker image version of KFP components for KFPv1 - KFP_DOCKER_VERSION_v2 - the docker image version of KFP components for KFPv2</p> <p>Note: The docker images are dependent on the libraries but use the python source code from the repository, so inorder  to build docker images, the python modules (libraries) do not have to be deployed. </p>"},{"location":"kfp/RELEASE/#2-choose-the-supported-kfp-version","title":"2. Choose the supported KFP version.","text":"<p>The docker images and some <code>workflow_support</code> libraries depend on KFP version. In order to build images and libraries for KFP v2, run the following command:</p> <pre><code>export KFPv2=1\n</code></pre>"},{"location":"kfp/RELEASE/#3-optional-build-the-library","title":"3. (Optional) Build the library","text":"<p>Run the <code>make -C shared_workflow_support build</code> command to build the shared library. If you need a library for KFPv1 Run <code>make -C kfp_v1_workflow_support build</code> For KFP v2 set the environment variable <code>KFPv2</code>, se above, and run <code>make -C kfp_v2_workflow_support build</code></p>"},{"location":"kfp/RELEASE/#4-optional-publish-the-library","title":"4. (Optional) Publish the library","text":"<p>Run <code>make -C shared_workflow_support publish</code>, and either <code>make -C kfp_v1_workflow_support publish</code> or  <code>make -C kfp_v2_workflow_support publish</code>command to push the libraries to the TestPyPI repository.</p>"},{"location":"kfp/RELEASE/#5-build-the-image","title":"5. Build the image","text":"<p>Run <code>make -C kfp_ray_components build</code> command to build the <code>kfp-data-processing</code> docker image, or <code>kfp-data-processing_v2</code>, when <code>KFPv2==1</code></p>"},{"location":"kfp/RELEASE/#5-push-the-image","title":"5. Push the image","text":"<p>Run <code>make -C kfp_ray_components publish</code> command to push the docker image.</p>"},{"location":"kfp/doc/deployment_on_MacOS/","title":"Development and Testing on macOS","text":"<p>Users can develop and test the system locally. This can be achieved by executing a local kind  Kubernetes cluster. On Linux or Windows (WSL) operating systems, users can utilize  Docker Engine. However, Docker on macOS necessitates  Docker Desktop, which for  commercial usage requires a paid subscription. Alternatively, Podman can serve as a free substitute. This document provides instructions for installing and  configuring Podman.</p>"},{"location":"kfp/doc/deployment_on_MacOS/#podman-installation","title":"Podman Installation","text":"<ul> <li>Download and install Podman from its website. (It can be either Podman Desktop or just Podman CLI)</li> <li>Running this project with KubeFlow pipelines  requires more CPU and Memory than what the default Podman virtual machine provides. Therefore, its settings should be  adjusted.</li> <li>For Podman CLI execute <code>podman machine init --cpus 5 --memory 61440</code></li> <li>For Podman Desktop, open the \"Settings\" and set required CPUS and Memory.  </li> <li>Execute <code>make setup</code> from the <code>scripts/k8s-setup</code> directory.</li> <li>The installation of Kubeflow pipelines takes time, and some of its pods may experience crashes during startup.    Therefore, we kindly ask for your patience and to wait until ALL pods are in the Ready state. </li> </ul>"},{"location":"kfp/doc/multi_transform_pipeline/","title":"Run several transforms","text":"<p>In a common scenario, users want to run a pipeline or a series of several transforms one after the other. Here we can show how to execute several transforms.</p> <p>Let's assume that the input folder path is <code>path/to/input</code> and a user wants to save the output of the data after several transforms in <code>path/to/output</code>. Let's assume also that the user wants to apply the transform <code>A</code> then the transform <code>B</code> on his data.</p> <p>In Executing pipeline and watching execution results you can learn how to run a KFP pipeline of one transform. We want to use the same method to combine several transform pipelines together.</p> <p>Let's start with transform A (in this example it is exact deduplication). After uploading the pipeline you can create a run from this page:</p> <p></p> <p>After clicking <code>create run</code> a list of input parameters is shown on the screen. In this document we want to deal with the <code>data_s3_config</code> input parameter. This parameter specifies the input folder path and output folder path. For the transform A we should have <code>input_folder = path/to/input</code> and the <code>output_folder=/path/to/&lt;B&gt;_input</code> which is an intermediate folder that we will use as an input for the next transform.</p> <p></p> <p>After completing the first transformation, we can continue to the next one. As in the previous step, we update the pipeline input parameters for the transformer <code>B</code> (in this example it is fuzzy deduplication) and create a Run.</p> <p></p> <p>In the list of its input parameters, we also see <code>data_s3_config</code>. Now, we have <code>input_folder = path/to/B_input</code> (the output folder of the previous transformation pipeline) and <code>output_folder=/path/to/output</code>, the desired output folder for the whole task. if we want to execute several transformation pipelines, we have to define more intermediate folders.</p> <p></p>"},{"location":"kfp/doc/multi_transform_pipeline/#examples","title":"Examples","text":"<p>The sections that follow display two super pipelines as examples:</p> <p>1) dedups super pipeline 1) programming languages super pipeline</p>"},{"location":"kfp/doc/multi_transform_pipeline/#dedups-super-pipeline","title":"Dedups super pipeline","text":"<p>This pipeline combines several transforms, <code>doc_id</code>, <code>ededup</code>, and <code>fdedup</code>, can be found in superworkflow_dedups_sample_wf.py.</p> <p></p> <p>The input parameters of the super pipelines are described in this section.</p>"},{"location":"kfp/doc/multi_transform_pipeline/#programming-languages-super-pipeline","title":"Programming languages super pipeline","text":"<p>This pipeline combines transforms for programming languages data preprocessing: <code>ededup</code>, <code>doc_id</code>, <code>fdedup</code>, <code>proglang_select</code>, <code>code_quality</code>,  <code>malware</code> and <code>tokenization</code>. It can be found in superworkflow_code_wf.py.</p> <p></p> <p>The input parameters of the super pipelines are described in this section.</p>"},{"location":"kfp/doc/multi_transform_pipeline/#super-pipeline-input-parameters","title":"Super Pipeline Input Parameters","text":"<p>There are several <code>groups</code> of input parameters for super pipelines, each group of parameters has a prefix of \"p_\" string, where  is an order number. <p>\"p1_\" prefix group of parameters:</p> <ul> <li>The pipeline names of the tasks that form the super pipeline.</li> </ul> <p>\"p2_\" group:</p> <ul> <li> <p>parameters of the ray cluster (head and workers options) that are common to all tasks.</p> </li> <li> <p>input/output paths: There are three parameters that specify the input and output paths for all the pipeline steps. The parameters are <code>p2_pipeline_input_parent_path</code>, <code>p2_pipeline_output_parent_path</code> <code>p2_pipeline_parent_path_suffix</code>.</p> </li> </ul> <p>The input of the first step is <code>p2_pipeline_input_parent_path</code> + <code>/</code> + <code>p2_pipeline_parent_path_suffix</code>.</p> <pre><code>The output of step S is: `p2_pipeline_output_parent_path` + `_&lt;task1Name&gt;_\u2026_&lt;taskSName&gt;` + `/` + `p2_pipeline_parent_path_suffix`.\n\nThe output of step S will be the input of the step S+1 (the next step).\n</code></pre> <ul> <li><code>p2_pipeline_additional_params</code>: it is a json string that includes a several parameters that passed to all steps of the super pipeline (similar to default values that can be overwritten in each step).</li> </ul> <p><code>p3_</code> to <code>p&lt;x&gt;_</code>: </p> <ul> <li> <p>Each group of <code>p&lt;s&gt;_</code> is a list of parameters of the step <code>&lt;s&gt;</code>. It includes a</p> <ul> <li> <p>Step name that is used in the output paths.</p> </li> <li> <p>Several parameters that pass to the step pipeline.</p> </li> <li> <p>Skip parameter: if <code>True</code> then skip this step.</p> </li> <li> <p>JSON string parameter named <code>overriding_params</code> that can be used to modify <code>p2_</code> prefixed input pipeline parameters and add additional inputs that are not listed as separate input parameters to the step pipeline.  Note: if there are Boolean parameters it should be specified as a string value of <code>\"True\"</code> or <code>\"False\"</code> (with quotation mark).</p> </li> </ul> </li> </ul>"},{"location":"kfp/doc/setup/","title":"Set up a Kubernetes clusters for KFP execution","text":""},{"location":"kfp/doc/setup/#table-of-contents","title":"\ud83d\udcdd Table of Contents","text":"<ul> <li>A Kind deployment supported platforms</li> <li>Preinstalled software components</li> <li>A Kind deployment</li> <li>An existing cluster</li> <li>Installation steps</li> <li>Installation on an existing Kubernetes cluster</li> <li>Clean up the cluster</li> </ul> <p>The project provides instructions and deployment automation to run all components in an all-inclusive fashion on a  single machine using a Kind cluster and a local data storage (MinIO). However, this topology is not suitable for processing medium and large datasets, and deployment should be carried out  on a real Kubernetes or OpenShift cluster. Therefore, we recommend using Kind cluster for only for local testing and  debugging, not production loads. For production loads use a real Kubernetes cluster.</p> <p>Running a Kind Kubernetes cluster with Kubeflow pipelines (KFP) and MinIO requires significant memory. We recommend deploying it on machines with at least 32 GB of RAM and 8-9 CPU cores. RHEL OS requires  more resources, e.g. 64 GB RAM and 32 CPU cores.</p>"},{"location":"kfp/doc/setup/#a-kind-deployment-supported-platforms","title":"A Kind deployment supported Platforms","text":"<p>Executing KFP, MinIO, and Ray on a single Kind cluster pushes the system to its load limits. Therefore, although we are  working on extending support for additional platforms, not all platforms/configurations are currently supported.</p> Operating System Container Agent Support Comments RHEL 7 any - Kind doesn't support RHEL 7 RHEL 8 RHEL 9.4 Docker Yes RHEL 9.4 Podman No Issues with Ray job executions Ubuntu 24-04 Docker Yes Ubuntu 24-04 Podman Windows WSL2 Docker Yes Windows WSL2 Podman MacOS amd64 Docker Yes MacOS amd64 Podman MacOS arm64 Docker MacOS arm64 Podman No Issues with Ray job executions"},{"location":"kfp/doc/setup/#preinstalled-software-components","title":"Preinstalled software components","text":"<p>Depending on whether a Kind cluster or an existing Kubernetes cluster is used, different software packages need to be preinstalled.</p>"},{"location":"kfp/doc/setup/#kind-deployment","title":"Kind deployment","text":"<p>The following programs should be manually installed:</p> <ul> <li>Helm 3.10.0 or greater must be installed and configured on your machine.</li> <li>Kind tool for running local Kubernetes clusters 0.14.0 or newer must be installed on your machine.</li> <li>Kubectl 1.26 or newer must be installed on your machine.</li> <li>MinIO Client (mc) must be installed on your machine. Please  choose your OS system, and process according to \"(Optional) Install the MinIO Client\". You have to install the <code>mc</code> client only.</li> <li>git client, we use git client to clone installation repository</li> <li>lsof usually it is part of Linux or MacOS distribution.</li> <li>Container agent such as Docker or Podman</li> </ul>"},{"location":"kfp/doc/setup/#existing-kubernetes-cluster","title":"Existing Kubernetes cluster","text":"<p>Deployment on an existing cluster requires less pre-installed software Only the following programs should be manually installed:</p> <ul> <li>Helm 3.10.0 or greater must be installed and configured on your machine.</li> <li>Kubectl 1.26 or newer must be installed on your machine, and be  able to connect to the external cluster.</li> <li>Deployment of the test data requires MinIO Client (mc) Please  choose your OS system, and process according to \"(Optional) Install the MinIO Client\". Only the <code>mc</code> client should be installed.</li> </ul>"},{"location":"kfp/doc/setup/#installation-steps","title":"Installation steps","text":"<p>Before installation, you have to decide which KFP version do you want to use.  In order to use KFP v2, please set the following environment variable:</p> <pre><code>export KFPv2=1\n</code></pre> <p>Now, you can create a Kind cluster with all required software installed using the following command: </p> <p><pre><code> make -C scripts/k8s-setup setup\n</code></pre> from this main package directory. If you do not want to upload the testing data into the locally deployed Minio, and reduce memory footprint, please set: <pre><code>export POPULATE_TEST_DATA=0\n</code></pre> You can access the KFP dashboard at http://localhost:8080/ and the MinIO dashboard at http://localhost:8090/</p>"},{"location":"kfp/doc/setup/#installation-on-an-existing-kubernetes-cluster","title":"Installation on an existing Kubernetes cluster","text":"<p>Alternatively you can deploy pipeline to the existing Kubernetes cluster. </p> <p>In order to execute data transformers on the remote Kubernetes cluster, the following packages should be installed on the cluster:</p> <ul> <li>KubeFlow Pipelines (KFP). Currently, we use  upstream Argo-based KFP v1.</li> <li>KubeRay controller and  KubeRay API Server </li> </ul> <p>You can install the software from their repositories, or you can use our installation scripts.</p> <p>Once your local kubectl is configured to connect to the external cluster do the following: <pre><code>export EXTERNAL_CLUSTER=1\nmake -C scripts/k8s-setup setup\n</code></pre></p> <ul> <li> <p>In addition, you should configure external access to the KFP UI (<code>svc/ml-pipeline-ui</code> in the <code>kubeflow</code> ns) and the Ray  Server API (<code>svc/kuberay-apiserver-service</code> in the <code>kuberay</code> ns). Depends on your cluster and its deployment it can be  LoadBalancer services, Ingresses or Routes. </p> </li> <li> <p>Optionally, you can upload the test data into the MinIO Object Store, deployed as part of KFP. In  order to do this, please provide external access to the Minio (<code>svc/minio-service</code> in the <code>kubeflow</code> ns) and execute the  following commands from the root directory:  <pre><code>export MINIO_SERVER=&lt;Minio external URL&gt;\nkubectl apply -f scripts/k8s-setup/s3_secret.yaml\nscripts/k8s-setup/populate_minio.sh\n</code></pre></p> </li> </ul>"},{"location":"kfp/doc/setup/#clean-up-the-cluster","title":"Clean up the cluster","text":"<p>If you use an external Kubernetes cluster set the <code>EXTERNAL_CLUSTER</code> environment variable.</p> <p><pre><code>export EXTERNAL_CLUSTER=1\n</code></pre> Now, you can cleanup the external or Kind Kubernetes clusters by running the following command:</p> <pre><code>make -C scripts/k8s-setup clean\n</code></pre>"},{"location":"kfp/doc/simple_transform_pipeline/","title":"Simplest Transform pipeline tutorial","text":"<p>In this example, we implement a pipeline to automate execution of the simple  noop transform</p> <p>In this tutorial, we will show the following:</p> <ul> <li>How to write the <code>noop</code> transform automation pipeline, leveraging KFP components.</li> <li>How to compile a pipeline and deploy it to KFP</li> <li>How to execute pipeline and view execution results</li> </ul> <p>Note: the project and the explanation below are based on KFPv1</p>"},{"location":"kfp/doc/simple_transform_pipeline/#table-of-contents","title":"\ud83d\udcdd Table of Contents","text":"<ul> <li>Implementing pipeline</li> <li>Imports definition </li> <li>Components definition</li> <li>Input parameters definition</li> <li>Pipeline definition</li> <li>Additional configuration</li> <li>Tolerations and node selector</li> <li>Compiling a pipeline</li> <li>Deploying a pipeline</li> <li>Executing pipeline and watching execution results</li> <li>Clean up the cluster</li> </ul>"},{"location":"kfp/doc/simple_transform_pipeline/#implementing-pipeline","title":"Implementing pipeline","text":"<p>Overall implementation roughly contains 5 major sections:</p> <ul> <li>Imports</li> <li>Components definition - definition of the main steps of our pipeline</li> <li>Input parameters definition </li> <li>Pipeline wiring - definition of the sequence of invocation (with parameter passing) of participating components</li> <li>Additional configuration</li> </ul>"},{"location":"kfp/doc/simple_transform_pipeline/#imports-definition","title":"Imports definition","text":"<pre><code>import kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nfrom kfp_support.workflow_support.runtime_utils import (\n  ONE_HOUR_SEC,\n  ONE_WEEK_SEC,\n  ComponentUtils,\n)\nfrom kubernetes import client as k8s_client\n</code></pre>"},{"location":"kfp/doc/simple_transform_pipeline/#components-definition","title":"Components definition","text":"<p>Our pipeline includes 4 steps - compute execution parameters, create Ray cluster, submit and watch Ray job, clean up  Ray cluster. For each step we have to define a component that will execute them:</p> <p><pre><code>    # components\n    base_kfp_image = \"quay.io/dataprep1/data-prep-kit/kfp-data-processing:0.0.2\"\n    # compute execution parameters. Here different transforms might need different implementations. As\n    # a result, instead of creating a component we are creating it in place here.\n    compute_exec_params_op = comp.func_to_container_op(\n      func=ComponentUtils.default_compute_execution_params, base_image=base_kfp_image\n    )\n    # create Ray cluster\n    create_ray_op = comp.load_component_from_file(\"../../../kfp_ray_components/createRayComponent.yaml\")\n    # execute job\n    execute_ray_jobs_op = comp.load_component_from_file(\"../../../kfp_ray_components/executeRayJobComponent.yaml\")\n    # clean up Ray\n    cleanup_ray_op = comp.load_component_from_file(\"../../../kfp_ray_components/cleanupRayComponent.yaml\")\n    # Task name is part of the pipeline name, the ray cluster name and the job name in DMF.\n    TASK_NAME: str = \"noop\"\n</code></pre> Note: here we are using shared components described in this document for <code>create_ray_op</code>,  <code>execute_ray_jobs_op</code> and <code>cleanup_ray_op</code>,  while <code>compute_exec_params_op</code> component is built inline, because it might differ significantly. For \"simple\" pipeline cases we can use the  default implementation, while, for example for exact dedup, we are using a very specialized one.</p>"},{"location":"kfp/doc/simple_transform_pipeline/#input-parameters-definition","title":"Input parameters definition","text":"<p>The input parameters section defines all the parameters required for the pipeline execution:</p> <pre><code>    # Ray cluster\n    ray_name: str = \"noop-kfp-ray\",  # name of Ray cluster\n    ray_head_options: str = '{\"cpu\": 1, \"memory\": 4, \\\n                 \"image\": \"' + task_image + '\" }',\n    ray_worker_options: str = '{\"replicas\": 2, \"max_replicas\": 2, \"min_replicas\": 2, \"cpu\": 2, \"memory\": 4, \\\n                \"image\": \"' + task_image + '\" }',\n    server_url: str = \"http://kuberay-apiserver-service.kuberay.svc.cluster.local:8888\",\n    # data access\n    data_s3_config: str = \"{'input_folder': 'test/noop/input/', 'output_folder': 'test/noop/output/'}\",\n    data_s3_access_secret: str = \"s3-secret\",\n    data_max_files: int = -1,\n    data_num_samples: int = -1,\n    # orchestrator\n    actor_options: str = \"{'num_cpus': 0.8}\",\n    pipeline_id: str = \"pipeline_id\",\n    code_location: str = \"{'github': 'github', 'commit_hash': '12345', 'path': 'path'}\",\n    # noop parameters\n    noop_sleep_sec: int = 10,\n    # additional parameters\n    additional_params: str = '{\"wait_interval\": 2, \"wait_cluster_ready_tmout\": 400, \"wait_cluster_up_tmout\": 300, \"wait_job_ready_tmout\": 400, \"wait_print_tmout\": 30, \"http_retries\": 5, \"delete_cluster_delay_minutes\": 0}',\n</code></pre> <p>The parameters used here are as follows:</p> <ul> <li>ray_name: name of the Ray cluster</li> <li>ray_head_options: head node options, containing the following:</li> <li>cpu - number of cpus</li> <li>memory - memory</li> <li>image - image to use</li> <li>image_pull_secret - image pull secret</li> <li>tolerations - (optional) tolerations for the ray pods</li> <li>ray_worker_options: worker node options (we here are using only 1 worker pool), containing the following:</li> <li>replicas - number of replicas to create</li> <li>max_replicas - max number of replicas</li> <li>min_replicas - min number of replicas</li> <li>cpu - number of cpus</li> <li>memory - memory</li> <li>image - image to use</li> <li>image_pull_secret - image pull secret</li> <li>tolerations - (optional) tolerations for the ray pods</li> <li>server_url - server url</li> <li>additional_params: additional (support) parameters, containing the following:</li> <li>wait_interval - wait interval for API server, sec</li> <li>wait_cluster_ready_tmout - time to wait for cluster ready, sec</li> <li>wait_cluster_up_tmout - time to wait for cluster up, sec</li> <li>wait_job_ready_tmout - time to wait for job ready, sec</li> <li>wait_print_tmout - time between prints, sec</li> <li>http_retries - http retries for API server calls</li> <li>data_s3_access_secret - s3 access secret</li> <li>data_s3_config - s3 configuration</li> <li>data_max_files - max files to process</li> <li>data_num_samples - num samples to process</li> <li>actor_options - actor options</li> <li>pipeline_id - pipeline id</li> <li>code_location - code location</li> <li>noop_sleep_sec - noop sleep time</li> </ul> <p>Note that here we are specifying initial values for all parameters that will be propagated to the workflow UI (see below)</p>"},{"location":"kfp/doc/simple_transform_pipeline/#pipeline-definition","title":"Pipeline definition","text":"<p>Now, when all components and input parameters are defined, we can implement pipeline wiring defining sequence of  component execution and parameters submitted to every component. </p> <pre><code>    # create clean_up task\n    clean_up_task = cleanup_ray_op(ray_name=ray_name, run_id=dsl.RUN_ID_PLACEHOLDER, server_url=server_url, additional_params=additional_params)\n    ComponentUtils.add_settings_to_component(clean_up_task, ONE_HOUR_SEC * 2)\n    # pipeline definition\n    with dsl.ExitHandler(clean_up_task):\n      # compute execution params\n      compute_exec_params = compute_exec_params_op(\n        worker_options=ray_worker_options,\n        actor_options=actor_options,\n      )\n      ComponentUtils.add_settings_to_component(compute_exec_params, ONE_HOUR_SEC * 2)\n      # start Ray cluster\n      ray_cluster = create_ray_op(\n        ray_name=ray_name,\n        run_id=dsl.RUN_ID_PLACEHOLDER,\n        ray_head_options=ray_head_options,\n        ray_worker_options=ray_worker_options,\n        server_url=server_url,\n        additional_params=additional_params,\n      )\n      ComponentUtils.add_settings_to_component(ray_cluster, ONE_HOUR_SEC * 2)\n      ray_cluster.after(compute_exec_params)\n      # Execute job\n      execute_job = execute_ray_jobs_op(\n        ray_name=ray_name,\n        run_id=dsl.RUN_ID_PLACEHOLDER,\n        additional_params=additional_params,\n        # note that the parameters below are specific for NOOP transform\n        exec_params={\n          \"data_s3_config\": data_s3_config,\n          \"data_max_files\": data_max_files,\n          \"data_num_samples\": data_num_samples,\n          \"num_workers\": compute_exec_params.output,\n          \"worker_options\": actor_options,\n          \"pipeline_id\": pipeline_id,\n          \"job_id\": dsl.RUN_ID_PLACEHOLDER,\n          \"code_location\": code_location,\n          \"noop_sleep_sec\": noop_sleep_sec,\n        },\n        exec_script_name=EXEC_SCRIPT_NAME,\n        server_url=server_url,\n      )\n      ComponentUtils.add_settings_to_component(execute_job, ONE_WEEK_SEC)\n      ComponentUtils.set_s3_env_vars_to_component(execute_job, data_s3_access_secret)\n      execute_job.after(ray_cluster)\n</code></pre> <p>Here we first create <code>cleanup_task</code> and the use it as an  exit handler which will be  invoked either the steps into it succeeded or failed.</p> <p>Then we create each individual component passing it required parameters and specify execution sequence, for example (<code>ray_cluster.after(compute_exec_params)</code>).</p>"},{"location":"kfp/doc/simple_transform_pipeline/#additional-configuration","title":"Additional configuration","text":"<p>The final thing that we need to do is set some pipeline global configuration:</p> <pre><code>    # Configure the pipeline level to one week (in seconds)\n    dsl.get_pipeline_conf().set_timeout(ONE_WEEK_SEC)\n</code></pre>"},{"location":"kfp/doc/simple_transform_pipeline/#kfp-pods-toleration-and-node-selector-optional","title":"KFP pods Toleration and node selector (Optional)","text":"<p>To apply kuberenetes Tolerations or nodeSelector to KFP pods, you need to set <code>KFP_TOLERATIONS</code> or <code>KFP_NODE_SELECTOR</code> environment variables respectively before compiling the pipeline. Here's an example:</p> <p><pre><code>export KFP_TOLERATIONS='[{\"key\": \"key\",\"operator\": \"Equal\", \"value1\": \"value\", \"effect\": \"NoSchedule\"}]'\n\nexport KFP_NODE_SELECTOR='{\"label_key\":\"cloud.google.com/gke-accelerator\",\"label_value\":\"nvidia-tesla-p4\"}'\n</code></pre> In KFP v1, setting <code>KFP_TOLERATIONS</code> will apply to the Ray pods, overriding any tolerations specified in the <code>ray_head_options</code> and <code>ray_worker_options</code> pipeline parameters if they are present.</p>"},{"location":"kfp/doc/simple_transform_pipeline/#compiling-a-pipeline","title":"Compiling a pipeline","text":"<p>To compile pipeline execute <code>make workflow-build</code> command in the same directory where your pipeline is. </p>"},{"location":"kfp/doc/simple_transform_pipeline/#deploying-a-pipeline","title":"Deploying a pipeline","text":"<p>Prepare local Kind or external Kubernetes cluster as described in Set Up a cluster</p> <p>Once the cluster is up, go to the kfp endpoint (<code>localhost:8080/kfp/</code> for Kind cluster, the end point of the external existing  cluster depends on the KFP end-point configuration), which will bring up  KFP UI, see below:</p> <p></p> <p>Click on the <code>Upload pipeline</code> link and follow instructions on the screen to upload your file (<code>noop_wf.yaml</code>) and name pipeline noop. Once this is done, you should see something as follows:</p> <p></p>"},{"location":"kfp/doc/simple_transform_pipeline/#executing-pipeline-and-watching-execution-results","title":"Executing pipeline and watching execution results","text":"<p>Before we can run the pipeline we need to create required secrets (one for image loading in case of secured  image registry and one for S3 access). As KFP is deployed in <code>kubeflow</code> namespace, workflow execution will happen there as well, which means that secrets have to be created there as well.</p> <p>When the MinIO Object Store, deployed as part of KFP, is used, its access secret is deployed as part of the cluster preparation,  see s3_secret.yaml.  Creation a secret to pull images from a private repository described here</p> <p>Once this is done we can execute the workflow. </p> <p>On the pipeline page (above) click on the <code>create run</code> button. You will see the list of the parameters, that you can redefine or use the default values that we specified above. After that, go to the bottom of the page and click the <code>start</code> button</p> <p>This will start workflow execution. Once it completes you will see something similar to below </p> <p></p> <p>Note that the log (on the left) has the complete execution log.</p> <p>Additionally, the log is saved to S3 (location is denoted but the last line in the log)</p>"},{"location":"kfp/doc/simple_transform_pipeline/#clean-up-the-cluster","title":"Clean up the cluster","text":"<p>The cluster clean up is described at Clean up the cluster</p>"},{"location":"kfp/kfp_ray_components/","title":"KFP components","text":""},{"location":"kfp/kfp_ray_components/#kfp-components_1","title":"KFP components","text":"<p>All data processing pipelines have the same <code>shape</code>. They all compute execution parameters, create Ray cluster, execute Ray job and then delete the cluster. With the exception of computing execution parameters all of the steps, although receiving different parameters are identical.</p> <p>To simplify implementation of the data processing KFP pipelines, this directory provides several components.</p> <p>As defined by KFP documentation <pre><code>A pipeline component is a self-contained set of code that performs one step in a workflow. \n</code></pre></p> <p>The first step in creation of components its implementation. The framework automation includes the following 3 components:</p> <ul> <li>Create Ray cluster is responsible for creation of the Ray cluster. Its implementation is    based on the RayRemoteJobs class</li> <li>execute Ray job is responsible for submission of the Ray job, watching its execution,   periodically printing job execution log and completing, once the job is completed. Its implementation is   based on the RayRemoteJobs class</li> <li>clean up Ray cluster is responsible for deletion of the Ray cluster, thus freeing   up cluster resources. Its implementation is based on the    RayRemoteJobs class</li> </ul> <p>Once the components are implemented we also implement their interfaces as a component specification which defines:</p> <ul> <li>The component\u2019s inputs and outputs.</li> <li>The container image that your component\u2019s code runs in, the command to use to run your component\u2019s code, and the  command-line arguments to pass to your component\u2019s code.</li> <li>The component\u2019s metadata, such as the name and description.</li> </ul> <p>Components specifications are provided here:</p> <ul> <li>Create Ray cluster Component</li> <li>execute Ray job component</li> <li>clean up Ray cluster component</li> </ul>"},{"location":"kfp/kfp_ray_components/#building-the-docker-image","title":"Building the docker image","text":"<p>To build the component docker image first execute the following commands to set the details of the docker registry as environment variables:</p> <pre><code>export DOCKER_SERVER=&lt;&gt; # for example us.icr.io \nexport DOCKER_USERNAME=iamapikey\nexport DOCKER_EMAIL=iamapikey\nexport DOCKER_PASSWORD=&lt;PASSWORD&gt;\n</code></pre> <p>Then build the image:</p> <pre><code>make build\nmake publish\n</code></pre>"},{"location":"kfp/kfp_support_lib/","title":"KFP support library","text":"<p>This provides support for implementing KFP pipelines automating transform's execution. It comprises 3 main modules</p> <ul> <li>shared_workflow_support </li> <li>kfp_v1_workflow_support</li> <li>kfp_v2_workflow_support</li> </ul> <p>Depends on the using KFV version either <code>kfp_v1_workflow_support</code> or <code>kfp_v2_workflow_support</code> should be used.</p> <p>See also how these libraries are used for kfp components implementation and implementation of the actual workflow</p>"},{"location":"kfp/kfp_support_lib/#development","title":"Development","text":""},{"location":"kfp/kfp_support_lib/#requirements","title":"Requirements","text":"<ol> <li>python 3.10 or later</li> <li>git command line tools</li> <li>pre-commit</li> <li>twine (pip install twine)<ul> <li>but on Mac you may have to include a dir in your PATH, such as <code>export PATH=$PATH:/Library/Frameworks/Python.framework/Versions/3.10/bin</code></li> </ul> </li> </ol>"},{"location":"kfp/kfp_support_lib/#git","title":"Git","text":"<p>Simple clone the repo and set up the pre-commit hooks. <pre><code>git clone git@github.com:IBM/data-prep-kit.git\ncd kfp/kfp_support_lib\npre-commit install\n</code></pre> If you don't have pre-commit, you can install from here</p>"},{"location":"kfp/kfp_support_lib/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>The process of creating a release for <code>fm_data_processing_kfp</code> package  involves the following steps:</p> <p>cd to the package directory.</p> <p>update the version in .make.versions file.</p> <p>run <code>make set-versions</code> and <code>make build</code> and <code>make publish</code>.</p>"},{"location":"kfp/kfp_support_lib/#testing","title":"Testing","text":"<p>To run the package tests perform the following:</p> <p>To begin with, establish a Kind cluster and deploy all required components by executing the makfefile command in the main directory of this repository. As an alternative, you can manually execute the instructions provided in the README.md file.</p> <pre><code>make setup\n</code></pre> <p>The next step is to deploy the <code>data-prep-kit-kfp</code> package locally within a Python virtual environment.</p> <pre><code>make  build\n</code></pre> <p>lastly, execute the tests:</p> <pre><code>make test\n</code></pre>"},{"location":"kfp/kfp_support_lib/#cleanup","title":"Cleanup","text":"<p>It is advisable to execute the following command prior to running <code>make test</code> once more. This will ensure that any  previous test runs resources are removed before starting new tests.</p> <pre><code>kubectl delete workflows -n kubeflow --all\n</code></pre>"},{"location":"kfp/kfp_support_lib/kfp_v1_workflow_support/","title":"Workflow Support Library that depends on KFPv1","text":"<p>This provides support for implementing KFP pipelines automating transform's execution.</p>"},{"location":"kfp/kfp_support_lib/kfp_v2_workflow_support/","title":"Workflow Support Library that depends on KFPv2","text":"<p>This provides support for implementing KFP pipelines automating transform's execution.</p>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/","title":"Shared Workflow Support","text":"<p>This provides support for implementing KFP pipelines automating transform's execution. This library is not dependent on  KFP version. KFP dependent modules are in kfp_v1_workflow_support and  kfp_v2_workflow_support</p> <p>this module combines 2 inner modules</p> <ul> <li>python apiserver client, which is a copy of Kuberay API server-client python APIs We added it into the project, because these APIs are not exposed by any PyPi.</li> <li>runtime_utils </li> </ul>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#development","title":"Development","text":""},{"location":"kfp/kfp_support_lib/shared_workflow_support/#requirements","title":"Requirements","text":"<ol> <li>python 3.10 or later</li> <li>git command line tools</li> <li>pre-commit</li> <li>twine (pip install twine)<ul> <li>but on Mac you may have to include a dir in your PATH, such as <code>export PATH=$PATH:/Library/Frameworks/Python.framework/Versions/3.10/bin</code></li> </ul> </li> </ol>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#git","title":"Git","text":"<p>Simple clone the repo and set up the pre-commit hooks. <pre><code>git clone git@github.com:IBM/data-prep-kit.git\ncd kfp/kfp_support_lib/shared_workflow_support\npre-commit install\n</code></pre> If you don't have pre-commit, you can install from here</p>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>The process of creating a release for the package involves the following steps:</p> <ul> <li>cd to the package directory.</li> <li>update the <code>DPK_LIB_KFP_SHARED</code> version in .make.versions file.</li> <li>run <code>make set-versions</code> and <code>make build</code> and <code>make publish</code>.</li> </ul>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#testing","title":"Testing","text":"<p>To run the package tests perform the following:</p> <p>To begin with, establish a Kind cluster and deploy all required components by executing the makfefile command in the main directory of this repository. As an alternative, you can manually execute the instructions provided in the README.md file.</p> <pre><code>make setup\n</code></pre> <p>The next step is to deploy the <code>data-prep-kit-kfp</code> package locally within a Python virtual environment.</p> <pre><code>make  build\n</code></pre> <p>lastly, execute the tests:</p> <pre><code>make test\n</code></pre>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#cleanup","title":"Cleanup","text":"<p>It is advisable to execute the following command prior to running <code>make test</code> once more. This will ensure that any  previous test runs resources are removed before starting new tests.</p> <p>```bash kubectl delete workflows -n kubeflow --all</p>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/src/python_apiserver_client/","title":"KubeRay API server APIs","text":"<p>This is a copy of Kuberay API server-client python APIs Because these APIs are not exposed by any PyPi, we added them to the project</p>"},{"location":"kfp/pipeline_generator/single-pipeline/","title":"Index","text":""},{"location":"kfp/pipeline_generator/single-pipeline/#steps-to-generate-a-new-pipeline","title":"Steps to generate a new pipeline","text":"<ul> <li>create a <code>pipeline_definitions.yaml</code> file for the required task (similar to the example pipeline_definitions.yaml for the noop task).</li> <li>execute <code>make -C ../../../transforms workflow-venv</code> from this directory</li> <li>execute <code>source ../../../transforms/venv/bin/activate</code></li> <li>execute <code>./run.sh --config_file &lt;pipeline_definitions_file_path&gt; --output_dir_file &lt;destination directory&gt;</code>. When <code>pipeline_definitions_file_path</code> is the path of the <code>pipeline_definitions.yaml</code> file that defines the pipeline and <code>destination directory</code> is a directory where new pipeline file  will be generated.</li> </ul>"},{"location":"kfp/pipeline_generator/superpipeline/","title":"Index","text":""},{"location":"kfp/pipeline_generator/superpipeline/#steps-to-generate-a-new-super-pipeline-in-kfp-v1","title":"Steps to generate a new super pipeline in KFP v1.","text":"<ul> <li>The super pipeline allows you to execute several transforms within a single pipeline. For more details, refer multi_transform_pipeline.md.</li> <li>Create a <code>super_pipeline_definitions.yaml</code> file for the required task. You can refer to the example super_pipeline_definitions.yaml.</li> <li>execute <code>make -C ../../../transforms workflow-venv</code> from this directory</li> <li>execute <code>source ../../../transforms/venv/bin/activate</code></li> <li>Execute <code>./run.sh --config_file &lt; super_pipeline_definitions.yaml&gt; --output_dir_file &lt;destination_directory&gt;</code>. Here, <code>super_pipeline_definitions.yaml</code> is the super pipeline definition file, that you created above, and <code>destination_directory</code> is the directory where the new super pipeline file will be generated.</li> </ul> <p>NOTE: the <code>component_spec_path</code> is the path to the <code>kfp_ray_components</code> folder and depends on where the workflow is compiled.</p>"},{"location":"kfp/superworkflows/ray/kfp_v2/","title":"Chaining transforms using KFP V2","text":"<p>As in super pipelines of KFP v1, we want to offer an option of running a series of transforms one after the other on the data. But, in KFP v2 we can make it easier to chain transforms using the nested pipelines that KFP v2 offers.</p> <p>One example of chaining <code>noop</code> and <code>document id</code> transforms can be found here. When running this pipeline it appears as hierarchical graph with two nested pipelines, one for each transform as shown in the following screenshots.</p> <p><code>root</code> Layer </p> <p><code>root -&gt; noop-ray-pipeline</code> Layer </p> <p><code>root -&gt; noop-ray-pipeline -&gt; exit-handler-1</code> Layer </p> <p>Another useful feature of the KFP v2 is the <code>Json</code> editor for the <code>dict</code> type input parameter as shown here: </p>"},{"location":"kfp/superworkflows/ray/kfp_v2/#main-differences-from-kfp-v1-superpipeline","title":"Main differences from KFP v1 superpipeline:","text":"<ul> <li>It is not required to upload the transforms pipelines before running the superpipeline. So, when compiling the superpipeline code it gets the up to date versions of the transforms.</li> <li>It creates just one run that includes all the nested transfroms and their sub-tasks.</li> <li>No need for additional component as <code>executeSubWorkflowComponent.yaml</code>. All the implementation in the same pipeline file.</li> <li>In superpipelines of KFP v1 there exists an option to override the common parameters with specific values for each one of the transforms. This option is missing in the KFP v2 superpipelines.</li> </ul>"},{"location":"kfp/superworkflows/ray/kfp_v2/#how-to-compile-the-superpipeline","title":"How to compile the superpipeline","text":"<pre><code>cd kfp/superworkflows/ray/kfp_v2/\nmake clean\nexport KFPv2=1\nexport PYTHONPATH=../../../../transforms\nmake workflow-build\n</code></pre>"},{"location":"scripts/k8s-setup/","title":"Building Kind cluster with everything installed","text":""},{"location":"scripts/k8s-setup/#pre-requirements","title":"Pre-requirements","text":"<ul> <li>Supported platforms</li> <li>Preinstalled software components</li> <li>Preparing a Kind cluster for testing</li> </ul> <p>As an alternative, you can execute the following manual installation instructions:</p>"},{"location":"scripts/k8s-setup/#create-cluster","title":"Create cluster","text":"<p>Run the following command to create the cluster:</p> <pre><code>cd /tmp\ngit clone https://github.com/IBM/data-prep-kit.git\ncd data-prep-kit\nexport REPOROOT=$PWD\nkind create cluster --name dataprep --config ${REPOROOT}/scripts/k8s-setup/kind-cluster-config.yaml\n</code></pre> <p>Note that by default this will create a kind cluster with 2 worker nodes. If you would like a different amount of node, modify cluster configuration</p>"},{"location":"scripts/k8s-setup/#install-kfp","title":"Install KFP","text":"<p>Install Kubeflow Pipelines and wait for it to be ready:</p> <pre><code># Set required KFP version. You can reference to the latest supported version in the [requirements.env](https://github.com/IBM/data-prep-kit/blob/dev/scripts/k8s-setup/requirements.env) file.\n# Currently, we support 1.8.5 for KFPv1 and 2.2.0 for KFP v2\nexport PIPELINE_VERSION=1.8.5\ncd $REPOROOT/scripts/k8s-setup/tools/ &amp;&amp; ./install_kubeflow.sh deploy &amp;&amp; cd -\nkubectl wait --for=condition=ready --all pod -n kubeflow --timeout=300s\n</code></pre>"},{"location":"scripts/k8s-setup/#install-kuberay","title":"Install KubeRay","text":"<p>Install Kuberay:</p> <pre><code>cd $REPOROOT/scripts/k8s-setup/tools/ &amp;&amp; KUBERAY_APISERVER=1.1.0 KUBERAY_OPERATOR=1.0.0 ./install_kuberay.sh deploy &amp;&amp; cd -\nkubectl wait --for=condition=ready --all pod -n kuberay --timeout=300s\n</code></pre>"},{"location":"scripts/k8s-setup/#install-ngnix","title":"Install NGNIX","text":"<p>To access the API server and Kubeflow pipeline UI externally, we make use NGINX ingress.</p> <p>Install Ingress NGNIX for KFP, RAY and MinIO and wait for it to be ready:</p> <pre><code>${REPOROOT}/scripts/k8s-setup/tools/install_nginx.sh deploy\nkubectl wait --namespace ingress-nginx \\\n          --for=condition=ready pod \\\n          --selector=app.kubernetes.io/component=controller \\\n          --timeout=90s\n</code></pre> <p>To deploy the ingress for Ray API Server, KFP and MinIO execute the following: <pre><code>kubectl apply -f $REPOROOT/scripts/k8s-setup/ray_api_server_ingress.yaml\nkubectl apply -f $REPOROOT/scripts/k8s-setup/kfp_ingress.yaml\nkubectl apply -f $REPOROOT/scripts/k8s-setup/minio_ingress.yaml\n</code></pre></p> <p>Open the Kubeflow Pipelines UI at  http://localhost:8080/</p>"},{"location":"scripts/k8s-setup/#working-with-a-minio-server-instead-of-s3-storage","title":"Working with a MinIO server instead of S3 storage","text":"<p>You can work with a real S3 storage, but for testing you can use the Mino server which is deployed as part of the KFP installation. You can access the Minio dashboard at http://localhost:8090/</p>"},{"location":"scripts/k8s-setup/#create-a-secret","title":"Create a secret","text":"<p>The MinIO service, deployed as a part of KFP, uses a username (<code>minio</code>) as an access_key/password (<code>minio123</code>) as the secret key. A secret needs to be created for accessing MinIO using the following command:</p> <pre><code>kubectl apply -f $REPOROOT/scripts/k8s-setup/s3_secret.yaml\n</code></pre>"},{"location":"scripts/k8s-setup/#copy-test-data","title":"Copy test data","text":"<p>Populating Minio server with test data can be done using <code>mc</code>. Use the following command:</p> <pre><code>$REPOROOT/scripts/k8s-setup/populate_minio.sh\n</code></pre> <p>This file creates an mc alias, creates the test bucket and copies the local test data into MinIO. If you need to load additional data, please load it using additional <code>mc</code> commands, similar to the ones being used by <code>populate_minio.sh</code></p>"},{"location":"scripts/k8s-setup/#cleanup","title":"Cleanup","text":"<p>See Clean up the cluster</p>"},{"location":"tools/ingest2parquet/","title":"INGEST2PARQUET","text":"<p>Please note: This tool is deprecated and will be removed soon.  It is superseded by the transform-based implementation,  code2parquet,  providing identical capability,  but with support for ray-based scalability.</p>"},{"location":"tools/ingest2parquet/#summary","title":"Summary","text":"<p>This Python script is designed to convert raw data files, particularly ZIP files, into Parquet format. It is built to handle concurrent processing of multiple files using multiprocessing for efficient execution. Each file contained within the ZIP is transformed into a distinct row within the Parquet dataset, adhering to the below schema.</p> <p>title: (string)</p> <ul> <li>Description: Path to the file within the ZIP archive.</li> <li>Example: <code>\"title\": \"data/file.txt\"</code></li> </ul> <p>document: (string)</p> <ul> <li>Description: Name of the ZIP file containing the current file.</li> <li>Example: <code>\"document\": \"example.zip\"</code></li> </ul> <p>repo_name:</p> <ul> <li>Description: Name of the repository the code belongs to. Repo_name is same as zip file name.</li> <li>Example: <code>\"repo_name\": \"data\"</code></li> </ul> <p>contents: (string)</p> <ul> <li>Description: Content of the file, converted to a string.</li> <li>Example: <code>\"contents\": \"This is the content of the file.\"</code></li> </ul> <p>document_id: (string)</p> <ul> <li>Description: Unique identifier generated for each file.</li> <li>Example: <code>\"document_id\": \"b1e4a879-41c5-4a6d-a4a8-0d7a53ec7e8f\"</code></li> </ul> <p>ext: (string)</p> <ul> <li>Description: File extension extracted from the file path.</li> <li>Example: <code>\"ext\": \".txt\"</code></li> </ul> <p>hash: (string)</p> <ul> <li>Description: sha256 hash value computed from the file content string.</li> <li>Example: <code>\"hash\": \"a1b2c3d4\"</code></li> </ul> <p>size: (int64)</p> <ul> <li>Description: Size of the file content in bytes.</li> <li>Example: <code>\"size\": 1024</code></li> </ul> <p>date_acquired: (string)</p> <ul> <li>Description: Timestamp indicating when the file was processed.</li> <li>Example: <code>\"date_acquired\": \"2024-03-25T12:00:00\"</code></li> </ul> <p>snapshot: (string)(optional)</p> <ul> <li>Description: Name indicating which dataset it belong to.</li> <li>Example: <code>\"snapshot\": \"github\"</code></li> </ul> <p>programming_language: (string)(optional)</p> <ul> <li>Description: Programming language detected using the file extension.</li> <li>Example: <code>\"programming_language\": \"Java\"</code></li> </ul> <p>domain: (string)(optional)</p> <ul> <li>Description: Name indicating which domain it belong to, whether code, natural language etc..</li> <li>Example: <code>\"domain\": \"code\"</code></li> </ul>"},{"location":"tools/ingest2parquet/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ingest2parquet  configuration for values are as follows: <pre><code>  --detect_programming_lang DETECT_PROGRAMMING_LANG\n                        generate programming language from the file extension\n  --snapshot SNAPSHOT\n                        Name the dataset\n  --domain DOMAIN\n                        To identify whether data is code or natural language\n  --data_s3_cred DATA_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access. access_key: access key help text secret_key: secret key help text url: optional s3 url region: optional s3 region Example: { 'access_key':\n                        'access', 'secret_key': 'secret', 'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud', 'region': 'us-east-1' }\n  --data_s3_config DATA_S3_CONFIG\n                        AST string containing input/output paths. input_folder: Path to input folder of files to be processed output_folder: Path to output folder of processed files Example: { 'input_folder': 's3-path/your-input-bucket',\n                        'output_folder': 's3-path/your-output-bucket' }\n  --data_local_config DATA_LOCAL_CONFIG\n                        ast string containing input/output folders using local fs. input_folder: Path to input folder of files to be processed output_folder: Path to output folder of processed files Example: { 'input_folder': './input',\n                        'output_folder': '/tmp/output' }\n  --data_max_files DATA_MAX_FILES\n                        Max amount of files to process\n  --data_checkpointing DATA_CHECKPOINTING\n                        checkpointing flag\n  --data_data_sets DATA_DATA_SETS\n                        List of data sets\n  --data_files_to_use DATA_FILES_TO_USE\n                        list of files extensions to choose\n  --data_num_samples DATA_NUM_SAMPLES\n                        number of random files to process\n</code></pre></p>"},{"location":"tools/ingest2parquet/#running","title":"Running","text":"<p>We provide several demos of the script usage for different data storage options: </p>"},{"location":"tools/ingest2parquet/#local-file-system","title":"local file system","text":"<p>This script processes data stored locally on the system. It sets up parameters for local file paths and invokes the ingest2parquet() function from ingest2parquet.py to convert raw data files to Parquet format.</p> <p>Run using make targets.</p> <p><code>run-local-sample</code> - runs src/ingest2parquet_local.py</p> <p>Run the script without any command-line arguments.</p> <pre><code>make venv\nsource venv/bin/activate\ncd src\npython ingest2parquet_local.py\n</code></pre> <p>Run the script via command-line </p> <pre><code>python ingest2parquet.py \\\n    --detect_programming_lang True \\\n    --snapshot github \\\n    --domain code \\\n    --data_local_config '{\"input_folder\": \"../test-data/input\", \"output_folder\": \"../test-data/output\"}' \\\n    --data_files_to_use '[\".zip\"]'\n</code></pre>"},{"location":"tools/ingest2parquet/#s3","title":"s3","text":"<p>This script is designed to process data stored on an S3 bucket. It sets up necessary parameters for accessing the S3 bucket and invokes the ingest2parquet() function from ingest2parquet.py to convert raw data files to Parquet format.</p> <p>To execute the script with S3 functionality, we utilize minio. </p> <p><code>make minio-start</code> - To start minio server <code>make minio-load</code> - To load the data into minio <code>make minio-stop</code> - To stop the minio server</p> <p>Please consult the documentation for further guidance: transform-s3-testing </p> <p>** Run script using make targets ** <code>run-s3-sample</code> : Starts minio server and load data from test-data into local minio for S3 access and runs src/ ingest2parquet_s3.py</p> <p>Run the script without any command-line arguments.</p> <pre><code>make venv\nsource venv/bin/activate\ncd src\npython ingest2parquet_s3.py\n</code></pre> <p>Run the script via command-line </p> <pre><code>python ingest2parquet.py \\\n    --detect_programming_lang True \\\n    --snapshot github \\\n    --domain code \\\n    --data_s3_cred '{\"access_key\": \"localminioaccesskey\", \"secret_key\": \"localminiosecretkey\", \"url\": \"http://localhost:9000\"}' \\\n    --data_s3_config '{\"input_folder\": \"test/ingest2parquet/input\", \"output_folder\": \"test/ingest2parquet/output\"}' \\\n    --data_files_to_use '[\".zip\"]'\n</code></pre> <p>The output directory will contain both the new genrated parquet files  and the <code>metadata.json</code> file.</p>"},{"location":"tools/ingest2parquet/#metadata-fields","title":"Metadata Fields","text":"<p>The metadata.json file contains following essential information regarding the processing of raw data files to Parquet format:</p> <p><code>total_files_given</code>: Total number of raw data files provided for processing. <code>total_files_processed</code>: Number of files successfully processed and converted to Parquet format. <code>total_files_failed_to_processed</code>: Count of files that encountered processing errors and failed conversion. <code>total_no_of_rows</code>: Aggregate count of rows across all successfully processed files. <code>total_bytes_in_memory</code>: Total memory usage in bytes for all processed data files. <code>failure_details</code>: Information about files that failed processing, including their paths and error messages.</p>"},{"location":"tools/ingest2parquet/#building-the-docker-image","title":"Building the Docker Image","text":"<pre><code>% make image \n</code></pre>"},{"location":"tools/ingest2parquet/#run-using-docker-image","title":"Run using docker image","text":"<pre><code>docker run -it -v $(pwd)/test-data/input:/test-data/input -v $(pwd)/test-data/output:/test-data/output quay.io/dataprep1/data-prep-kit/ingest2parquet:0.1 sh -c \"python ingest2parquet.py \\\n    --detect_programming_lang True \\\n    --snapshot github \\\n    --domain code \\\n    --data_local_config '{\\\"input_folder\\\": \\\"/test-data/input\\\", \\\"output_folder\\\":\\\"/test-data/output\\\"}' \\\n    --data_files_to_use '[\\\".zip\\\"]'\"\n</code></pre> <p>In addition, there are some useful <code>make</code> targets (see conventions above): * <code>make venv</code> - creates the virtual environment. * <code>make test</code> - runs the tests in test directory * <code>make build</code> - to build the docker image * <code>make help</code> - displays the available <code>make</code> targets and help text.</p>"},{"location":"transforms/","title":"Transforms","text":"<p>The transformation framework is designed to operate on rows of columnar data, generally contained in parquet files and read as pyarrow tables.</p> <p>Transforms are written to process the table to, for example:</p> <ul> <li>Annotate the tables to add additional data such as document quality score, language, etc.</li> <li>Filter the table to remove or edit rows and/or columns, for example to remove rows from blocked domain.</li> </ul> <p>While these transformation modules were originally built for pre-training, they are also useful for fine-tuning data preparation.</p>"},{"location":"transforms/#annotating-transforms","title":"Annotating Transforms","text":"<p>Annotating transforms examine 1 or more columns of data, typically a content column containing a document to be annotated.  The content is often spoken/text or programming language, generally to build a large language model (LLM).  Examples of annotation might include:</p> <ul> <li>Language identification - an additional string column is added to identify the language of the document content.</li> <li>Document quality - an additional float column is added to associated a quality score with the document.</li> <li>Block listing - an addtional boolean column is added that indicates if the content source url   (in one of the columns) is from a blocked domain.</li> </ul>"},{"location":"transforms/#filtering-transforms","title":"Filtering Transforms","text":"<p>Filtering transforms modify the rows and/or columns, usually based on associated column values. For example,</p> <ul> <li>Language selection - remove rows that do not match the desired language</li> <li>Document quality threshold - remove rows that do not meet a minimum document quality value.</li> <li>Block listing - remove rows that have been flagged as having been sourced from undesirable domains.</li> </ul>"},{"location":"transforms/#transform-organization","title":"Transform Organization","text":"<p>This directory hierarchy of transforms is organized as follows:</p> <ul> <li><code>universal</code> - transforms applicable across code and language model data include</li> <li><code>language</code> - spoken language model specific transforms</li> <li><code>code</code> - programming language specific transforms.</li> </ul> <p>Each of the <code>universal</code>, <code>language</code> and <code>code</code>  directories contains a directory for a specific transform. Each transform is expected to be a standalone entity that generally runs at scale from within a docker image. As such they each have their own virtual environments for development.</p>"},{"location":"transforms/#transform-project-conventions","title":"Transform Project Conventions","text":"<p>The transform projects all try to use a common set of conventions including code layout, build, documentation and IDE recommendations.  For a transformed named <code>xyz</code>, it is expected to have its project located under one of</p> <p><code>transforms/code/xyz</code> <code>transforms/language/xyz</code>, OR  <code>transforms/universal/xyz</code>.</p>"},{"location":"transforms/#makefile","title":"Makefile","text":"<p>The Makefile is the primary entry point for performing most functions for the build and management of a transform. This includes cleanup, testing, creating the virtual environment, building a docker image and more. Use <code>make help</code> in any directory with a Makefile to see the available targets. Each Makefile generally requires  the following macro definitions:</p> <ul> <li>REPOROOT - specifies a relative path to the local directory that is the root of the repository.</li> <li>TRANSFORM_NAME - specifies the simple name of the transform that will be used in creating pypi artifacts and docker images.</li> <li>DOCKER_IMAGE_VERSION - sets the version of the docker image and is usually set from one of the macros in <code>.make.versions</code> at the top of the repository</li> </ul> <p>These are used with the project conventions outlined below to  build and manage the transform.</p>"},{"location":"transforms/#runtime-organization","title":"Runtime Organization","text":"<p>Transforms support one or more runtimes (e.,g python, Ray, Spark, KFP, etc). Each runtime implementation is placed in a sub-directory under the transform's primary directory, for example:</p> <p><code>transforms/universal/xyz/python</code> <code>transforms/universal/xyz/ray</code> <code>transforms/universal/xyz/spark</code> <code>transforms/universal/xyz/kfp</code></p> <p>A transform only need implement the python runtime, and the others generally build on this.</p> <p>All runtime projects are structured as a standard python project with the following:</p> <ul> <li><code>src</code> - directory contains all implementation code</li> <li><code>test</code> - directory contains test code</li> <li><code>test-data</code> - directory containing data used in the tests</li> <li><code>pyproject.toml</code> or <code>requirements.txt</code> (the latter is being phased out)</li> <li><code>Makefile</code>- runs most operations, try <code>make help</code> to see a list of targets.</li> <li><code>Dockerfile</code> to build the transform and runtime into a docker image </li> <li><code>output</code> - temporary directory capturing any test/local run output.  Ignored by .gitignore.</li> </ul> <p>A virtual environment is created for the runtime project using <code>make venv</code>.</p> <p>In general, all runtime-specific python files use an <code>_&lt;runtime&gt;.py&gt;</code> suffix, and docker images use a <code>-&lt;runtime&gt;</code> suffix in their names.  For example,</p> <ul> <li><code>noop_transform_python.py</code></li> <li><code>test_noop_spark.py</code></li> <li><code>dpk-noop-transform-ray</code></li> </ul> <p>Finally, the command <code>make conventions</code> run from within a runtime directory will examine the runtime project structure and make recommendations.</p>"},{"location":"transforms/#python-runtime","title":"Python Runtime","text":"<p>The python runtime project contains the core transform implementation and its configuration, along with the python-runtime classes to launch the transform. The following organization and  naming conventions are strongly recommended and in some cases required for the Makefile to do its work.</p> <ol> <li><code>src</code> directory contain python source for the transform with the following naming conventions/requirements.<ul> <li><code>xyz_transform.py</code> generally contains the core transform implementation:<ul> <li><code>XYZTransform</code> class implementing the transformation</li> <li><code>XYXTransformConfiguration</code> class that defines CLI configuration for the transform </li> </ul> </li> </ul> </li> <li><code>xyz_transform_python.py</code> - runs the transform on input using the python runtime          * <code>XYZPythonTransformConfiguration</code> class         * main() to start the <code>PythonTransformLauncher</code> with the above.</li> <li><code>test</code> directory contains pytest test sources<ul> <li><code>test_xyz.py</code> - a standalone (non-ray launched) transform test.  This is best for initial debugging.<ul> <li>Inherits from an abstract test class so that to test one needs only to provide test data.</li> </ul> </li> <li><code>test_xyz_python.py</code> - runs the transform via the Python launcher. <ul> <li>Again, inherits from an abstract test class so that to test one needs only to provide test data.</li> </ul> </li> </ul> </li> </ol> <p>Tests are expected to be run from anywhere and so need to use    <code>__file__</code> location to create absolute directory paths to the data in the <code>../test-data</code> directory.    From the command line, <code>make test</code> sets up the virtual environment and PYTHONPATH to include <code>src</code>    From the IDE, you must add the <code>src</code> directory to the project's Sources Root (see below).    Do not add <code>sys.path.append(...)</code> in the test python code.    All test data should be referenced as <code>../test-data</code>.</p>"},{"location":"transforms/#rayspark-runtimes","title":"Ray/Spark Runtimes","text":"<p>These projects are structured in a similar way and replace the python  runtime source and test files with the following:</p> <p><code>src/xyz_transform_[ray|spark].py</code>      * <code>[Ray|Spark]TransformRuntimeConfiguration</code> - runtime configuration class     * contains a main() that launches the runtime <code>test/test_xyz_[ray|spark].py</code> - tests the transform running in the given runtime.</p>"},{"location":"transforms/#configuration-and-command-line-options","title":"Configuration and command line options","text":"<p>A transform generally accepts a dictionary of configuration to control its operation.  For example, the size of a table, the location of a model, etc. These are set either explicitly in dictionaries (e.g. during testing) or from the command line when run from a Ray launcher.</p> <p>When specified on the command line, transform <code>xyz</code> should use an <code>xyz</code> prefix with <code>--xyz_</code> (dash dash) to define its command line options. For example, <code>--xyz_some_cfg somevalue</code> sets the value for the <code>xyz_some_cfg</code> configuration key value to <code>somevalue</code>. To avoid potential collisions with options for the Ray launcher, Data Access Factory and others, it is strongly encouraged to not use single dash options with a single or small number of characters (e.g. -n).</p>"},{"location":"transforms/#release-process","title":"Release process","text":"<p>The transform versions are managed in a central file named <code>.make.versions</code>. This file is where the versions are automatically propagated to the Makefile rules when building and pushing the transform images. When a new transform version is created, the tag of the transform should be updated in this file. If there is no entry for the transform in the file yet, create a new one and add a reference to it in the transform Makefile,  following the format used for other transforms. ore specifically, the entry should be of the following format: <code>&lt;transform image name&gt;_&lt;RUNTIME&gt;_VERSION=&lt;version&gt;</code>,  for example: <code>FDEDUP_RAY_VERSION=0.2.77</code></p>"},{"location":"transforms/#building-the-docker-image","title":"Building the docker image","text":"<p>Generally to build a docker image, one uses the <code>make image</code> command, which uses the <code>Dockerfile</code>, which in turn uses the <code>src</code> and <code>requirements.txt</code> to build the image. Note that the <code>Makefile</code> defines the TRANSFORM_NAME and DOCKER_IMAGE_VERSION and should be redefined if copying from another transform project.</p> <p>To build individual transform image use <code>make -C &lt;path to transform directory&gt;</code>, for example: <code>make -C universal/fdedup image</code>. To push all the images run <code>make push</code>, or <code>make -C &lt;path to transform directory&gt; push</code> for individual transform.</p>"},{"location":"transforms/#ide-setup","title":"IDE Setup","text":"<p>When running in an IDE, such as PyCharm or VS Code, the following are generally required:</p> <ul> <li>From the command line, build the venv using <code>make venv</code>.</li> <li>In the IDE<ul> <li>Set your project/run configuration to use the venv/bin/python as your runtime virtual environment.<ul> <li>In PyCharm, this can be done through the PyCharm-&gt;Settings-&gt;Project...-&gt;Python Interpreter page</li> <li>In VS Code, click on the current Python Interpreter in the bottom right corner and make sure that the Interpreter path is venv/bin/python</li> </ul> </li> <li>Mark the <code>src</code> as a source root so that it is included in your PYTHONPATH when running .py files in the IDE<ul> <li>In Pycharm this can be done by selecting the <code>src</code> directory, and then selecting <code>Mark Directory as</code> -&gt; <code>Sources Root</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"transforms/README-list/","title":"DPK Python Transforms","text":""},{"location":"transforms/README-list/#installation","title":"installation","text":"<p>The transforms are delivered as a standard pyton library available on pypi and can be installed using pip install:</p> <p><code>python -m pip install data-prep-toolkit-transforms</code> or <code>python -m pip install data-prep-toolkit-transforms[ray]</code></p> <p>installing the python transforms will also install  <code>data-prep-toolkit</code></p> <p>installing the ray transforms will also install  <code>data-prep-toolkit[ray]</code></p>"},{"location":"transforms/README-list/#list-of-transforms-in-current-package","title":"List of Transforms in current package","text":"<p>Note: This list includes the transforms that were part of the release starting with data-prep-toolkit-transforms:0.2.1. This list may not always reflect up to date information. Users are encourage to raise an issue in git when they discover missing components or packages that are listed below but not in the current release they get from pypi.</p> <ul> <li>code<ul> <li>code2parquet</li> <li>header_cleanser (Not available on MacOS)</li> <li>code_quality</li> <li>proglang_select</li> </ul> </li> <li>language<ul> <li>doc_chunk</li> <li>doc_quality</li> <li>lang_id</li> <li>pdf2parquet</li> <li>text_encoder</li> <li>pii_redactor</li> </ul> </li> <li>universal<ul> <li>ededup</li> <li>filter</li> <li>resize</li> <li>tokenization</li> <li>doc_id</li> </ul> </li> </ul>"},{"location":"transforms/add_new_kfp_workflow/","title":"Adding new KFP workflows","text":"<p>This README outlines the steps to add a new KFP workflow for a new transform under transforms directory.</p> <p>1) Create a new <code>kfp_ray</code> directory in the transform directory, similar to this directory.</p> <p>2) Create the workflow and add it to <code>kfp_ray</code> directory. It is recommended to use the pipeline generator for that. If the workflow was generated using the pipeline generator also include <code>pipeline_definitions.yaml</code> file used to generate the workflow in the <code>kfp_ray</code> directory.</p> <p>3) Add <code>Makefile</code> file to <code>kfp_ray</code> directory similar to this Makefile example.</p> <p>3) Add the path to the transform input directory in the populate_minio script. This path is used when testing the workflow. 4) Create a GitHub Action for the kfp workflow using the <code>make</code> command in the .github/workflows/ directory. 5) Update the workflows list in README.md file.</p>"},{"location":"transforms/code/code2parquet/","title":"Code2Parquet Transform","text":"<p>This code2parquet transform is designed to convert raw particularly ZIP files contain programming files (.py, .c, .java, etc) ,  into Parquet format.  Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation and python runtime.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/code2parquet/kfp_ray/","title":"Code to Parquet Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/code2parquet/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/code2parquet/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/code2parquet/python/","title":"Code2Parquet","text":""},{"location":"transforms/code/code2parquet/python/#summary","title":"Summary","text":"<p>This code2parquet transform is designed to convert raw particularly ZIP files contain programming files (.py, .c, .java, etc) ,  into Parquet format.  As a transform It is built to handle concurrent processing of Ray-based multiple files using multiprocessing for efficient execution. Each file contained within the ZIP is transformed into a distinct row within the Parquet dataset, adhering to the below schema.</p> <p>title: (string)</p> <ul> <li>Description: Path to the file within the ZIP archive.</li> <li>Example: <code>\"title\": \"data/file.txt\"</code></li> </ul> <p>document: (string)</p> <ul> <li>Description: Name of the ZIP file containing the current file.</li> <li>Example: <code>\"document\": \"example.zip\"</code></li> </ul> <p>repo_name:</p> <ul> <li>Description: The name of the repository to which the code belongs. This should match the name of the zip file containing the repository.</li> <li>Example: <code>\"repo_name\": \"example\"</code></li> </ul> <p>contents: (string)</p> <ul> <li>Description: Content of the file, converted to a string.</li> <li>Example: <code>\"contents\": \"This is the content of the file.\"</code></li> </ul> <p>document_id: (string)</p> <ul> <li>Description: Unique identifier computed as a uuid. </li> <li>Example: <code>\"document_id\": \"b1e4a879-41c5-4a6d-a4a8-0d7a53ec7e8f\"</code></li> </ul> <p>ext: (string)</p> <ul> <li>Description: File extension extracted from the file path.</li> <li>Example: <code>\"ext\": \".txt\"</code></li> </ul> <p>hash: (string)</p> <ul> <li>Description: sha256 hash value computed from the file content string.</li> <li>Example: <code>\"hash\": \"a1b2c3d4\"</code></li> </ul> <p>size: (int64)</p> <ul> <li>Description: Size of the file content in bytes.</li> <li>Example: <code>\"size\": 1024</code></li> </ul> <p>date_acquired: (string)</p> <ul> <li>Description: Timestamp indicating when the file was processed.</li> <li>Example: <code>\"date_acquired\": \"2024-03-25T12:00:00\"</code></li> </ul> <p>snapshot: (string)(optional)</p> <ul> <li>Description: Name indicating which dataset it belong to.</li> <li>Example: <code>\"snapshot\": \"github\"</code></li> </ul> <p>programming_language: (string)(optional)</p> <ul> <li>Description: Programming language detected using the file extension.</li> <li>Example: <code>\"programming_language\": \"Java\"</code></li> </ul> <p>domain: (string)(optional)</p> <ul> <li>Description: Name indicating which domain it belong to, whether code, natural language etc..</li> <li>Example: <code>\"domain\": \"code\"</code></li> </ul>"},{"location":"transforms/code/code2parquet/python/#configuration","title":"Configuration","text":"<p>The set of dictionary keys holding code2parquet  configuration for values are as follows:</p> <p>The transform can be configured with the following key/value pairs from the configuration dictionary. * <code>supported_languages</code> - a dictionary mapping file extensions to language names. * <code>supported_langs_file</code> - used if <code>supported_languages</code> key is not provided,   and specifies the path to a JSON file containing the mapping of languages   to extensions. The json file is expected to contain a dictionary of   languages names as keys, with values being a list of strings specifying the   associated extensions. As an example, see    lang_extensions . * <code>data_access_factory</code> - used to create the DataAccess instance used to read the file specified in <code>supported_langs_file</code>. * <code>detect_programming_lang</code> - a flag that indicates if the language:extension mappings   should be applied in a new column value named <code>programming_language</code>. * <code>domain</code> - optional value assigned to the imported data in the 'domain' column. * <code>snapshot</code> -  optional value assigned to the imported data in the 'snapshot' column.</p>"},{"location":"transforms/code/code2parquet/python/#running","title":"Running","text":""},{"location":"transforms/code/code2parquet/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <ul> <li><code>--code2parquet_supported_langs_file</code> - set the <code>supported_langs_file</code> configuration key. </li> <li><code>--code2parquet_detect_programming_lang</code> - set the <code>detect_programming_lang</code> configuration key. </li> <li><code>--code2parquet_domain</code> - set the <code>domain</code> configuration key. </li> <li><code>--code2parquet_snapshot</code> -  set the <code>snapshot</code> configuration key. </li> </ul>"},{"location":"transforms/code/code2parquet/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/code2parquet_transform_ray.py using command line args</li> <li><code>run-local-sample</code> - runs src/code2parquet.py</li> <li><code>run-s3-sample</code> - runs src/code2parquet.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code2parquet/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code2parquet/ray/","title":"NOOP Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code2parquet/ray/#summary","title":"Summary","text":"<p>This project wraps the code2parquet transform with a Ray runtime.</p>"},{"location":"transforms/code/code2parquet/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>code2parquet transform configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/code/code2parquet/ray/#running","title":"Running","text":""},{"location":"transforms/code/code2parquet/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/code/code2parquet/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/code2parquet_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/code2parquet_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/code2parquet_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code2parquet/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_profiler/","title":"Code Profiler Transform","text":"<p>This module extracts the base syntactic concepts from the multi-language source codes and represent these concepts in a unified langauge-agnostic representation that can be further used for multi-language data profiling. While programming languages expose similar syntactic building blocks to represent programming intent, such as importing packages/libraries, functions, classes, loops, conditionals, comments and others, these concepts are expressed through language-specific grammar, defined by distinct keywords and syntactic form. Our framework abstracts language-specific concepts by transforming them into a unified, language-agnostic representation called universal base syntactic representation (UBSR), referred to as a concept, which is consistently encoded within the proposed schema structure. The current version supports the base syntactic concept for importing/including package/libraries, comments, functions. </p> <p>Table 1 outlines the fields of the UBSR, which maps AST nodes to a structured schema. This schema captures syntactic nodes (based on AST node types) and the relationships between those nodes (derived from AST edges). The UBSR framework currently supports 21 languages, grouped according to their syntactic paradigms.</p> <p>Table 1: UBSR Schema Representation</p> Key Possible Values Description \"nodes\": <code>\"id\"</code> Integer (e.g., <code>0</code>, <code>1</code>) Unique identifier of the node. <code>\"code_snippet\"</code> String (e.g., <code>\"ubsr_package math\"</code>) A snippet of code or a description of the node. <code>\"node_type\"</code> String (e.g., <code>\"ubsr_root\"</code>, <code>\"ubsr_package\"</code>, etc.) Type of node representing various syntactic concepts. <code>\"parents\"</code> Array of Integers (e.g., <code>[1, 2]</code>) List of parent node IDs. <code>\"children\"</code> Array of Integers (e.g., <code>[1, 2]</code>) List of child node IDs. \"metadata\" (within nodes): <code>\"info\"</code> String General information about the node. <code>\"language\"</code> String (<code>\"cpp\"</code>, <code>\"python\"</code>, etc.) Programming language of the node. <code>\"original_code\"</code> String (e.g., <code>\"int main() {...}\"</code>) Original code snippet corresponding to the node. <code>\"loc_original_code\"</code> Integer Line of code of the concept. \"edges\": <code>\"directed_relation\"</code> String (<code>\"parent_node\"</code>) Type of relationship between nodes e.g. parent-child. <code>\"metadata\"</code> Object Additional metadata for the edge, which can be empty. <p>As shown in Table 2, the framework standardizes code representation by categorizing languages within these paradigms for 21 languages. In cases where certain concepts are absent in a language, they are marked as NA in the table. The base syntactic concepts extracted from the UBSR derived from code can be used to derive syntactic and semantic insights of the code data.</p> <p>Table 2: Base Syntactic Concepts Supported by the UBSR across Different Syntactical Paradigms</p> Syntactical Paradigms Languages Supported (Known*) Package Function Comment C-like Syntax C*, Java*, C#, CPP, Objective C, Rust, Golang, Kotlin Yes Yes Yes Scripting and Dynamic Syntax Python*, JavaScript*, Dart, Typescript Yes Yes Yes QML Yes NA Yes Perl Yes Yes NA Functional and Expression-Oriented Syntax Haskell*, Elm*, Agda, D, Nim, Scala Yes Yes Yes Ocaml Yes NA Yes <ul> <li>python - provides the base python-based syntactic concept extractor implementation.</li> <li>ray - provides the base ray-based syntactic concept extractor implementation.</li> </ul> <p>Offline Path for Syntactic Rule Generation</p> <p>The offline path is critical for expanding and refining the syntactic rule database, enabling the UBSR framework to adapt to new languages and syntactic constructs. This process leverages LLMs to generate syntactic rules for languages that are not yet included in the rule database. To achieve this, we utilize a Few-shot Chain of Thought prompting technique, guiding the LLM through a step-by-step rule generation process. By providing carefully curated training exemplars and detailed instructions, this method ensures the LLM can accurately generalize from these examples to produce effective syntactic rules for a wide range of languages. This structured approach enhances the flexibility of the UBSR framework, allowing it to seamlessly handle evolving language constructs.</p> <p>The implementation for UI-based offline customization tool is present here. To run the tool, use the following command.</p> <p><code>streamlit run LLM_runner_app.py</code></p> <p>The high-level system design is as follows:</p> <p></p> <p>For each new target language, the offline phase is utilized to create deterministic rules by harnessing the capabilities of LLMs and working with exemplar code samples from the target language. In this process, Workflow W1 facilitates the creation of rules around syntactic structures based on exemplar code samples, while Workflow W2 is used to establish semantic dimensions for profiling. Subsequently, we derive rules that connect syntactic constructs to the predefined semantic concepts. These rules are then stored in a rule database, ready to be employed during the online phase.</p> <p>In the online phase, the system dynamically generates profiling outputs for any incoming code snippets. This is achieved by extracting concepts from the snippets using the rules in the database and storing these extractions in a tabular format. The structured tabular format allows for generating additional concept columns, which are then utilized to create comprehensive profiling reports.</p>"},{"location":"transforms/code/code_profiler/python/","title":"Code Profiler Transform","text":""},{"location":"transforms/code/code_profiler/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding code_profiler_transform  configuration for values are as follows:</p> <ul> <li>content - specifies the column name in the dataframe that has the code snippet</li> <li>language - specifies the programming languages of the code snippet</li> </ul>"},{"location":"transforms/code/code_profiler/python/#running","title":"Running","text":""},{"location":"transforms/code/code_profiler/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher.</p>"},{"location":"transforms/code/code_profiler/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-local-sample</code> - runs src/code_profiler_local.py</li> <li><code>run-local-python-sample</code> - runs src/code_profiler_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-local-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_profiler/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_profiler/ray/","title":"Code Profiler Transform","text":""},{"location":"transforms/code/code_profiler/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding code_profiler_transform  configuration for values are as follows:</p> <ul> <li>content - specifies the column name in the dataframe that has the code snippet</li> <li>language - specifies the programming languages of the code snippet</li> </ul>"},{"location":"transforms/code/code_profiler/ray/#running","title":"Running","text":""},{"location":"transforms/code/code_profiler/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher.</p>"},{"location":"transforms/code/code_profiler/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-local-ray-sample</code> - runs src/code_profiler_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-local-ray-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_profiler/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_quality/","title":"Code Quality Transform","text":"<p>The Code Quality transforms  captures code specific metrics of input data. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/code_quality/kfp_ray/","title":"Code Quality Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/code_quality/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the Code Quality Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/code_quality/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/code_quality/python/","title":"Code Quality","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code_quality/python/#summary","title":"Summary","text":"<p>This module captures code specific metrics of input data. The implementation is borrowed from the work done in CodeParrot and StarCoder projects. In the current implementation, the module includes the following metrics &amp; reports each metrics in individual column:</p> <ul> <li>line specific metrics include mean &amp; max line length</li> <li>character and token ratio - uses the input tokenizer to tokenize the input data &amp; measure the ratio between the characters and tokens</li> <li>identifies the high occurrence of the keywords \"test \" or \"config\" and tags them as config or test samples</li> <li>tags the samples as autogenerated if the sample contains keywords like <code>auto-generated</code>, <code>autogenerated</code> or <code>automatically generated</code></li> <li>programming language specific identification, where:<ul> <li>if the input sample is <code>python</code> programming language and sample has no reference to constructs like def, class, it is highlighted as <code>has_no_keywords</code> </li> </ul> </li> </ul> <p>This module adds the following fields into the output file:</p> <ul> <li>line_mean</li> <li>line_max</li> <li>total_num_lines</li> <li>avg_longest_lines</li> <li>alphanum_frac</li> <li>char_token_ratio</li> <li>autogenerated</li> <li>config_or_test</li> <li>has_no_keywords</li> <li>has_few_assignments</li> <li>is_xml</li> <li>is_html</li> </ul> <p>It uses a tokenizer to collect metrics specific to token ratio.  It is designed to download the tokenizer from the Huggingface if the input tokenizer is not found in the local cache. By default, it uses codeparrot/codeparrot tokenizer.</p>"},{"location":"transforms/code/code_quality/python/#running","title":"Running","text":""},{"location":"transforms/code/code_quality/python/#launcher-command-line-options","title":"Launcher Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the ray launcher and the python launcher.</p> <ul> <li>\"--contents_column_name\" - input a column name which contains data to process. The default column name: <code>contents</code></li> <li>\"--language_column_name\" - input a column name which contains programming language details. The default column name: <code>language</code></li> <li>\"--tokenizer\" - input a tokenizer to convert the data into tokens. The default tokenizer is <code>codeparrot/codeparrot</code></li> <li>\"--hf_token\" - input the Hugging Face auth token to download the tokenizer. This option is only required for the tokenizer's whose access is restricted in Hugging Face.</li> </ul>"},{"location":"transforms/code/code_quality/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/code_quality_transform_python.py using command line args</li> <li><code>run-local-sample</code> - runs src/code_quality_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_quality/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_quality/ray/","title":"Code Quality","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code_quality/ray/#summary","title":"Summary","text":"<p>This project enables the python malware transform to be run in a Ray runtime. Please see the python project for details on the transform implementation and use.</p>"},{"location":"transforms/code/code_quality/ray/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>Transform configuration options are the same as the base python transform.</p>"},{"location":"transforms/code/code_quality/ray/#running","title":"Running","text":""},{"location":"transforms/code/code_quality/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of ray launcher are available.</p>"},{"location":"transforms/code/code_quality/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-ray-sample</code> - runs src/code_quality_transform.py using command line args</li> <li><code>run-local-ray-sample</code> - runs src/code_quality_local_ray.py</li> <li><code>run-s3-ray-sample</code> - runs src/code_quality_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_quality/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/header_cleanser/","title":"Header Cleanser Transform","text":"<p>The Header cleanser transforms  Detect and remove license and copyright of input data. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime.</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/header_cleanser/kfp_ray/","title":"Header Cleanser Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/header_cleanser/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the header cleanser Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/header_cleanser/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/header_cleanser/python/","title":"Header cleanser","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/header_cleanser/python/#summary","title":"Summary","text":"<p>This module is designed to detect and remove license and copyright information from code files. It leverages the ScanCode Toolkit to accurately identify and process licenses and copyrights in various programming languages.</p> <p>After locating the position of license or copyright in the input code/sample, this module delete/remove those lines and returns the updated code as parquet file.</p>"},{"location":"transforms/code/header_cleanser/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding configuration for values are as follows:</p> <ul> <li>contents_column_name - used to define input column name. Default value is 'contents'.</li> <li>license - write 'true' to remove license from input data else 'false'. By default set as 'true'.</li> <li>copyright - write 'true' to remove copyright from input data else 'false'. by default set as 'true'.</li> </ul>"},{"location":"transforms/code/header_cleanser/python/#running","title":"Running","text":"<p>You can run the header_cleanser_local.py (python-only implementation) or header_cleanser_local_ray.py (ray-based  implementation) to transform the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p>"},{"location":"transforms/code/header_cleanser/python/#running_1","title":"Running","text":""},{"location":"transforms/code/header_cleanser/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the python launcher. * --header_cleanser_contents_column_name - set the contents_column_name configuration key. * --header_cleanser_license - set the license configuration key. * --header_cleanser_copyright - set the copyright configuration key. </p>"},{"location":"transforms/code/header_cleanser/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/header_cleanser_transform_python.py using command line args</li> <li><code>run-local-python-sample</code> - runs src/header_cleanser_local_python.py</li> <li><code>run-local-sample</code> - runs src/header_cleanser_local.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/header_cleanser/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/header_cleanser/ray/","title":"Header cleanser","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/header_cleanser/ray/#summary","title":"Summary","text":"<p>This module is designed to detect and remove license and copyright information from code files. It leverages the ScanCode Toolkit to accurately identify and process licenses and copyrights in various programming languages.</p> <p>After locating the position of license or copyright in the input code/sample, this module delete/remove those lines and returns the updated code as parquet file.</p>"},{"location":"transforms/code/header_cleanser/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>This project wraps the header cleanser transform with a Ray runtime.</p>"},{"location":"transforms/code/header_cleanser/ray/#running","title":"Running","text":""},{"location":"transforms/code/header_cleanser/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of ray launcher are available.</p>"},{"location":"transforms/code/header_cleanser/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-ray-sample</code> - runs src/header_cleanser_transform.py using command line args</li> <li><code>run-local-ray-sample</code> - runs src/header_cleanser_local_ray.py</li> <li><code>run-s3-ray-sample</code> - runs src/header_cleanser_s3_ray.py<ul> <li>Requires prior invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-ray-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/header_cleanser/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/license_select/","title":"License Select","text":"<p>The License Select transform checks if the <code>license</code> of input data is in approved/denied list. It is implemented as per the set of transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/license_select/python/","title":"License Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/license_select/python/#summary","title":"Summary","text":"<p>The License Select transform checks if the <code>license</code> of input data is in approved/denied list. It is implemented as per the set of transform project conventions the following runtimes are available: This filter scans the license column of an input dataset and appends the <code>license_status</code> column to the dataset.</p> <p>The type of the license column can be either string or list of strings. For strings, the license name is checked against the list of approved licenses. For list of strings, each license name in the list is checked against the list of approved licenses, and all must be approved.</p> <p>If the license is approved, the license_status column contains True; otherwise False. </p>"},{"location":"transforms/code/license_select/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding license_select configuration for values are as follows:</p> <p>The transform can be configured with the following key/value pairs from the configuration dictionary.</p> <pre><code># Sample params dictionary passed to the transform\n\n{ \n\"license_select_params\" : {\n        \"license_column_name\": \"license\",\n        \"deny_licenses\": False,\n        \"licenses\": [ 'MIT', 'Apache'],\n        \"allow_no_license\": False,\n    }\n}\n</code></pre> <p>license_column_name - The name of the column with licenses.</p> <p>deny_licenses - A boolean value, True for denied licesnes, False for approved licenses.</p> <p>licenses - A list of licenses used as approve/deny list.</p> <p>allow_no_license - A boolean value, used to retain the values with no license in the column <code>license_column_name</code> </p>"},{"location":"transforms/code/license_select/python/#running","title":"Running","text":""},{"location":"transforms/code/license_select/python/#launcher-command-line-options","title":"Launcher Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the python launcher.</p> <p><code>--lc_license_column_name</code> - set the name of the column holds license to process</p> <p><code>--lc_allow_no_license</code> - allow entries with no associated license (default: false)</p> <p><code>--lc_licenses_file</code> - S3 or local path to allowed/denied licenses JSON file</p> <p><code>--lc_deny_licenses</code> - allow all licences except those in licenses_file (default: false)</p> <ul> <li> <p>The optional <code>lc_license_column_name</code> parameter is used to specify the column name in the input dataset that contains the license information. The default column name is license.</p> </li> <li> <p>The optional <code>lc_allow_no_license</code> option allows any records without a license to be accepted by the filter. If this option is not set, records without a license are rejected.</p> </li> <li> <p>The required <code>lc_licenses_file</code> options allows a list of licenses to be specified. An S3 or local file path should be supplied (including bucket name, for example: bucket-name/path/to/licenses.json) with the file contents being a JSON list of strings. For example:</p> </li> </ul> <p>[     'Apache-2.0',     'MIT'    ]</p> <ul> <li>The optional <code>lc_deny_licenses</code> flag is used when <code>lc_licenses_file</code> specifies the licenses that will be rejected, with all other licenses being accepted. These parameters do not affect handling of records with no license information, which is dictated by the allow_no_license option.</li> </ul>"},{"location":"transforms/code/license_select/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following make targets</p> <p><code>run-cli-sample</code></p> <p><code>run-local-python-sample</code> </p> <p>These targets will activate the virtual environment and set up any configuration needed. Use the -n option of make to see the detail of what is done to run the sample.</p> <p>For example, <pre><code>make run-cli-sample\n</code></pre> ... Then</p> <p>ls output To see results of the transform.</p>"},{"location":"transforms/code/license_select/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/license_select/ray/","title":"License Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/license_select/ray/#summary","title":"Summary","text":"<p>This project wraps the license select transform with a Ray runtime.</p>"},{"location":"transforms/code/license_select/ray/#running","title":"Running","text":""},{"location":"transforms/code/license_select/ray/#launcher-command-line-options","title":"Launcher Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/code/license_select/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-ray-sample</code> </li> <li><code>run-local-ray-sample</code> </li> <li><code>run-s3-ray-sample</code> <ul> <li>Requires prior invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example, </p> <pre><code>make run-cli-ray-sample\n...\n</code></pre> <p>Then </p> <p><pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/license_select/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/malware/","title":"Malware Transform","text":"<p>The Malware Transform  scans code parquet files containing code documents to identify the potential presence of a virus signature adding a column to the input parquet file identifying the virus signature. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/malware/kfp_ray/","title":"Code Malware Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/malware/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/malware/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/malware/python/","title":"Malware Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/malware/python/#summary","title":"Summary","text":"<p>This filter scans the 'contents' column of an input table using ClamAV, and outputs corresponding tables containing 'virus_detection' column (by default).</p> <p>If a virus is detected, the 'virus_detection' column contains the detected virus signature name; otherwise null.</p>"},{"location":"transforms/code/malware/python/#pre-requisites-for-mac","title":"Pre-requisites for Mac","text":"<p>For testing and running this transform on local, we are using a unix socket shared with a docker container. However, docker for mac doesn't support a shared unix socket. For Mac users, ClamAV will be set up by running <code>make venv</code>. If thet script doesn't work for you, please ensure that you have installed <code>clamd</code> command, and it runs with a local unix socket: <code>/var/run/clamav/clamd.ctl</code>.</p>"},{"location":"transforms/code/malware/python/#example-for-manual-set-up-for-mac","title":"Example for manual set up for Mac:","text":"<ol> <li>Install ClamAV with Homebrew     <pre><code>brew install clamav\n</code></pre></li> <li>Copy and edit config files.     <pre><code>cp $(brew --prefix)/etc/clamav/clamd.conf.sample $(brew --prefix)/etc/clamav/clamd.conf\nsed -i '' -e 's/^Example/# Example/' $(brew --prefix)/etc/clamav/clamd.conf\necho \"DatabaseDirectory /var/lib/clamav\" &gt;&gt; $(brew --prefix)/etc/clamav/clamd.conf\necho \"LocalSocket /var/run/clamav/clamd.ctl\" &gt;&gt; $(brew --prefix)/etc/clamav/clamd.conf\ncp $(brew --prefix)/etc/clamav/freshclam.conf.sample $(brew --prefix)/etc/clamav/freshclam.conf\nsed -i '' -e 's/^Example/# Example/' $(brew --prefix)/etc/clamav/freshclam.conf\necho \"DatabaseDirectory /var/lib/clamav\" &gt;&gt; $(brew --prefix)/etc/clamav/freshclam.conf\n</code></pre></li> <li>Create a directory for a local unix socket     <pre><code>sudo mkdir -p /var/run/clamav\nsudo chown $(id -u):$(id -g) /var/run/clamav\n</code></pre></li> <li>Create a direcotry for a database of ClamAV     <pre><code>sudo mkdir -p /var/lib/clamav\nsudo chown $(id -u):$(id -g) /var/lib/clamav\n</code></pre></li> <li>Update a database of ClamAV     <pre><code>freshclam\n</code></pre></li> <li>Edit <code>venv/bin/activate</code>, and add following lines to start <code>clamd</code> by <code>source venv/bin/activate</code> <pre><code>if [ ! -e /var/run/clamav/clamd.ctl ]; then\n    clamd --config-file=$(brew --prefix)/etc/clamav/clamd.conf\nfi\n</code></pre></li> </ol>"},{"location":"transforms/code/malware/python/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>The set of dictionary keys holding MalwareTransform  configuration for values are as follows:</p> <ul> <li>malware_input_column - specifies the input column's name to scan. (default: <code>contents</code>)</li> <li>malware_output_column - specifies the output column's name of the detected virus signature name. (default: <code>virus_detection</code>)</li> </ul>"},{"location":"transforms/code/malware/python/#metadata-fields","title":"Metadata Fields","text":"<p>As shown in the output of the local run of malware transform, the metadata contains several statistics: * Global statistics:   * <code>infected</code>: total number of documents (rows) in which any malwares were detected.    * <code>clean</code>: total number of documents (rows) in which no malwares were detected.</p>"},{"location":"transforms/code/malware/python/#running","title":"Running","text":""},{"location":"transforms/code/malware/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the python launcher and the python launcher.</p> <pre><code>  --malware_input_column MALWARE_INPUT_COLUMN\n                        input column name\n  --malware_output_column MALWARE_OUTPUT_COLUMN\n                        output column name\n</code></pre>"},{"location":"transforms/code/malware/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/malware_transform_python.py using command line args</li> <li><code>run-local-sample</code> - runs src/malware_local.py</li> <li><code>run-local-python-sample</code> - runs src/malware_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/malware/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/malware/ray/","title":"Malware Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/malware/ray/#summary","title":"Summary","text":"<p>This project enables the python malware transform to be run in a Ray runtime. Please see the python project for details on the transform implementation and use.</p>"},{"location":"transforms/code/malware/ray/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>Transform configuration options are the same as the base python transform.</p>"},{"location":"transforms/code/malware/ray/#running","title":"Running","text":""},{"location":"transforms/code/malware/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of ray launcher are available.</p>"},{"location":"transforms/code/malware/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/malware_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/malware_local.py</li> <li><code>run-local-ray-sample</code> - runs src/malware_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/malware/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/proglang_select/","title":"Programming Language Selection Transform","text":"<p>The Programming Language Selection Transform  annotates input parquet files to add a True/False column indicating if the row's language matches one of those specified in the transform configuration. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/proglang_select/kfp_ray/","title":"Programming Language Select Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/proglang_select/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/proglang_select/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/proglang_select/python/","title":"Programming Language Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/proglang_select/python/#summary","title":"Summary","text":"<p>This is a transform which can be used while preprocessing code data. It allows the user to specify the programming languages for which the data should be identifies as matching a defined set of programming languages. It adds a new annotation column which can specify boolean True/False based on whether the rows belong to the specified programming languages. The rows which belongs to the programming languages which are not matched are annotated as False.</p> <p>It requires a text file specifying the allowed languages. It is specified by the command line param <code>proglang_select_allowed_langs_file</code>.  A sample file is included at <code>test-data/languages/allowed-code-languages.lst</code>. The column specifying programming languages is to be specified by commandline params <code>proglang_select_language_column</code>.</p>"},{"location":"transforms/code/proglang_select/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding configuration for values are as follows:</p> <ul> <li>proglang_select_allowed_langs_file - specifies the location of the list of supported languages</li> <li>proglang_select_language_column - specifies the name of the column containing the language</li> <li>proglang_select_output_column - specifies the name of the annotation column appended to the parquet. </li> <li>proglang_select_return_known - specifies whether to return supported or unsupported languages</li> </ul>"},{"location":"transforms/code/proglang_select/python/#running","title":"Running","text":""},{"location":"transforms/code/proglang_select/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the python launcher and the python launcher.</p> <pre><code>  --proglang_select_allowed_langs_file PROGLANG_MATCH_ALLOWED_LANGS_FILE\n                        Path to file containing the list of languages to be matched.\n  --proglang_select_language_column PROGLANG_MATCH_LANGUAGE_COLUMN\n                        The column name holding the name of the programming language assigned to the document\n  --proglang_select_output_column PROGLANG_MATCH_OUTPUT_COLUMN\n                        The column name to add and that contains the matching information\n  --proglang_select_s3_cred PROGLANG_MATCH_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access.\n                        access_key: access key help text\n                        secret_key: secret key help text\n                        url: optional s3 url\n                        region: optional s3 region```\n</code></pre>"},{"location":"transforms/code/proglang_select/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/proglang_select_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/proglang_select_local.py</li> <li><code>run-local-python-sample</code> - runs src/proglang_select_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/proglang_select/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/proglang_select/ray/","title":"Programming Language Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/proglang_select/ray/#summary","title":"Summary","text":"<p>This project enables the python malware transform to be run in a Ray runtime. Please see the python project for details on the transform implementation and use.</p>"},{"location":"transforms/code/proglang_select/ray/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>Transform configuration options are the same as the base python transform.</p>"},{"location":"transforms/code/proglang_select/ray/#running","title":"Running","text":""},{"location":"transforms/code/proglang_select/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of ray launcher are available.</p>"},{"location":"transforms/code/proglang_select/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/proglang_select_transform_ray.py using command line args</li> <li><code>run-local-ray-sample</code> - runs src/proglang_select_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/proglang_select/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/repo_level_ordering/","title":"Repo Level Order Transform","text":"<p>The repo level order transforms can arrange code data by repos and additionally run transformations on it like sorting, grouping by language and combining rows to a single row.  Per the set of transform project conventions the following runtimes are available:</p> <ul> <li>ray - enables the running of the base python transformation in a Ray runtime</li> </ul>"},{"location":"transforms/code/repo_level_ordering/ray/","title":"Repo Level Order Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#summary","title":"Summary","text":"<p>This transform does repository level packing of data and arranging them to prioritise semantic dependancies. This  was done to prepare long context data for Scaling Granite Code Models to 128K Context  . Quoting the paper. </p> <p>To create long-context data, we develop a new approach that packs files from the same repository together, arranging them to prioritize semantic dependencies. We identify these dependencies by analyzing file imports and create a directed acyclic graph, where each file is a node and edges represent API imports between files. After breaking any cycles in the graph, we perform a topological sort to establish an ordering of files based on their semantic dependencies. We then organize the files in a repository by placing documentation and build files first, followed by the ordered set of files with semantic dependencies, and finally the remaining non-connected files. These non-connected files are arranged according to their folder structure, using a depth-first search to traverse the repository. Finally, we determine the dominant programming language of a repository based on file extensions and presence of build files, to organise repo-ordered files by programming languages</p> <p>This transform can group the data by <code>repo_name</code> and apply additional transformations like( sorting or output_by_language or combining rows) on the  grouped data. This transform requires the input data to have at least the following columns: </p> <ul> <li> <p>repo name: Name of the repo, it is used for grouping in this transform.</p> </li> <li> <p>title : Which is usually file path.</p> </li> <li> <p>language: Programming language of content</p> </li> </ul> <p>The input data for this transform should be in parquet format. The input data is expected to have code data arranged in rows such that each row represents a file. The required columns in the input data shoud correspond to a) repository name b) file path c) content. This transform supports searching the repo across the whole dataset and writing the files of a repo as a single  parquet file. </p> <p>The transform gives the option to write the repo to file in the following ways.</p> <p>a) sort the repo content by file path and write a parquet with multiple rows</p> <p>b) sort the repo content by file path and write a parquet with a single combined row.</p> <p>c) sort the repo content in semantic ordering and write the parquet with multiple rows.</p> <p>d) sort the repo content in semantic ordering and write the parquet with a single combined row.</p> <p>Additionally this transform can grooup the repos in the folders named after the most dominant language in the repo. For more information on this transform, please refer to here.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>This transform is dependant on ray runtime. </p> <p>Transform Configuration.</p> <ul> <li>For output:    either the output is directly a file or if dominant language flag is enabled, it should output    it in folder of that langauge.</li> <li>Enable sorting:     if sorting is enabled, we should be able to choose one of the available ones (SORT_BY_PATH, SORT_SEMANTIC, SORT_SEMANTIC_NORMALISED)<ul> <li>SORT_BY_PATH: Normal ascending sort by filename</li> <li>SORT_SEMANTIC: Uses semantic analysis to sort the files within a repo.</li> <li>SORT_SEMANTIC_NORMALISED: Normalises the title to remove https:// prefix, if present, and then runs SORT_SEMANTIC</li> </ul> </li> <li>Enable superrows:    of writing superrows is enabled, then it combines rows of a repo to a single row otherwise writes normal.</li> </ul> <p>Limitation of transform. This expects a flat input folder structure with parquet files.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#running","title":"Running","text":""},{"location":"transforms/code/repo_level_ordering/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>Ray runtime options of ray launcher are available.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-local-sample</code> - runs src/repo_level_order_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-local-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#using-the-cli","title":"Using the CLI","text":"<ol> <li>Simplest Usage:</li> </ol> <p>Running on local computer setup with data available on local filesystem. Sorting the data at repo level using default algorithm. (SORT_BY_PATH). In this configuration, the workers run locally, store is local. This is a recommended method if number of available cpus is less.</p> <pre><code>export INPUT_FOLDER=\"input_data_folder/\"\nexport OUTPUT_FOLDER=\"output_data_folder/\"\n\n\nlocal_conf=\"{'input_folder' : '$INPUT_FOLDER', 'output_folder' : '$OUTPUT_FOLDER'  }\"\nrm -rf /tmp/mystore # remove if it exists\n\npython src/repo_level_order_transform_ray.py \\\n       --run_locally True \\\n       --data_local_config \"$local_conf\" \\\n       --repo_lvl_store_type  local  \\\n       --repo_lvl_store_backend_dir '/tmp/mystore' \\\n       --repo_lvl_sorting_enabled true,\n       --repo_lvl_sorting_algo SORT_SEMANTIC\n</code></pre> <p>NOTE: Make sure you use an empty folder as <code>repo_lvl_store_backend_dir</code>. </p> <ol> <li>Running on Cluster</li> </ol> <p>This configuration can be used when data is on S3 like COS storage and computation is done on a ray cluster.</p> <p>Recommended <code>repo_lvl_store_type</code> for cluster is 'ray'. We need to dedicate some ray actors specially for ray store</p> <p>for example, if we have a ray cluster with 100 actors available. We can dedicate some for the store. Let us randomly use 35 for computation and 10 for store and let remaining free. </p> <p>NOTE: The transform reads parquet files in two stages and each stage has its own actor pool for computations. The  actor pool to read parquet data in the first stage is managed by the library and the actor pool used in the second stage is managed by the ray runtime of the transform. The number of actors in each pool is same and is configured using <code>--runtime_num_workers</code>. So we have to carefully choose number of actors required based on the number resources available.  The number of workers/actors should be less than 35% of total resources available. </p> <p>We need to add the following cli args:</p> <p><code>--runtime_num_workers 35</code> <code>--repo_lvl_store_type \"ray\" --repo_lvl_store_ray_cpus 0.2 --repo_lvl_store_ray_nworkers 10</code> </p> <p>When we want the output with the following configuration enabled:</p> <p>NOTE: store_backend=<code>s3/local</code> are persistent stores and retain the mappings stores in them, they need to be cleaned/deleted after use. It is recommented to use a different location for store if data is different. They are added to aid in large data processing in multiple stages.   store_backend=<code>ray</code>, is not presistent and ephimeral and does not retain any data. It is the simplest to use if resources are available.</p> <pre><code>export INPUT_FOLDER=\"input_cos_bucket\"\nexport OUTPUT_FOLDER=\"output_cos_bucket\"\n\ns3_kreds=\"{ ... }\" # in the deafult way used for all transforms. \ns3_conf=\"{'input_folder' : '$INPUT_FOLDER', 'output_folder' : '$OUTPUT_FOLDER'  }\"\n\npython src/repo_level_order_transform_ray.py \\\n       --run_locally True \\\n       --data_s3_cred \"$s3_kreds\" \\\n       --data_s3_config \"$s3_conf\" \\\n       --repo_lvl_store_type  ray  \\\n       --repo_lvl_combine_rows True\\\n       --repo_lvl_sorting_enabled True\\\n       --repo_lvl_store_ray_cpus 0.2 \\\n       --repo_lvl_store_ray_nworkers 1 \\\n       --repo_lvl_sorting_algo SORT_SEMANTIC \\\n       --repo_lvl_output_by_langs True   \n</code></pre>"},{"location":"transforms/code/repo_level_ordering/ray/src/dpk_repo_level_order/internal/sorting/","title":"Index","text":""},{"location":"transforms/code/repo_level_ordering/ray/src/dpk_repo_level_order/internal/sorting/#sorting-module","title":"Sorting Module","text":"<p>This module supports sorting the pandas dataframes by files. It can sort files in lexical  or semantic order.</p> <p>It uses emerge-viz for source code analysis and networkx for building network.</p> <p>Used like:</p> <pre><code>  from dpk_repo_level_order.internal.sorting.semantic_ordering import (\n    sort_by_path,\n    sort_sem,\n    check_and_update_title,\n)\n</code></pre> <p>Types of sorting:</p> <p>They require <code>title</code> field. </p> <p>There is a <code>check_and_update_title</code> utility function to normalise title for usage with these sorting functions. </p> <ol> <li>Lexical sorting of paths: <code>sort_by_path</code></li> </ol> <pre><code> df: pd.DataFrame\n sorted_df=sort_by_path(df=df, \n              logger=logger, \n              title_column_name=title_column_name)\n</code></pre> <ol> <li>Semantic sorting: <code>sort_sem</code> </li> </ol> <pre><code> df: pd.DataFrame\n\n check_and_update_title(df)\n sorted_df = sort_sem(df=df, \n           logger=logger, \n           title_column_name=title_column_name)\n</code></pre>"},{"location":"transforms/language/doc_chunk/","title":"Chunk documents Transform","text":"<p>This transform is chunking documents. It supports multiple chunker modules. More details as well as a description of the parameters can be found in the python/README.md.</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/language/doc_chunk/kfp_ray/","title":"chunk documents Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/doc_chunk/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the doc_chunk Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/doc_chunk/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/doc_chunk/python/","title":"Chunk documents Transform","text":"<p>This transform is chunking documents. It supports multiple chunker modules (see the <code>chunking_type</code> parameter).</p> <p>When using documents converted to JSON, the transform leverages the Docling Core <code>HierarchicalChunker</code> to chunk according to the document layout segmentation, i.e. respecting the original document components as paragraphs, tables, enumerations, etc. It relies on documents converted with the Docling library in the pdf2parquet transform using the option <code>contents_type: \"application/json\"</code>, which provides the required JSON structure.</p> <p>When using documents converted to Markdown, the transform leverages the Llama Index <code>MarkdownNodeParser</code>, which is relying on its internal Markdown splitting logic.</p>"},{"location":"transforms/language/doc_chunk/python/#output-format","title":"Output format","text":"<p>The output parquet file will contain all the original columns, but the content will be replaced with the individual chunks.</p>"},{"location":"transforms/language/doc_chunk/python/#tracing-the-origin-of-the-chunks","title":"Tracing the origin of the chunks","text":"<p>The transform allows to trace the origin of the chunk with the <code>source_doc_id</code> which is set to the value of the <code>document_id</code> column (if present) in the input table. The actual name of columns can be customized with the parameters described below.</p>"},{"location":"transforms/language/doc_chunk/python/#running","title":"Running","text":""},{"location":"transforms/language/doc_chunk/python/#parameters","title":"Parameters","text":"<p>The transform can be tuned with the following parameters.</p> Parameter Default Description <code>chunking_type</code> <code>dl_json</code> Chunking type to apply. Valid options are <code>li_markdown</code> for using the LlamaIndex Markdown chunking, <code>dl_json</code> for using the Docling JSON chunking, <code>li_token_text</code> for using the LlamaIndex Token Text Splitter, which chunks the text into fixed-sized windows of tokens. <code>content_column_name</code> <code>contents</code> Name of the column containing the text to be chunked. <code>doc_id_column_name</code> <code>document_id</code> Name of the column containing the doc_id to be propagated in the output. <code>dl_min_chunk_len</code> <code>None</code> Minimum number of characters for the chunk in the dl_json chunker. Setting to None is using the library defaults, i.e. a <code>min_chunk_len=64</code>. <code>chunk_size_tokens</code> <code>128</code> Size of the chunk in tokens for the token text chunker. <code>chunk_overlap_tokens</code> <code>30</code> Number of tokens overlapping between chunks for the token text chunker. <code>output_chunk_column_name</code> <code>contents</code> Column name to store the chunks in the output table. <code>output_source_doc_id_column_name</code> <code>source_document_id</code> Column name to store the <code>doc_id</code> from the input table. <code>output_jsonpath_column_name</code> <code>doc_jsonpath</code> Column name to store the document path of the chunk in the output table. <code>output_pageno_column_name</code> <code>page_number</code> Column name to store the page number of the chunk in the output table. <code>output_bbox_column_name</code> <code>bbox</code> Column name to store the bbox of the chunk in the output table. <p>When invoking the CLI, the parameters must be set as <code>--doc_chunk_&lt;name&gt;</code>, e.g. <code>--doc_chunk_column_name_key=myoutput</code>.</p>"},{"location":"transforms/language/doc_chunk/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/doc_chunk_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/doc_chunk_local.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/doc_chunk/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/doc_chunk/ray/","title":"Chunk documents Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/doc_chunk/ray/#summary","title":"Summary","text":"<p>This project wraps the doc_chunk transform with a Ray runtime.</p>"},{"location":"transforms/language/doc_chunk/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>chunk documents configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/doc_chunk/ray/#running","title":"Running","text":""},{"location":"transforms/language/doc_chunk/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/language/doc_chunk/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/doc_chunk_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/doc_chunk_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/doc_chunk_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/doc_chunk/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/doc_quality/","title":"Document Quality Transform","text":"<p>The Document Quality transforms serves as a transform to check see the quality of document. Per the set of transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image for document quality in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/language/doc_quality/python/","title":"Document Quality Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/doc_quality/python/#summary","title":"Summary","text":"<p>This transform will calculate and annotate several metrics related to document, which are usuful to see the quality of document. </p> <p>In this transform, following metrics will be included:</p> output column name data type description supported language docq_total_words int the total number of words ALL docq_mean_word_len int the mean of words' lengths ALL docq_symbol_to_word_ratio float the ratio of symbol-to-word ratio (Reference for symbols like emojis: https://textacy.readthedocs.io/en/0.11.0/api_reference/preprocessing.html, currently used symbol: <code>#</code>, <code>...</code>) ALL docq_sentence_count int the number of sentences ALL docq_curly_bracket_ratio float the ratio between the number of occurrences of <code>{</code> or <code>}</code> over the text length ALL docq_lorem_ipsum_ratio float the ratio between the number of occurrences of <code>lorem ipsum</code> over the text length. Lorem ipsum, or lipsum as it is sometimes known, is dummy text used in laying out print, graphic or web designs. ALL docq_contain_bad_word bool whether text containst bad words ALL docq_bullet_point_ratio float the ratio of lines starting with a bullet point ALL docq_ellipsis_line_ratio float the ratio of lines ending with an ellipsis ALL docq_alphabet_word_ratio float the ratio of words having at least one alphabetic character ALL docq_contain_common_en_words bool whether the given <code>text</code> contains common English words like <code>the</code>, <code>and</code>, <code>to</code>, <code>that</code>, <code>of</code>, <code>with</code>, <code>be</code>, and <code>have</code> ALL docq_avg_ja_sentence_len int average sentence length for an input text, inspired by an OSS HojiChar. ja docq_first_ja_alphabet_pos int first position of occurrence of Japanese alphabets (i.e., Hiragana or Katakana) ja <p>You can see more detailed backgrounds of some columns in Deepmind's Gopher paper</p>"},{"location":"transforms/language/doc_quality/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding DocQualityTransform  configuration for values are as follows:</p> <ul> <li>text_lang - specifies language used in the text content. By default, \"en\" is used.</li> <li>doc_content_column - specifies column name that contains document text. By default, \"contents\" is used.</li> <li>bad_word_filepath - specifies a path to bad word file: local folder (file or directory) that points to bad word file. You don't have to set this parameter if you don't need to set bad words.</li> </ul>"},{"location":"transforms/language/doc_quality/python/#running","title":"Running","text":""},{"location":"transforms/language/doc_quality/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by  the python launcher. <pre><code>  --docq_text_lang DOCQ_TEXT_LANG   language used in the text content. By default, \"en\" is used.\n  --docq_doc_content_column DOCQ_DOC_CONTENT_COLUMN   column name that contain document text. By default, \"contents\" is used.\n  --docq_bad_word_filepath DOCQ_BAD_WORD_FILEPATH   path to bad word file: local folder (file or directory) that points to bad word file. You don't have to set this parameter if you don't need to set bad words.\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/language/doc_quality/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/doc_quality_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/doc_quality_local.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/doc_quality/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/doc_quality/python/#troubleshooting-guide","title":"Troubleshooting guide","text":"<p>For M1 Mac user, if you see following error during make command, <code>error: command '/usr/bin/clang' failed with exit code 1</code>, you may better follow this step</p>"},{"location":"transforms/language/doc_quality/ray/","title":"Document Quality Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/doc_quality/ray/#summary","title":"Summary","text":"<p>This project wraps the Document Quality transform with a Ray runtime.</p>"},{"location":"transforms/language/doc_quality/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Document Quality configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/doc_quality/ray/#running","title":"Running","text":""},{"location":"transforms/language/doc_quality/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/language/doc_quality/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/doc_quality_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/doc_quality_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/doc_quality_s3_ray.py<ul> <li>Requires prior invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/doc_quality/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/doc_quality/ray/#troubleshooting-guide","title":"Troubleshooting guide","text":"<p>For M1 Mac user, if you see following error during make command, <code>error: command '/usr/bin/clang' failed with exit code 1</code>, you may better follow this step</p>"},{"location":"transforms/language/html2parquet/python/","title":"html2parquet Transform","text":"<p>This tranforms iterate through zip of HTML files or single HTML files and generates parquet files containing the converted document in string.</p> <p>The HTML conversion is using the Trafilatura.</p>"},{"location":"transforms/language/html2parquet/python/#output-format","title":"Output format","text":"<p>The output format will contain the following colums</p> <pre><code>{\n    \"title\": \"string\",             // the member filename\n    \"document\": \"string\",          // the base of the source archive\n    \"contents\": \"string\",          // the content of the HTML\n    \"document_id\": \"string\",      // the document id, a hash of `contents`\n    \"size\": \"string\",             // the size of `contents`\n    \"date_acquired\": \"date\",      // the date when the transform was executing\n}\n</code></pre>"},{"location":"transforms/language/html2parquet/python/#parameters","title":"Parameters","text":"<p>The table below provides the parameters that users can adjust to control the behavior of the extraction:</p> Parameter Default Description <code>output_format</code> <code>markdown</code> Specifies the format of the extracted content. Options: <code>markdown</code>, <code>txt</code>. <code>favor_precision</code> <code>True</code> Prefers less content but more accurate extraction. Options: <code>True</code>, <code>False</code>. <code>favor_recall</code> <code>True</code> Extracts more content when uncertain. Options: <code>True</code>, <code>False</code>. <p>The table below provides the parameters that are enabled by default to ensure a comprehensive extraction process:</p> Parameter Default Description <code>include_tables</code> <code>True</code> Extracts content from HTML <code>&lt;table&gt;</code> elements. <code>include_images</code> <code>True</code> Extracts image references (experimental feature). <code>include_links</code> <code>True</code> Extracts hyperlinks from the HTML content. <code>include_formatting</code> <code>True</code> Preserves basic HTML formatting (e.g., bold, italic) in the extracted content. <p>Note: If both <code>favor_precision</code> and <code>favor_recall</code> are set to <code>True</code>, <code>favor_recall</code> takes precedence.</p> <ul> <li>To set the output format to plain text, use <code>output_format='txt'</code>.</li> <li>To prioritize extracting more content over accuracy, set <code>favor_recall=True</code> and <code>favor_precision=False</code>.</li> <li>When invoking the CLI, use the following syntax for these parameters: <code>--html2parquet_&lt;parameter_name&gt;</code>. For example: <code>--html2parquet_output_format='markdown'</code>.</li> </ul>"},{"location":"transforms/language/html2parquet/python/#example","title":"Example","text":""},{"location":"transforms/language/html2parquet/python/#sample-html","title":"Sample HTML","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Sample HTML File&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to My Sample HTML Page&lt;/h1&gt;\n    &lt;h2&gt;Overview&lt;/h2&gt;\n    &lt;p&gt;This page contains various HTML components to demonstrate structure and formatting.&lt;/p&gt;\n    &lt;p&gt;This paragraph contains &lt;a href=\"https://example.com\"&gt;a link to Example.com&lt;/a&gt;.&lt;/p&gt;\n\n    &lt;h2&gt;Sample Image&lt;/h2&gt;\n    &lt;img src=\"https://via.placeholder.com/300\" alt=\"Placeholder Image\" /&gt;\n\n    &lt;h2&gt;Key Features&lt;/h2&gt;\n    &lt;ul&gt;\n        &lt;li&gt;Easy to use&lt;/li&gt;\n        &lt;li&gt;Highly customizable&lt;/li&gt;\n        &lt;li&gt;Supports multiple components&lt;/li&gt;\n    &lt;/ul&gt;\n\n    &lt;h2&gt;Sample Data Table&lt;/h2&gt;\n    &lt;table border=\"1\"&gt;\n        &lt;tr&gt;\n            &lt;th&gt;Name&lt;/th&gt;\n            &lt;th&gt;Age&lt;/th&gt;\n            &lt;th&gt;City&lt;/th&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Alice&lt;/td&gt;\n            &lt;td&gt;30&lt;/td&gt;\n            &lt;td&gt;New York&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Bob&lt;/td&gt;\n            &lt;td&gt;25&lt;/td&gt;\n            &lt;td&gt;Los Angeles&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Charlie&lt;/td&gt;\n            &lt;td&gt;35&lt;/td&gt;\n            &lt;td&gt;Chicago&lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/table&gt;\n\n    &lt;h2&gt;Contact Us&lt;/h2&gt;\n    &lt;form action=\"/submit\" method=\"POST\"&gt;\n        &lt;label for=\"name\"&gt;Name:&lt;/label&gt;\n        &lt;input type=\"text\" id=\"name\" name=\"name\" required&gt;&lt;br&gt;&lt;br&gt;\n        &lt;label for=\"email\"&gt;Email:&lt;/label&gt;\n        &lt;input type=\"email\" id=\"email\" name=\"email\" required&gt;&lt;br&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit\"&gt;\n    &lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"transforms/language/html2parquet/python/#sample-output-using-default-parameters","title":"Sample Output (Using Default Parameters)","text":"<pre><code># Welcome to My Sample HTML Page\n\n## Overview\n\nThis page contains various HTML components to demonstrate structure and formatting.\n\nThis paragraph contains [a link to Example.com](https://example.com).\n\n## Sample Image\n\n\n## Key Features\n\n- Easy to use\n- Highly customizable\n- Supports multiple components\n\n## Getting Started\n\n- Download the HTML file\n- Open it in your browser\n- Explore the content\n\n## Sample Data Table\n\nName |\nAge |\nCity |\n\nAlice |\n30 |\nNew York |\n\nBob |\n25 |\nLos Angeles |\n\nCharlie |\n35 |\nChicago |\n\n\n## Contact Us\n</code></pre>"},{"location":"transforms/language/html2parquet/ray/","title":"html2parquet Ray Transform","text":"<p>This module implements the ray version of the html2parquet transform.</p> <p>The HTML conversion is using the Trafilatura.</p>"},{"location":"transforms/language/html2parquet/ray/#prometheus-metrics","title":"Prometheus metrics","text":"<p>The transform will produce the following statsd metrics:</p> metric name Description worker_html_doc_count Number of HTML documents converted by the worker worker_html_pages_count Number of HTML pages converted by the worker worker_html_page_avg_convert_time Average time for converting a single HTML page on each worker worker_html_convert_time Time spent converting a single document"},{"location":"transforms/language/lang_id/","title":"Language Identification Transform","text":"<p>The Language Identification transforms serves as a simple exemplar to demonstrate the development of a simple 1:1 transform.  Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/language/lang_id/kfp_ray/","title":"language Identification Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/lang_id/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/lang_id/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/lang_id/python/","title":"Language Identification Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/lang_id/python/#summary","title":"Summary","text":"<p>This transform will identify language of each text with confidence score with fasttext language identification model. ref</p>"},{"location":"transforms/language/lang_id/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding LangIdentificationTransform  configuration for values are as follows:</p> Key name Default Description model_credential unset specifies the credential you use to get model. This will be huggingface token. Guide to get huggingface token model_kind unset specifies what kind of model you want to use for language identification. Currently, only <code>fasttext</code> is available. model_url unset specifies url that model locates. For fasttext, this will be repo nme of the model, like <code>facebook/fasttext-language-identification</code> content_column_name <code>contents</code> specifies name of the column containing documents output_lang_column_name <code>lang</code> specifies name of the output column to hold predicted language code output_score_column_name <code>score</code> specifies name of the output column to hold score of prediction"},{"location":"transforms/language/lang_id/python/#running","title":"Running","text":""},{"location":"transforms/language/lang_id/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher. <pre><code>  --lang_id_model_credential LANG_ID_MODEL_CREDENTIAL   the credential you use to get model. This will be huggingface token.\n  --lang_id_model_kind LANG_ID_MODEL_KIND   what kind of model you want to use for language identification. Currently, only `fasttext` is available.\n  --lang_id_model_url LANG_ID_MODEL_URL   url that model locates. For fasttext, this will be repo name of the model, like `facebook/fasttext-language-identification`\n  --lang_id_content_column_name LANG_ID_CONTENT_COLUMN_NAME   A name of the column containing documents\n  --lang_id_output_lang_column_name LANG_ID_OUTPUT_LANG_COLUMN_NAME   Column name to store identified language\n  --lang_id_output_score_column_name LANG_ID_OUTPUT_SCORE_COLUMN_NAME   Column name to store the score of language identification\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/language/lang_id/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/lang_id_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/lang_id_local.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/lang_id/python/#troubleshooting-guide","title":"Troubleshooting guide","text":"<p>For M1 Mac user, if you see following error during make command, <code>error: command '/usr/bin/clang' failed with exit code 1</code>, you may better follow this step</p>"},{"location":"transforms/language/lang_id/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/lang_id/ray/","title":"Language Identification Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/lang_id/ray/#summary","title":"Summary","text":"<p>This project wraps the language identification transform with a Ray runtime.</p>"},{"location":"transforms/language/lang_id/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Language Identification configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/lang_id/ray/#running","title":"Running","text":""},{"location":"transforms/language/lang_id/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/language/lang_id/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/lang_id_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/lang_id_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/lang_id_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/lang_id/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/pdf2parquet/","title":"PDF2PARQUET Transform","text":"<p>The PDF2PARQUET transforms iterate through PDF files or zip of PDF files and generates parquet files containing the converted document in Markdown format.</p> <p>The PDF conversion is using the Docling package.</p> <p>The following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/language/pdf2parquet/kfp_ray/","title":"PDF2PARQUET Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/pdf2parquet/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the pdf2parquet Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/pdf2parquet/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/pdf2parquet/python/","title":"Ingest PDF to Parquet","text":"<p>This tranforms iterate through PDF files or zip of PDF files and generates parquet files containing the converted document in Markdown format.</p> <p>The PDF conversion is using the Docling package.</p>"},{"location":"transforms/language/pdf2parquet/python/#output-format","title":"Output format","text":"<p>The output format will contain all the columns of the metadata CSV file, with the addition of the following columns</p> <pre><code>{\n    \"source_filename\": \"string\",  // the basename of the source archive or file\n    \"filename\": \"string\",         // the basename of the PDF file\n    \"contents\": \"string\",         // the content of the PDF\n    \"document_id\": \"string\",      // the document id, a random uuid4 \n    \"ext\": \"string\",              // the detected file extension\n    \"hash\": \"string\",             // the hash of the `contents` column\n    \"size\": \"string\",             // the size of `contents`\n    \"date_acquired\": \"date\",      // the date when the transform was executing\n    \"num_pages\": \"number\",        // number of pages in the PDF\n    \"num_tables\": \"number\",       // number of tables in the PDF\n    \"num_doc_elements\": \"number\", // number of document elements in the PDF\n    \"pdf_convert_time\": \"float\",  // time taken to convert the document in seconds\n}\n</code></pre>"},{"location":"transforms/language/pdf2parquet/python/#parameters","title":"Parameters","text":"<p>The transform can be initialized with the following parameters.</p> Parameter Default Description <code>artifacts_path</code> Path where to Docling models artifacts are located, if unset they will be downloaded and fetched from the HF_HUB_CACHE folder. <code>contents_type</code> <code>text/markdown</code> The output type for the <code>contents</code> column. Valid types are <code>text/markdown</code> and <code>application/json</code>. <code>do_table_structure</code> <code>True</code> If true, detected tables will be processed with the table structure model. <code>do_ocr</code> <code>True</code> If true, optical character recognition (OCR) will be used to read the content of bitmap parts of the document. <code>double_precision</code> <code>8</code> If set, all floating points (e.g. bounding boxes) are rounded to this precision. For tests it is advised to use 0. <p>When invoking the CLI, the parameters must be set as <code>--pdf2parquet_&lt;name&gt;</code>, e.g. <code>--pdf2parquet_do_ocr=true</code>.</p>"},{"location":"transforms/language/pdf2parquet/python/#credits","title":"Credits","text":"<p>The PDF document conversion is developed by the AI for Knowledge group in IBM Research Zurich. The main package is Docling.</p>"},{"location":"transforms/language/pdf2parquet/ray/","title":"PDF2PARQUET Ray Transform","text":"<p>This module implements the ray version of the pdf2parquet transform.</p>"},{"location":"transforms/language/pdf2parquet/ray/#prometheus-metrics","title":"Prometheus metrics","text":"<p>The transform will produce the following statsd metrics:</p> metric name Description worker_pdf_doc_count Number of PDF documents converted by the worker worker_pdf_pages_count Number of PDF pages converted by the worker worker_pdf_page_avg_convert_time Average time for converting a single PDF page on each worker worker_pdf_convert_time Time spent converting a single document"},{"location":"transforms/language/pii_redactor/","title":"PII Redactor Transform","text":"<ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/language/pii_redactor/kfp_ray/","title":"PII Redactor Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/pii_redactor/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the pii redactor ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/pii_redactor/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/pii_redactor/python/","title":"PII Redactor Transform","text":"<p>This transform redacts Personally Identifiable Information (PII) from the input data.</p> <p>The transform leverages the Microsoft Presidio SDK for PII detection and uses the Flair recognizer for entity recognition.</p>"},{"location":"transforms/language/pii_redactor/python/#supported-entities","title":"Supported Entities","text":"<p>The transform detects the following PII entities by default: - PERSON: Names of individuals - EMAIL_ADDRESS: Email addresses - ORGANIZATION: Names of organizations - DATE_TIME: Dates and times - PHONE_NUMBER: Phone number - CREDIT_CARD: Credit card numbers</p> <p>You can configure the entities to detect by passing the required entities as argument param ( --pii_redactor_entities ). To know more about different entity types supported - Entities</p>"},{"location":"transforms/language/pii_redactor/python/#redaction-techniques","title":"Redaction Techniques","text":"<p>Two redaction techniques are supported: - replace: Replaces detected PII with a placeholder (default) - redact: Removes the detected PII from the text</p> <p>You can choose the redaction technique by passing it as an argument parameter (--pii_redactor_operator).</p>"},{"location":"transforms/language/pii_redactor/python/#input-and-output","title":"Input and Output","text":""},{"location":"transforms/language/pii_redactor/python/#input","title":"Input","text":"<p>The input data should be a <code>py.Table</code> with a column containing the text where PII detection and redaction will be applied. By default, this column is named <code>contents</code>.</p> <p>Example Input Table Structure: Table 1: Sample input to the pii redactor transform</p> contents doc_id My name is John Doe doc001 I work at apple doc002"},{"location":"transforms/language/pii_redactor/python/#output","title":"Output","text":"<p>The output table will include the original columns plus an additional column <code>new_contents</code> which is configurable with redacted text and <code>detected_pii</code>  column consisting the type of PII entities detected in that document for replace operator.</p> <p>Example Output Table Structure for replace operator:</p> contents doc_id new_contents detected_pii My name is John Doe doc001 My name is <code>&lt;PERSON&gt;</code> <code>[PERSON]</code> I work at apple doc002 I work at <code>&lt;ORGANIZATION&gt;</code> <code>[ORGANIZATION]</code> <p>When <code>redact</code> operator is chosen the output will look like below</p> <p>Example Output Table Structure for redact operator</p> contents doc_id new_contents detected_pii My name is John Doe doc001 My name is <code>[PERSON]</code> I work at apple doc002 I work at <code>[ORGANIZATION]</code>"},{"location":"transforms/language/pii_redactor/python/#running","title":"Running","text":""},{"location":"transforms/language/pii_redactor/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher.</p> <pre><code>  --pii_redactor_entities PII_ENTITIES\n                        list of PII entities to be captured for example: [\"PERSON\", \"EMAIL\"]\n  --pii_redactor_operator REDACTOR_OPERATOR\n                        Two redaction techniques are supported - replace(default), redact \n  --pii_redactor_transformed_contents PII_TRANSFORMED_CONTENT_COLUMN_NAME\n                        Mention the column name in which transformed contents will be added. This is required argument. \n  --pii_redactor_score_threshold SCORE_THRESHOLD\n                        The score_threshold is a parameter that sets the minimum confidence score required for an entity to be considered a match.\n                        Provide a value above 0.6\n</code></pre>"},{"location":"transforms/language/pii_redactor/ray/","title":"PII Redactor Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/pii_redactor/ray/#summary","title":"Summary","text":"<p>This project wraps the pii redactor transform with a Ray runtime.</p>"},{"location":"transforms/language/pii_redactor/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>PII redactor configuration and command line options are the same as for the base python transform and in additional it also supports other options refer pii redactor transform . </p>"},{"location":"transforms/language/pii_redactor/ray/#running","title":"Running","text":""},{"location":"transforms/language/pii_redactor/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/language/pii_redactor/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/pii_redactor_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/pii_redactor_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/pii_redactor_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/pii_redactor/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/text_encoder/","title":"Text encoder Transform","text":"<p>This transform is using sentence encoder models to create embeddings of the text in each row of the input .parquet table.</p> <p>Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/language/text_encoder/kfp_ray/","title":"TextEncoder Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/text_encoder/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the text_encoder Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/text_encoder/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/text_encoder/python/","title":"Text Encoder Transform","text":""},{"location":"transforms/language/text_encoder/python/#summary","title":"Summary","text":"<p>This transform is using sentence encoder models to create embedding vectors of the text in each row of the input .parquet table.</p> <p>The embeddings vectors generated by the transform are useful for tasks like sentence similarity, features extraction, etc which are also at the core of retrieval-augmented generation (RAG) applications.</p>"},{"location":"transforms/language/text_encoder/python/#running","title":"Running","text":""},{"location":"transforms/language/text_encoder/python/#parameters","title":"Parameters","text":"<p>The transform can be tuned with the following parameters.</p> Parameter Default Description <code>model_name</code> <code>BAAI/bge-small-en-v1.5</code> The HF model to use for encoding the text. <code>content_column_name</code> <code>contents</code> Name of the column containing the text to be encoded. <code>output_embeddings_column_name</code> <code>embeddings</code> Column name to store the embeddings in the output table. <code>output_path_column_name</code> <code>doc_path</code> Column name to store the document path of the chunk in the output table. <p>When invoking the CLI, the parameters must be set as <code>--text_encoder_&lt;name&gt;</code>, e.g. <code>--text_encoder_column_name_key=myoutput</code>.</p>"},{"location":"transforms/language/text_encoder/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/text_encoder_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/text_encoder_local.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/text_encoder/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/text_encoder/ray/","title":"TextEncoder Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/text_encoder/ray/#summary","title":"Summary","text":"<p>This project wraps the text_encoder transform with a Ray runtime.</p>"},{"location":"transforms/language/text_encoder/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Text Encoder configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/text_encoder/ray/#running","title":"Running","text":""},{"location":"transforms/language/text_encoder/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/language/text_encoder/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/text_encoder_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/text_encoder_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/text_encoder_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/text_encoder/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/doc_id/","title":"Doc ID Transform","text":"<p>The Document ID transforms adds a document identification (unique integers and content hashes), which later can be  used in de-duplication operations, per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>pythom - enables the running of the base python transformation   in a Python runtime</li> <li>ray - enables the running of the base python transformation   in a Ray runtime</li> <li>spark - enables the running of a spark-based transformation in a Spark runtime. </li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/doc_id/#summary","title":"Summary","text":"<p>This transform annotates documents with document \"ids\". It supports the following transformations of the original data: * Adding document hash: this enables the addition of a document hash-based id to the data.   The hash is calculated with <code>hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()</code>.   To enable this annotation, set <code>hash_column</code> to the name of the column,   where you want to store it. * Adding integer document id: this allows the addition of an integer document id to the data that   is unique across all rows in all tables provided to the <code>transform()</code> method.   To enable this annotation, set <code>int_id_column</code> to the name of the column, where you want   to store it.</p> <p>Document IDs are generally useful for tracking annotations to specific documents. Additionally fuzzy deduping relies on integer IDs to be present. If your dataset does not have document ID column(s), you can use this transform to create ones.</p>"},{"location":"transforms/universal/doc_id/kfp_ray/","title":"Document Identification Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/doc_id/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/doc_id/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/doc_id/python/","title":"Document ID Python Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/doc_id/python/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/doc_id/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys defined in DocIDTransform configuration for values are as follows:</p> <ul> <li>doc_column - specifies name of the column containing the document (required for ID generation)</li> <li>hash_column - specifies name of the column created to hold the string document id, if None, id is not generated</li> <li>int_id_column - specifies name of the column created to hold the integer document id, if None, id is not generated</li> <li>start_id - an id from which ID generator starts () </li> </ul> <p>At least one of hash_column or int_id_column must be specified.</p>"},{"location":"transforms/universal/doc_id/python/#running","title":"Running","text":""},{"location":"transforms/universal/doc_id/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by the ray launcher. <pre><code>  --doc_id_doc_column DOC_ID_DOC_COLUMN\n                        doc column name\n  --doc_id_hash_column DOC_ID_HASH_COLUMN\n                        Compute document hash and place in the given named column\n  --doc_id_int_column DOC_ID_INT_COLUMN\n                        Compute unique integer id and place in the given named column\n  --doc_id_start_id DOC_ID_START_ID\n                        starting integer id\n</code></pre> These correspond to the configuration keys described above.</p> <p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/doc_id/ray/","title":"Document ID Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/doc_id/ray/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/doc_id/ray/#driver-options","title":"Driver options","text":""},{"location":"transforms/universal/doc_id/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>See Python documentation</p>"},{"location":"transforms/universal/doc_id/ray/#running","title":"Running","text":""},{"location":"transforms/universal/doc_id/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by the ray launcher.</p> <p>To use the transform image to transform your data, please refer to the running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/doc_id/spark/","title":"Document ID Generation","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/doc_id/spark/#summary","title":"Summary","text":"<p>This transform assigns a unique integer ID to each row in a Spark DataFrame. It relies on the monotonically_increasing_id pyspark function to generate the unique integer IDs. As described in the documentation of this function:</p> <p>The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. </p>"},{"location":"transforms/universal/doc_id/spark/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding DocIdTransform  configuration for values are as follows:</p> <ul> <li>doc_id_column_name - specifies the name of the DataFrame column that holds the generated document IDs.</li> </ul>"},{"location":"transforms/universal/doc_id/spark/#running","title":"Running","text":"<p>You can run the doc_id_local.py (spark-based implementation) to transform the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p>"},{"location":"transforms/universal/doc_id/spark/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e. SparkTransformLauncher), the following command line arguments are available in addition to  the options provided by the python launcher.</p> <pre><code>  --doc_id_column_name DOC_ID_COLUMN_NAME\n                        name of the column that holds the generated document ids\n</code></pre>"},{"location":"transforms/universal/doc_id/spark/#running-as-spark-based-application","title":"Running as spark-based application","text":"<pre><code>(venv) cma:src$ python doc_id_local.py\n18:32:13 INFO - data factory data_ is using local data access: input_folder - /home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/test-data/input output_folder - /home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/output at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:185\"\n18:32:13 INFO - data factory data_ max_files -1, n_sample -1 at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:201\"\n18:32:13 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:214\"\n18:32:13 INFO - pipeline id pipeline_id at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:80\"\n18:32:13 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'} at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:83\"\n18:32:13 INFO - spark execution config : {'spark_local_config_filepath': '/home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/config/spark_profile_local.yml', 'spark_kube_config_filepath': 'config/spark_profile_kube.yml'} at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_execution_config.py:42\"\n24/05/26 18:32:14 WARN Utils: Your hostname, li-7aed0a4c-2d51-11b2-a85c-dfad31db696b.ibm.com resolves to a loopback address: 127.0.0.1; using 192.168.1.223 instead (on interface wlp0s20f3)\n24/05/26 18:32:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/26 18:32:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18:32:17 INFO - files = ['/home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/test-data/input/test_doc_id_1.parquet', '/home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/test-data/input/test_doc_id_2.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_launcher.py:184\"\n24/05/26 18:32:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n</code></pre>"},{"location":"transforms/universal/doc_id/spark/#doc-id-statistics","title":"Doc ID Statistics","text":"<p>The metadata generated by the Spark <code>doc_id</code> transform contains the following statistics:   * <code>total_docs_count</code>, <code>total_columns_count</code>: total number of documents (rows), and columns in the input table, before the <code>doc_id</code> transform ran     * <code>docs_after_doc_id</code>, <code>columns_after_doc_id</code>: total number of documents (rows), and columns in the output table, after the <code>doc_id</code> transform ran  </p>"},{"location":"transforms/universal/doc_id/spark/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/ededup/","title":"Exect Deduplification Transform","text":""},{"location":"transforms/universal/ededup/#summary","title":"Summary","text":"<p>Exact data deduplication is used to identify (and remove) records determined by native documents. * It\u2019s O(N2) complexity * shuffling with lots of data movement</p> <p>It can be implemented using 2 approaches: * Exact string matching * Hash-based matching (ASSUMPTION: a hash is unique to each native document.) \u2013 moving hash value is cheaper than moving full content</p> <p>Implementation here is using \u201cstreaming\u201d deduplication, based on central hash:</p> <p></p> <ul> <li>At the heart of the implementation is a hash cache implemented as a set of Ray actors and containing   unique hashes seen so far.</li> <li>Individual data processors are responsible for:</li> <li>Reading data from data plane</li> <li>Converting documents into hashes</li> <li>Coordinating with distributed hashes cache to remove the duplicates</li> <li>Storing unique documents back to the data plane</li> </ul> <p>The complication of mapping this model to transform model is the fact that implementation requires a hash cache, that transform mode knows nothing about. The solution here is to use transform runtime to create haches cache. and pass it as a parameter to transforms.</p>"},{"location":"transforms/universal/ededup/#transform-runtime","title":"Transform runtime","text":"<p>Transform runtime is responsible for creation of the hashes cache. Additionally it  enhances statistics information with the information about hashes cache size and utilization</p>"},{"location":"transforms/universal/ededup/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding EdedupTransform configuration for values (common for Python and Ray) are as follows:</p> <ul> <li>doc_column - specifies name of the column containing documents</li> <li>doc_id_column - specifies the name of the column containing a document id</li> <li>use_snapshot - specifies that ededup execution starts from a set of already seen hashes. This can be used   for the incremental ededup execution</li> <li>snapshot_directory - specifies a directory from which snapshots are read. If this is not specified, a default   location (output_folder/snapshot is used)</li> </ul>"},{"location":"transforms/universal/ededup/#snapshotting","title":"Snapshotting","text":"<p>In the current implementation we also provide snapshotting. At the end of execution, the content of the hash cache to storage (local disk or S3). The reason this is done is to enable incremental execution of dedup. You can run dedup on a set of existing files and snapshot the hash cache. Now when additional files come in, instead of running dedup on all the files, you can load snapshot from the previous run and run dedup only on new files</p>"},{"location":"transforms/universal/ededup/#available-runtimes","title":"Available runtimes","text":"<p>As per transform project conventions the following runtimes are available:</p> <ul> <li>python - enables running of the base python transformation   in a Python runtime</li> <li>ray - enables running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/ededup/kfp_ray/","title":"Exact Deduplication Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/ededup/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/ededup/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/ededup/python/","title":"Ededup Python Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p> <p>Also see here on details of implementation</p>"},{"location":"transforms/universal/ededup/python/#summary","title":"Summary","text":"<p>This is a python version of ededup</p>"},{"location":"transforms/universal/ededup/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>See common ededup parameters</p>"},{"location":"transforms/universal/ededup/python/#running","title":"Running","text":""},{"location":"transforms/universal/ededup/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher. <pre><code>  --ededup_doc_column EDEDUP_DOC_COLUMN\n                        name of the column containing document\n  --ededup_doc_id_column EDEDUP_DOC_ID_COLUMN\n                        name of the column containing document id\n  --ededup_use_snapshot EDEDUP_USE_SNAPSHOT\n                        flag to continue from snapshot\n  --ededup_snapshot_directory EDEDUP_SNAPSHOT_DIRECTORY\n                        location of snapshot files  \n</code></pre> These correspond to the configuration keys described above.</p> <p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/ededup/ray/","title":"Exact Dedup","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/ededup/ray/#additional-parameters","title":"Additional parameters","text":"<p>In addition to common ededup parameters Ray implementation provides two additional ones</p> <ul> <li>hash_cpu - specifies amount of CPU per hash actor</li> <li>num_hashes - specifies number of hash actors</li> </ul>"},{"location":"transforms/universal/ededup/ray/#additional-support","title":"\u00e5dditional support","text":"<p>We also provide an estimate to roughly determine cluster size for running transformer.</p>"},{"location":"transforms/universal/ededup/ray/#running","title":"Running","text":""},{"location":"transforms/universal/ededup/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <p><code>--ededup_hash_cpu EDEDUP_HASH_CPU                         number of CPUs per hash   --ededup_num_hashes EDEDUP_NUM_HASHES                         number of hash actors to use   --ededup_doc_column EDEDUP_DOC_COLUMN                         name of the column containing document   --ededup_doc_id_column EDEDUP_DOC_ID_COLUMN                         name of the column containing document id   --ededup_use_snapshot EDEDUP_USE_SNAPSHOT                         flag to continue from snapshot   --ededup_snapshot_directory EDEDUP_SNAPSHOT_DIRECTORY                         location of snapshot files</code></p> <p>These correspond to the configuration keys described above.</p>"},{"location":"transforms/universal/fdedup/","title":"Fuzzy Deduplification Transform","text":"<p>The fdedup transforms removes documents that are very similar to each other within a set of parquet files,  per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/fdedup/kfp_ray/","title":"Fuzzy Deduplication Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/fdedup/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/fdedup/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/fdedup/ray/","title":"Fuzzy Dedup","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/fdedup/ray/#summary","title":"Summary","text":"<p>The basic implementation of the fuzzy dedup is based on MinHash. Also see here for more details. The architecture of the implementation is presented here:</p> <p></p> <p>The main components of implementation are driver, processors (implemented as actor pools) - table processor, table  filter and bucket hash processor, and hash actors - minhash, buckets and docs. </p> <p>The complication of mapping this model to transform model is the fact that in this model assumes a two pass processing,  while a transform model is a single pass. The solution to this mismatch is to use transform runtime to implement the  first path and use the native transform pipeline to implement filtering.</p>"},{"location":"transforms/universal/fdedup/ray/#transform-runtime","title":"Transform runtime","text":"<p>The transform runtime is implementing complete first path of the fuzzy deduping: * creates bucket and minhash collectors * implements initial file processing to populate bucket and minhash caches * creates doc collectors  * implement bucket processing * Clean up everything except for doc collectors in preparation to filter, that is implemented by the framework proper The main components of runtime are described below</p>"},{"location":"transforms/universal/fdedup/ray/#tableprocessor-actor","title":"TableProcessor Actor","text":"<p>Table processing actor is implemented following framework itself is implemented as a pair - <code>FdedupTransform</code> implementing the actual transformation and and  transform table processor  (from the framework itself).</p>"},{"location":"transforms/universal/fdedup/ray/#docsminhash-actor","title":"DocsMinHash Actor","text":"<p>This actor stores MInHashes</p>"},{"location":"transforms/universal/fdedup/ray/#bucketshash-actor","title":"BucketsHash Actor","text":"<p>This actor actor</p>"},{"location":"transforms/universal/fdedup/ray/#buckethashprocessor","title":"BucketHashProcessor","text":"<p>BucketHash actor implement the actual buckets processing, removing duplicates.  Implementation of this actor allows to better manage this \"expensive\" process, by using Actor pool load balancing thus minimizing overall time for this operation. Instead of pre partitioning buckets, it is using dynamic load partitioning. We also are processing \"longest\" buckets first thus further improving performance. To further improve the overall performance we can in future implement bucket splitting - its faster to process more smaller buckets  then the long ones</p>"},{"location":"transforms/universal/fdedup/ray/#buckethashprocessor_1","title":"BucketHashProcessor","text":"<p>This actor is queueing up requests to the <code>BucketHashProcessor</code> actor pool, which load  balances their execution</p>"},{"location":"transforms/universal/fdedup/ray/#doccollector-actor","title":"DocCollector Actor","text":"<p>This actor is a collector for unique documents</p>"},{"location":"transforms/universal/fdedup/ray/#transformer","title":"Transformer","text":"<p>In the fuzzy dedup implementation, the transformer only implements filtering. For every table, it checks document ids with the <code>DocumentsCollector</code> cache and removes all of the rows which do not have ids in  the hash </p>"},{"location":"transforms/universal/fdedup/ray/#snapshotting","title":"Snapshotting","text":"<p>Fuzzy dedup often runs on very large data sets and implements three very distinct phases: * Building buckets * Processing buckets * Filtering data To improve recoverability of fuzzy dedup, current implementation includes snapshotting - at the end of the first two  phases we snapshot the current state of execution - bucket and minhash actors after the first phase and document actors  after the second. This snapshotting provide code with the ability to restart from the existing snapshot. You can use one of two configuration flags (assuming snapshots exist): * <code>use_bucket_snapshot</code> to start from the second phase * <code>use_doc_snapshot</code> to start from the third phase</p>"},{"location":"transforms/universal/fdedup/ray/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use </p> <pre><code>make build to build it\n</code></pre>"},{"location":"transforms/universal/fdedup/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding BlockListTransform configuration for values are as follows:</p> <ul> <li>bucket_cpu - specifies number of CPUs for bucket actor</li> <li>doc_cpu - specifies number of CPUs for doc actor</li> <li>mhash_cpu - specifies number of CPUs for minhash actor</li> <li>num_doc_actors - specifies number of doc actors</li> <li>num_bucket_actors - specifies number of bucket actors</li> <li>num_minhash_actors - specifies number of minhash actors</li> <li>num_preprocessors - specifies number of preprocessors</li> <li>num_permutations - specifies number of permutations</li> <li>threshold - specifies threshold</li> <li>shingles_size - specifies shingles size</li> <li>japanese_data - specifies whether to use japanese specific document splitting</li> <li>delimiters - specifies delimiter for non japanese document splitting</li> <li>snapshot_delay - delay between different actors reading/writing snapshot not to overwhelm storage</li> <li>-use_bucket_snapshot_ - run from the existing buckets snapshot (bypass building buckets)</li> <li>-use_doc_snapshot_ - run from the existing docs snapshot (bypass building and processing buckets)</li> </ul> <p>Above you see both parameters and their values for small runs (tens of files). We also provide an  estimate to roughly determine cluster size for running transformer.</p>"},{"location":"transforms/universal/fdedup/ray/#running","title":"Running","text":""},{"location":"transforms/universal/fdedup/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <pre><code>  --fdedup_doc_column FDEDUP_DOC_COLUMN\n                        document column name\n  --fdedup_id_column FDEDUP_ID_COLUMN\n                        integer document id column name\n  --fdedup_cluster_column FDEDUP_CLUSTER_COLUMN\n                        cluster column name\n  --fdedup_bucket_cpu FDEDUP_BUCKET_CPU\n                        number of CPUs per bucket hash\n  --fdedup_mhash_cpu FDEDUP_MHASH_CPU\n                        number of CPUs per minhash hash\n  --fdedup_doc_cpu FDEDUP_DOC_CPU\n                        number of CPUs per doc hash\n  --fdedup_num_doc_actors FDEDUP_NUM_DOC_ACTORS\n                        number of doc actors to use\n  --fdedup_num_minhash_actors FDEDUP_NUM_MINHASH_ACTORS\n                        number of minhash actors to use\n  --fdedup_num_bucket_actors FDEDUP_NUM_BUCKET_ACTORS\n                        number of bucket actors to use\n  --fdedup_num_preprocessors FDEDUP_NUM_PREPROCESSORS\n                        number of preprocessors to use\n  --fdedup_num_permutations FDEDUP_NUM_PERMUTATIONS\n                        number of permutations\n  --fdedup_threshold FDEDUP_THRESHOLD\n                        threshold\n  --fdedup_shingles_size FDEDUP_SHINGLES_SIZE\n                        number of words in shingle\n  --fdedup_delimiters FDEDUP_DELIMITERS\n                        delimiter for splitting document\n  --fdedup_snapshot_delay FDEDUP_SNAPSHOT_DELAY\n                        snapshot delay time\n  --fdedup_use_bucket_snapshot FDEDUP_USE_BUCKET_SNAPSHOT\n                        flag to continue with bucket snapshot\n  --fdedup_use_doc_snapshot FDEDUP_USE_DOC_SNAPSHOT\n                        flag to continue with doc snapshot\n  --fdedup_random_delay_limit FDEDUP_RANDOM_DELAY_LIMIT\n                        maximum delay between read\n</code></pre> <p>These correspond to the configuration keys described above.</p>"},{"location":"transforms/universal/fdedup/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/fdedup_transform_ray.py using command line args</li> <li><code>run-local-sample</code> - runs src/fdedup_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/fdedup_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/fdedup/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/filter/","title":"Filter Transform","text":"<p>The filter transforms provides SQL-based expressions for filtering rows and optionally column removal from parquet files,  per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the python-based transformation in a Ray runtime</li> <li>spark - enables the running of a spark-based transformation in a Spark runtime. </li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/filter/kfp_ray/","title":"Filter Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/filter/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/filter/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/filter/python/","title":"Filtering Transform for Python Runtime","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/filter/python/#summary","title":"Summary","text":"<p>Filtering cleans up data by:  * Removing the rows that do not meet a specific set of criteria.  * Dropping the columns that are no longer needed (e.g. annotation columns, used for filtering rows).</p>"},{"location":"transforms/universal/filter/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding FilterTransform  configuration for values are as follows:</p> <ul> <li>filter_criteria_list - specifies the list of row filter criteria (in SQL WHERE clause format). Each filter criterion is a string. The default value of this parameter is <code>[]</code> (an empty list, meaning that all the rows in the input table will be kept). </li> <li>filter_logical_operator - specifies the logical operator that joins filter criteria (<code>AND</code> or <code>OR</code>). The default value of this parameter is <code>AND</code>.</li> <li>filter_columns_to_drop - the list with the names of the columns to drop after row filtering is complete. The default value of this parameter is <code>[]</code> (an empty list, meaning that all the columns in the input table will be kept)</li> </ul>"},{"location":"transforms/universal/filter/python/#example","title":"Example","text":"<p>Consider a table with eight text documents, where each row has additional info about that document (date acquired, source URL, etc.), and a set of quality signals for that document.  </p> <pre><code>|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 document | title    | contents | date_acq | extra    | cluster  | ft_lang | ft_score | docq_tot | docq_mea | docq_pe \u2502\n\u2502 ---      | ---      | ---      | uired    | ---      | ---      | ---     | ---      | al_words | n_word_l | rplex_s \u2502\n\u2502 str      | str      | str      | ---      | struct[5 | i64      | str     | f64      | ---      | en       | core    \u2502\n\u2502          |          |          | datetime | ]        |          |         |          | i64      | ---      | ---     \u2502\n\u2502          |          |          | [ns]     |          |          |         |          |          | f64      | f64     \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 CC-MAIN- | https:// | BACKGROU | 2023-07- | {\"applic | -1       | en      | 1.0      | 77       | 5.662338 | 226.5   \u2502\n\u2502 20190221 | www.sema | ND       | 05       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 132217-2 | nticscho | The      | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01902211 | lar.org/ | Rhinitis |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Control  |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502          |          | \u2026        |          |          |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Travel + | 2023-07- | {\"applic | -1       | en      | 1.0      | 321      | 5.05919  | 245.0   \u2502\n\u2502 20200528 | www.torn | Leisure: | 27       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 232803-2 | osnews.g | The 5    | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 02005290 | r/en/tou | best     |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | res\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Stourbri | 2023-07- | {\"applic | -1       | en      | 1.0      | 646      | 5.27709  | 230.3   \u2502\n\u2502 20190617 | www.stou | dge      | 04       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 103006-2 | rbridgen | College  | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01906171 | ews.co.u | to close |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | BMe\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Our      | 2023-07- | {\"applic | -1       | en      | 1.0      | 242      | 5.557851 | 407.2   \u2502\n\u2502 20180318 | www.univ | Guidance | 19       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 184945-2 | ariety.c | Philosop | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01803182 | om/app/s | hy       |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | High     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502          |          | sch\u2026     |          |          |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://h | Hukun    | 2023-07- | {\"applic | -1       | en      | 1.0      | 169      | 4.840237 | 240.5   \u2502\n\u2502 20180120 | urur.com | Hurur    | 18       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 083038-2 | /hukun-h | running  | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01801201 | urur-run | for Ward |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | 1 c\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Life's   | 2023-07- | {\"applic | -1       | en      | 1.0      | 61       | 4.786885 | 244.0   \u2502\n\u2502 20180522 | www.chap | Reverie  | 18       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 131652-2 | ters.ind | Kobo     | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01805221 | igo.ca/e | ebook |  |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Sept\u2026    |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://w | Kamis,   | 2023-07- | {\"applic | 18008253 | en      | 1.0      | 509      | 4.738703 | 224.6   \u2502\n\u2502 20181120 | ww.onedo | 10 Maret | 05       | ation/ht | 113      |         |          |          |          |         \u2502\n\u2502 130743-2 | llarfoll | 2016     | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01811201 | owers.co | Buy      |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Twitter\u2026 |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://w | Rory     | 2023-07- | {\"applic | -1       | en      | 1.0      | 223      | 4.829596 | 167.5   \u2502\n\u2502 20171213 | ww.iron- | Fallon   | 09       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 104259-2 | bru.co.u | joins    | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01712131 | k/rory-f | Bristol  |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Rovers\u2026  |          | \u2026        |          |         |          |          |          |         \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n</code></pre>"},{"location":"transforms/universal/filter/python/#example-1-two-numerical-filtering-criteria-joined-by-and","title":"Example 1 - two numerical filtering criteria joined by AND","text":"<p>To filter this table and only keep the documents that have between 100 and 500 words and a perplexity score less than 230, and furthermore, drop the <code>extra</code> and <code>cluster</code> columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"docq_total_words &gt; 100 AND docq_total_words &lt; 500\", \"docq_perplex_score &lt; 230\"]\nfilter_logical_operator = \"AND\"\nfilter_columns_to_drop = [\"extra\", \"cluster\"]\n</code></pre> This filter operation applied on the table above will return the following result: <pre><code>|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 document    | title       | contents    | date_acquir | ft_lang | ft_score | docq_total_ | docq_mean_w | docq_perple \u2502\n\u2502 ---         | ---         | ---         | ed          | ---     | ---      | words       | ord_len     | x_score     \u2502\n\u2502 str         | str         | str         | ---         | str     | f64      | ---         | ---         | ---         \u2502\n\u2502             |             |             | datetime[ns |         |          | i64         | f64         | f64         \u2502\n\u2502             |             |             | ]           |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 CC-MAIN-201 | http://www. | Rory Fallon | 2023-07-09  | en      | 1.0      | 223         | 4.829596    | 167.5       \u2502\n\u2502 71213104259 | iron-bru.co | joins       | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201712131\u2026 | .uk/rory-f\u2026 | Bristol     |             |         |          |             |             |             \u2502\n\u2502             |             | Rovers\u2026     |             |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n</code></pre></p>"},{"location":"transforms/universal/filter/python/#example-2-two-numerical-filtering-criteria-joined-by-or","title":"Example 2 - two numerical filtering criteria joined by OR","text":"<p>To filter this table and only keep the documents that have between 100 and 500 words or a perplexity score less than 230, and furthermore, drop the <code>extra</code> and <code>cluster</code> columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"docq_total_words &gt; 100 AND docq_total_words &lt; 500\", \"docq_perplex_score &lt; 230\"]\nfilter_logical_operator = \"OR\"\nfilter_columns_to_drop = [\"extra\", \"cluster\"]\n</code></pre> This filter operation applied on the table above will return the following result: <pre><code>|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 document    | title       | contents    | date_acquir | ft_lang | ft_score | docq_total_ | docq_mean_w | docq_perple |\n\u2502 ---         | ---         | ---         | ed          | ---     | ---      | words       | ord_len     | x_score     \u2502\n\u2502 str         | str         | str         | ---         | str     | f64      | ---         | ---         | ---         \u2502\n\u2502             |             |             | datetime[ns |         |          | i64         | f64         | f64         \u2502\n\u2502             |             |             | ]           |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 CC-MAIN-201 | https://www | BACKGROUND  | 2023-07-05  | en      | 1.0      | 77          | 5.662338    | 226.5       \u2502\n\u2502 90221132217 | .semanticsc | The         | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201902211\u2026 | holar.org/\u2026 | Rhinitis    |             |         |          |             |             |             \u2502\n\u2502             |             | Control \u2026   |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://www. | Kamis, 10   | 2023-07-05  | en      | 1.0      | 509         | 4.738703    | 224.6       \u2502\n\u2502 81120130743 | onedollarfo | Maret 2016  | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201811201\u2026 | llowers.co\u2026 | Buy         |             |         |          |             |             |             \u2502\n\u2502             |             | Twitter\u2026    |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://www. | Rory Fallon | 2023-07-09  | en      | 1.0      | 223         | 4.829596    | 167.5       \u2502\n\u2502 71213104259 | iron-bru.co | joins       | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201712131\u2026 | .uk/rory-f\u2026 | Bristol     |             |         |          |             |             |             \u2502\n\u2502             |             | Rovers\u2026     |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-202 | https://www | Travel +    | 2023-07-27  | en      | 1.0      | 321         | 5.05919     | 245.0       \u2502\n\u2502 00528232803 | .tornosnews | Leisure:    | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -202005290\u2026 | .gr/en/tou\u2026 | The 5 best  |             |         |          |             |             |             \u2502\n\u2502             |             | res\u2026        |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | https://www | Our         | 2023-07-19  | en      | 1.0      | 242         | 5.557851    | 407.2       \u2502\n\u2502 80318184945 | .univariety | Guidance    | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201803182\u2026 | .com/app/s\u2026 | Philosophy  |             |         |          |             |             |             \u2502\n\u2502             |             | High sch\u2026   |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://huru | Hukun Hurur | 2023-07-18  | en      | 1.0      | 169         | 4.840237    | 240.5       \u2502\n\u2502 80120083038 | r.com/hukun | running for | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201801201\u2026 | -hurur-run\u2026 | Ward 1 c\u2026   |             |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n</code></pre></p>"},{"location":"transforms/universal/filter/python/#example-3-two-filtering-criteria-based-on-non-numerical-datetime-and-string-types","title":"Example 3 - two filtering criteria based on non-numerical (datetime and string) types","text":"<p>To filter this table and only keep the documents that were acquired between 2023-07-04 and 2023-07-08 and were downloaded using the <code>HTTPS</code> protocol, without dropping any columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\", \"title LIKE 'https://%'\"]\nfilter_logical_operator = \"AND\"\nfilter_columns_to_drop = []\n</code></pre></p> <p>This filter operation applied on the table above will return the following result: <pre><code>|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 document | title    | contents | date_acq | extra    | cluster  | ft_lang | ft_score | docq_tot | docq_mea | docq_pe \u2502\n\u2502 ---      | ---      | ---      | uired    | ---      | ---      | ---     | ---      | al_words | n_word_l | rplex_s \u2502\n\u2502 str      | str      | str      | ---      | struct[5 | i64      | str     | f64      | ---      | en       | core    \u2502\n\u2502          |          |          | datetime | ]        |          |         |          | i64      | ---      | ---     \u2502\n\u2502          |          |          | [ns]     |          |          |         |          |          | f64      | f64     \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 CC-MAIN- | https:// | BACKGROU | 2023-07- | {\"applic | -1      | en      | 1.0      | 77       | 5.662338 | 226.5    \u2502\n\u2502 20190221 | www.sema | ND       | 05       | ation/ht |         |         |          |          |          |          \u2502\n\u2502 132217-2 | nticscho | The      | 05:00:00 | tp; msgt |         |         |          |          |          |          \u2502\n\u2502 01902211 | lar.org/ | Rhinitis |          | ype=resp |         |         |          |          |          |          \u2502\n\u2502 \u2026        | \u2026        | Control  |          | \u2026        |         |         |          |          |          |          \u2502\n\u2502          |          | \u2026        |          |          |         |         |          |          |          |          \u2502\n\u2502 CC-MAIN- | https:// | Stourbri | 2023-07- | {\"applic | -1      | en      | 1.0      | 646      | 5.27709  | 230.3    \u2502\n\u2502 20190617 | www.stou | dge      | 04       | ation/ht |         |         |          |          |          |          \u2502\n\u2502 103006-2 | rbridgen | College  | 05:00:00 | tp; msgt |         |         |          |          |          |          \u2502\n\u2502 01906171 | ews.co.u | to close |          | ype=resp |         |         |          |          |          |          \u2502\n\u2502 \u2026        | \u2026        | BMe\u2026     |          | \u2026        |         |         |          |          |          |          \u2502\n|----------|----------|----------|----------|----------|---------|---------|----------|----------|----------|----------|\n</code></pre></p>"},{"location":"transforms/universal/filter/python/#running","title":"Running","text":"<p>You can run the filter_local.py (python-only implementation) or filter_local_ray.py (ray-based  implementation) to transform the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p>"},{"location":"transforms/universal/filter/python/#running-as-ray-based-application","title":"Running as ray-based application","text":"<pre><code>(venv) cma:src$ python filter_local_ray.py\n12:48:01 INFO - Running locally\n12:48:01 INFO - Using local configuration with: input_folder - /home/cma/de/data-prep-kit/transforms/universal/filtering/test-data/input output_folder - /home/cma/de/data-prep-kit/transforms/universal/filtering/output\n12:48:01 INFO - Not using data sets, checkpointing False, max files -1\n12:48:01 INFO - number of workers 5 worker options {'num_cpus': 0.8}\n12:48:01 INFO - pipeline id pipeline_id; number workers 5\n12:48:01 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n12:48:01 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n12:48:01 INFO - actor creation delay 0\n2024-03-31 12:48:03,390 INFO worker.py:1715 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 \n(orchestrate pid=308034) 12:48:04 INFO - orchestrator started at 2024-03-31 12:48:04\n(orchestrate pid=308034) 12:48:04 INFO - Number of files is 1, source profile {'max_file_size': 0.15915775299072266, 'min_file_size': 0.15915775299072266, 'total_file_size': 0.15915775299072266}\n(orchestrate pid=308034) 12:48:04 INFO - Cluster resources: {'cpus': 20, 'gpus': 0, 'memory': 31.60095291212201, 'object_store': 15.800476455129683}\n(orchestrate pid=308034) 12:48:04 INFO - Number of workers - 5 with {'num_cpus': 0.8} each\n(orchestrate pid=308034) 12:48:04 INFO - Completed 0 files in 5.312760670979818e-06 min. Waiting for completion\n(orchestrate pid=308034) 12:48:06 INFO - Completed processing in 0.022701112429300944 min\n12:48:16 INFO - Completed execution in 0.24697633584340414 min, execution result 0\n</code></pre>"},{"location":"transforms/universal/filter/python/#running-as-pure-python-application","title":"Running as pure python application","text":"<pre>\n% make venv\n% source venv/bin/activate\n(venv) % cd src\n(venv) % python filter_local_ray.py\ninput table has 100 rows\n\noutput table has 11 rows\noutput metadata : {'total_docs_count': 100, 'total_bytes_count': 478602, 'total_columns_count': 25, \"docs_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'\": 78, \"bytes_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'\": 429191, \"docs_filtered_by 'docq_perplex_score &lt; 230'\": 53, \"bytes_filtered_by 'docq_perplex_score &lt; 230'\": 275911, 'docs_after_filter': 11, 'bytes_after_filter': 24061, 'columns_after_filter': 23}\n(venv) % deactivate\n% ls ../output\nmetadata.json   test1.parquet\n%\n</pre>"},{"location":"transforms/universal/filter/python/#passing-parameters-through-command-line-interface","title":"Passing parameters through command-line-interface","text":"<p>When running filtering on a local terminal, double quotes need to be escaped accordingly. For example, to find documents that are written in Java or Python programming languages, a SQL query using the <code>IN</code> keyword is needed in the <code>filter_criteria_list</code> argument. The example below shows how to properly pass this argument to the filter app: <pre><code>python filter_transform_ray.py --filter_criteria_list \"[\\\"language IN ('Java', 'Python')\\\"]\" ...\n</code></pre> When filter runs from the command line, it needs to include the entire <code>filter_criteria_list</code> parameter within double quotes (<code>\"</code>), so that the command line parser can determine where the parameter begins and ends. This, however, will conflict with the internal double quotes that are used to specify the conditions inside the list (<code>language IN ('Java', 'Python')</code>). To resolve this problem, the internal double quotes need to be escaped, as in the \\\"language IN ('Java', 'Python')\\\" notation.</p>"},{"location":"transforms/universal/filter/python/#filter-statistics","title":"Filter Statistics","text":"<p>As shown in the output of the local run of filtering, the metadata contains several statistics: * Global statistics:   * <code>total_docs_count</code>, <code>total_bytes_count</code>, <code>total_columns_count</code>: total number of documents (rows), bytes, and columns that were present in the input table, before filtering took place   * <code>docs_after_filter</code>, <code>bytes_after_filter</code>, <code>columns_after_filter</code>: total number of documents (rows), bytes, and columns that were present in the output table, after filtering took place * Per-criteria statistics: these statistics show the impact of each filtering criteria - number of documents and bytes that it filters out, when applied by itself. We ran the local filter with two filtering criteria, and these are the stats for each of them:   * <code>docs_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'</code>, <code>bytes_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'</code> - the number of documents and bytes filtered out by the <code>docq_total_words &gt; 100 AND docq_total_words &lt; 200</code> filtering condition   * <code>docs_filtered_by 'docq_perplex_score &lt; 230'</code>, <code>bytes_filtered_by 'docq_perplex_score &lt; 230'</code>  - the number of documents and bytes filtered out by the <code>docq_perplex_score &lt; 230</code> filtering condition  </p>"},{"location":"transforms/universal/filter/python/#running_1","title":"Running","text":""},{"location":"transforms/universal/filter/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher.</p> <pre><code>  --filter_criteria_list FILTER_CRITERIA_LIST\n                        list of filter criteria (in SQL WHERE clause format), for example: [\n                          \"docq_total_words &gt; 100 AND docq_total_words &lt; 200\",\n                          \"docq_perplex_score &lt; 230\",\n                          \"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\",\n                          \"title LIKE 'https://%'\",\n                          \"document_id IN ('doc-id-1', 'doc-id-2', 'doc-id-3')\"\n                        ]\n  --filter_columns_to_drop FILTER_COLUMNS_TO_DROP\n                        list of columns to drop after filtering, for example: [\"column1\", \"column2\"]\n  --filter_logical_operator {AND,OR}\n                        logical operator (AND or OR) that joins filter criteria\n</code></pre>"},{"location":"transforms/universal/filter/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/filter_transform_ray.py using command line args</li> <li><code>run-local-python-only-sample</code> - runs src/filter_local.py</li> <li><code>run-local-sample</code> - runs src/filter_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/filter_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/filter/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/filter/ray/","title":"Fltering Transform for Ray","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/filter/ray/#summary","title":"Summary","text":"<p>This project wraps the filter transform with a Ray runtime.</p>"},{"location":"transforms/universal/filter/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Noop configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/filter/ray/#running","title":"Running","text":""},{"location":"transforms/universal/filter/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/universal/filter/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/filter_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/filter_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/filter_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/filter/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/filter/spark/","title":"Filtering","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/filter/spark/#summary","title":"Summary","text":"<p>Filtering cleans up data by:  * Removing the rows that do not meet a specific set of criteria.  * Dropping the columns that are no longer needed (e.g. annotation columns, used for filtering rows).</p>"},{"location":"transforms/universal/filter/spark/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding FilterTransform  configuration for values are as follows:</p> <ul> <li>filter_criteria_list - specifies the list of row filter criteria (in SQL WHERE clause format). Each filter criterion is a string. The default value of this parameter is <code>[]</code> (an empty list, meaning that all the rows in the input table will be kept). </li> <li>filter_logical_operator - specifies the logical operator that joins filter criteria (<code>AND</code> or <code>OR</code>). The default value of this parameter is <code>AND</code>.</li> <li>filter_columns_to_drop - the list with the names of the columns to drop after row filtering is complete. The default value of this parameter is <code>[]</code> (an empty list, meaning that all the columns in the input table will be kept)</li> </ul>"},{"location":"transforms/universal/filter/spark/#example","title":"Example","text":"<p>Consider a table with eight text documents, where each row has additional info about that document (date acquired, source URL, etc.), and a set of quality signals for that document.  </p> <pre><code>|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 document | title    | contents | date_acq | extra    | cluster  | ft_lang | ft_score | docq_tot | docq_mea | docq_pe \u2502\n\u2502 ---      | ---      | ---      | uired    | ---      | ---      | ---     | ---      | al_words | n_word_l | rplex_s \u2502\n\u2502 str      | str      | str      | ---      | struct[5 | i64      | str     | f64      | ---      | en       | core    \u2502\n\u2502          |          |          | datetime | ]        |          |         |          | i64      | ---      | ---     \u2502\n\u2502          |          |          | [ns]     |          |          |         |          |          | f64      | f64     \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 CC-MAIN- | https:// | BACKGROU | 2023-07- | {\"applic | -1       | en      | 1.0      | 77       | 5.662338 | 226.5   \u2502\n\u2502 20190221 | www.sema | ND       | 05       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 132217-2 | nticscho | The      | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01902211 | lar.org/ | Rhinitis |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Control  |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502          |          | \u2026        |          |          |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Travel + | 2023-07- | {\"applic | -1       | en      | 1.0      | 321      | 5.05919  | 245.0   \u2502\n\u2502 20200528 | www.torn | Leisure: | 27       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 232803-2 | osnews.g | The 5    | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 02005290 | r/en/tou | best     |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | res\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Stourbri | 2023-07- | {\"applic | -1       | en      | 1.0      | 646      | 5.27709  | 230.3   \u2502\n\u2502 20190617 | www.stou | dge      | 04       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 103006-2 | rbridgen | College  | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01906171 | ews.co.u | to close |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | BMe\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Our      | 2023-07- | {\"applic | -1       | en      | 1.0      | 242      | 5.557851 | 407.2   \u2502\n\u2502 20180318 | www.univ | Guidance | 19       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 184945-2 | ariety.c | Philosop | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01803182 | om/app/s | hy       |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | High     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502          |          | sch\u2026     |          |          |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://h | Hukun    | 2023-07- | {\"applic | -1       | en      | 1.0      | 169      | 4.840237 | 240.5   \u2502\n\u2502 20180120 | urur.com | Hurur    | 18       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 083038-2 | /hukun-h | running  | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01801201 | urur-run | for Ward |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | 1 c\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Life's   | 2023-07- | {\"applic | -1       | en      | 1.0      | 61       | 4.786885 | 244.0   \u2502\n\u2502 20180522 | www.chap | Reverie  | 18       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 131652-2 | ters.ind | Kobo     | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01805221 | igo.ca/e | ebook |  |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Sept\u2026    |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://w | Kamis,   | 2023-07- | {\"applic | 18008253 | en      | 1.0      | 509      | 4.738703 | 224.6   \u2502\n\u2502 20181120 | ww.onedo | 10 Maret | 05       | ation/ht | 113      |         |          |          |          |         \u2502\n\u2502 130743-2 | llarfoll | 2016     | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01811201 | owers.co | Buy      |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Twitter\u2026 |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://w | Rory     | 2023-07- | {\"applic | -1       | en      | 1.0      | 223      | 4.829596 | 167.5   \u2502\n\u2502 20171213 | ww.iron- | Fallon   | 09       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 104259-2 | bru.co.u | joins    | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01712131 | k/rory-f | Bristol  |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Rovers\u2026  |          | \u2026        |          |         |          |          |          |         \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n</code></pre>"},{"location":"transforms/universal/filter/spark/#example-1-two-numerical-filtering-criteria-joined-by-and","title":"Example 1 - two numerical filtering criteria joined by AND","text":"<p>To filter this table and only keep the documents that have between 100 and 500 words and a perplexity score less than 230, and furthermore, drop the <code>extra</code> and <code>cluster</code> columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"docq_total_words &gt; 100 AND docq_total_words &lt; 500\", \"docq_perplex_score &lt; 230\"]\nfilter_logical_operator = \"AND\"\nfilter_columns_to_drop = [\"extra\", \"cluster\"]\n</code></pre> This filter operation applied on the table above will return the following result: <pre><code>|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 document    | title       | contents    | date_acquir | ft_lang | ft_score | docq_total_ | docq_mean_w | docq_perple \u2502\n\u2502 ---         | ---         | ---         | ed          | ---     | ---      | words       | ord_len     | x_score     \u2502\n\u2502 str         | str         | str         | ---         | str     | f64      | ---         | ---         | ---         \u2502\n\u2502             |             |             | datetime[ns |         |          | i64         | f64         | f64         \u2502\n\u2502             |             |             | ]           |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 CC-MAIN-201 | http://www. | Rory Fallon | 2023-07-09  | en      | 1.0      | 223         | 4.829596    | 167.5       \u2502\n\u2502 71213104259 | iron-bru.co | joins       | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201712131\u2026 | .uk/rory-f\u2026 | Bristol     |             |         |          |             |             |             \u2502\n\u2502             |             | Rovers\u2026     |             |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n</code></pre></p>"},{"location":"transforms/universal/filter/spark/#example-2-two-numerical-filtering-criteria-joined-by-or","title":"Example 2 - two numerical filtering criteria joined by OR","text":"<p>To filter this table and only keep the documents that have between 100 and 500 words or a perplexity score less than 230, and furthermore, drop the <code>extra</code> and <code>cluster</code> columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"docq_total_words &gt; 100 AND docq_total_words &lt; 500\", \"docq_perplex_score &lt; 230\"]\nfilter_logical_operator = \"OR\"\nfilter_columns_to_drop = [\"extra\", \"cluster\"]\n</code></pre> This filter operation applied on the table above will return the following result: <pre><code>|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 document    | title       | contents    | date_acquir | ft_lang | ft_score | docq_total_ | docq_mean_w | docq_perple |\n\u2502 ---         | ---         | ---         | ed          | ---     | ---      | words       | ord_len     | x_score     \u2502\n\u2502 str         | str         | str         | ---         | str     | f64      | ---         | ---         | ---         \u2502\n\u2502             |             |             | datetime[ns |         |          | i64         | f64         | f64         \u2502\n\u2502             |             |             | ]           |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 CC-MAIN-201 | https://www | BACKGROUND  | 2023-07-05  | en      | 1.0      | 77          | 5.662338    | 226.5       \u2502\n\u2502 90221132217 | .semanticsc | The         | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201902211\u2026 | holar.org/\u2026 | Rhinitis    |             |         |          |             |             |             \u2502\n\u2502             |             | Control \u2026   |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://www. | Kamis, 10   | 2023-07-05  | en      | 1.0      | 509         | 4.738703    | 224.6       \u2502\n\u2502 81120130743 | onedollarfo | Maret 2016  | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201811201\u2026 | llowers.co\u2026 | Buy         |             |         |          |             |             |             \u2502\n\u2502             |             | Twitter\u2026    |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://www. | Rory Fallon | 2023-07-09  | en      | 1.0      | 223         | 4.829596    | 167.5       \u2502\n\u2502 71213104259 | iron-bru.co | joins       | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201712131\u2026 | .uk/rory-f\u2026 | Bristol     |             |         |          |             |             |             \u2502\n\u2502             |             | Rovers\u2026     |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-202 | https://www | Travel +    | 2023-07-27  | en      | 1.0      | 321         | 5.05919     | 245.0       \u2502\n\u2502 00528232803 | .tornosnews | Leisure:    | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -202005290\u2026 | .gr/en/tou\u2026 | The 5 best  |             |         |          |             |             |             \u2502\n\u2502             |             | res\u2026        |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | https://www | Our         | 2023-07-19  | en      | 1.0      | 242         | 5.557851    | 407.2       \u2502\n\u2502 80318184945 | .univariety | Guidance    | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201803182\u2026 | .com/app/s\u2026 | Philosophy  |             |         |          |             |             |             \u2502\n\u2502             |             | High sch\u2026   |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://huru | Hukun Hurur | 2023-07-18  | en      | 1.0      | 169         | 4.840237    | 240.5       \u2502\n\u2502 80120083038 | r.com/hukun | running for | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201801201\u2026 | -hurur-run\u2026 | Ward 1 c\u2026   |             |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n</code></pre></p>"},{"location":"transforms/universal/filter/spark/#example-3-two-filtering-criteria-based-on-non-numerical-datetime-and-string-types","title":"Example 3 - two filtering criteria based on non-numerical (datetime and string) types","text":"<p>To filter this table and only keep the documents that were acquired between 2023-07-04 and 2023-07-08 and were downloaded using the <code>HTTPS</code> protocol, without dropping any columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\", \"title LIKE 'https://%'\"]\nfilter_logical_operator = \"AND\"\nfilter_columns_to_drop = []\n</code></pre></p> <p>This filter operation applied on the table above will return the following result: <pre><code>|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 document | title    | contents | date_acq | extra    | cluster  | ft_lang | ft_score | docq_tot | docq_mea | docq_pe \u2502\n\u2502 ---      | ---      | ---      | uired    | ---      | ---      | ---     | ---      | al_words | n_word_l | rplex_s \u2502\n\u2502 str      | str      | str      | ---      | struct[5 | i64      | str     | f64      | ---      | en       | core    \u2502\n\u2502          |          |          | datetime | ]        |          |         |          | i64      | ---      | ---     \u2502\n\u2502          |          |          | [ns]     |          |          |         |          |          | f64      | f64     \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 CC-MAIN- | https:// | BACKGROU | 2023-07- | {\"applic | -1      | en      | 1.0      | 77       | 5.662338 | 226.5    \u2502\n\u2502 20190221 | www.sema | ND       | 05       | ation/ht |         |         |          |          |          |          \u2502\n\u2502 132217-2 | nticscho | The      | 05:00:00 | tp; msgt |         |         |          |          |          |          \u2502\n\u2502 01902211 | lar.org/ | Rhinitis |          | ype=resp |         |         |          |          |          |          \u2502\n\u2502 \u2026        | \u2026        | Control  |          | \u2026        |         |         |          |          |          |          \u2502\n\u2502          |          | \u2026        |          |          |         |         |          |          |          |          \u2502\n\u2502 CC-MAIN- | https:// | Stourbri | 2023-07- | {\"applic | -1      | en      | 1.0      | 646      | 5.27709  | 230.3    \u2502\n\u2502 20190617 | www.stou | dge      | 04       | ation/ht |         |         |          |          |          |          \u2502\n\u2502 103006-2 | rbridgen | College  | 05:00:00 | tp; msgt |         |         |          |          |          |          \u2502\n\u2502 01906171 | ews.co.u | to close |          | ype=resp |         |         |          |          |          |          \u2502\n\u2502 \u2026        | \u2026        | BMe\u2026     |          | \u2026        |         |         |          |          |          |          \u2502\n|----------|----------|----------|----------|----------|---------|---------|----------|----------|----------|----------|\n</code></pre></p>"},{"location":"transforms/universal/filter/spark/#running","title":"Running","text":"<p>You can run the Spark filter transform filter_local.py to filter the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain one or several filtered parquet files and the <code>metadata.json</code> file.</p>"},{"location":"transforms/universal/filter/spark/#running-as-spark-based-application","title":"Running as Spark-based application","text":"<pre><code>(venv) cma:spark$ python src/filter_local.py \n18:57:46 INFO - data factory data_ is using local data access: input_folder - /home/cma/de/data-prep-kit/transforms/universal/filter/spark/test-data/input output_folder - /home/cma/de/data-prep-kit/transforms/universal/filter/spark/output at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:185\"\n18:57:46 INFO - data factory data_ max_files -1, n_sample -1 at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:201\"\n18:57:46 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:214\"\n18:57:46 INFO - pipeline id pipeline_id at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:80\"\n18:57:46 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'} at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:83\"\n18:57:46 INFO - spark execution config : {'spark_local_config_filepath': '/home/cma/de/data-prep-kit/transforms/universal/filter/spark/config/spark_profile_local.yml', 'spark_kube_config_filepath': 'config/spark_profile_kube.yml'} at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_execution_config.py:42\"\n24/05/26 18:57:47 WARN Utils: Your hostname, li-7aed0a4c-2d51-11b2-a85c-dfad31db696b.ibm.com resolves to a loopback address: 127.0.0.1; using 192.168.1.223 instead (on interface wlp0s20f3)\n24/05/26 18:57:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/26 18:57:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18:57:50 INFO - files = ['/home/cma/de/data-prep-kit/transforms/universal/filter/spark/test-data/input/test1.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_launcher.py:188\"\n</code></pre> <p>Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/filter/spark/#passing-parameters-through-command-line-interface","title":"Passing parameters through command-line-interface","text":"<p>When running filtering on a local terminal, double quotes need to be escaped accordingly. For example, to find documents that are written in Java or Python programming languages, a SQL query using the <code>IN</code> keyword is needed in the <code>filter_criteria_list</code> argument. The example below shows how to properly pass this argument to the filter app: <pre><code>python filter_transform_spark.py --filter_criteria_list \"[\\\"language IN ('Java', 'Python')\\\"]\" ...\n</code></pre> When filter runs from the command line, it needs to include the entire <code>filter_criteria_list</code> parameter within double quotes (<code>\"</code>), so that the command line parser can determine where the parameter begins and ends. This, however, will conflict with the internal double quotes that are used to specify the conditions inside the list (<code>language IN ('Java', 'Python')</code>). To resolve this problem, the internal double quotes need to be escaped, as in the \\\"language IN ('Java', 'Python')\\\" notation.</p>"},{"location":"transforms/universal/filter/spark/#filter-statistics","title":"Filter Statistics","text":"<p>As shown in the output of the local run of filtering, the metadata contains several statistics: * Global statistics:   * <code>total_docs_count</code>, <code>total_bytes_count</code>, <code>total_columns_count</code>: total number of documents (rows), bytes, and columns that were present in the input table, before filtering took place   * <code>docs_after_filter</code>, <code>bytes_after_filter</code>, <code>columns_after_filter</code>: total number of documents (rows), bytes, and columns that were present in the output table, after filtering took place * Per-criteria statistics: these statistics show the impact of each filtering criteria - number of documents and bytes that it filters out, when applied by itself. We ran the local filter with two filtering criteria, and these are the stats for each of them:   * <code>docs_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'</code>, <code>bytes_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'</code> - the number of documents and bytes filtered out by the <code>docq_total_words &gt; 100 AND docq_total_words &lt; 200</code> filtering condition   * <code>docs_filtered_by 'docq_perplex_score &lt; 230'</code>, <code>bytes_filtered_by 'docq_perplex_score &lt; 230'</code>  - the number of documents and bytes filtered out by the <code>docq_perplex_score &lt; 230</code> filtering condition  </p>"},{"location":"transforms/universal/filter/spark/#running_1","title":"Running","text":""},{"location":"transforms/universal/filter/spark/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e. SparkTransformLauncher), the following command line arguments are available in addition to  the options provided by the spark launcher.</p> <pre><code>  --filter_criteria_list FILTER_CRITERIA_LIST\n                        list of filter criteria (in SQL WHERE clause format), for example: [\n                          \"docq_total_words &gt; 100 AND docq_total_words &lt; 200\",\n                          \"docq_perplex_score &lt; 230\",\n                          \"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\",\n                          \"title LIKE 'https://%'\",\n                          \"document_id IN ('doc-id-1', 'doc-id-2', 'doc-id-3')\"\n                        ]\n  --filter_columns_to_drop FILTER_COLUMNS_TO_DROP\n                        list of columns to drop after filtering, for example: [\"column1\", \"column2\"]\n  --filter_logical_operator {AND,OR}\n                        logical operator (AND or OR) that joins filter criteria\n</code></pre>"},{"location":"transforms/universal/filter/spark/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/hap/python/","title":"Hate, Abuse, and Profanity (HAP) Annotation","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/hap/python/#prerequisite","title":"Prerequisite","text":"<p>This repository needs NLTK and please refer to <code>requirements.txt</code>.</p>"},{"location":"transforms/universal/hap/python/#summary","title":"Summary","text":"<p>The hap transform maps a non-empty input table to an output table with an added <code>hap_score</code> column. Each row in the table represents a document, and the hap transform performs the following three steps to calculate the hap score for each document:</p> <ul> <li>Sentence spliting: we use NLTK to split the document into sentence pieces.</li> <li>hap annotation: each sentence is assigned a hap score between 0 and 1, where 1 represents hap and 0 represents non-hap.</li> <li>Aggregation: the document hap score is determined by selecting the maximum hap score among its sentences.</li> </ul>"},{"location":"transforms/universal/hap/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding HAPTransformConfiguration  configuration for values are as follows:</p> <ul> <li>--model_name_or_path - specify the HAP model, which should be compatible with HuggingFace's AutoModelForSequenceClassification. Defaults to IBM's open-source toxicity classifier <code>ibm-granite/granite-guardian-hap-38m</code>.</li> <li>--batch_size - modify it based on the infrastructure capacity. Defaults to <code>128</code>.</li> <li>--max_length - the maximum length for the tokenizer. Defaults to <code>512</code>.</li> <li>--doc_text_column - the column name containing the document text in the input .parquet file. Defaults to <code>contents</code>.</li> <li>--annotation_column - the column name containing hap (toxicity) score in the output .parquet file. Defaults to <code>hap_score</code>.</li> </ul>"},{"location":"transforms/universal/hap/python/#input-format","title":"input format","text":"<p>The input is in .parquet format and contains the following columns:</p> doc_id contents 1 GSC is very much a little Swiss Army knife for... 2 Here are only a few examples. And no, I'm not ..."},{"location":"transforms/universal/hap/python/#output-format","title":"output format","text":"<p>The output is in .parquet format and includes an additional column, in addition to those in the input:</p> doc_id contents hap_score 1 GSC is very much a little Swiss Army knife for... 0.002463 2 Here are only a few examples. And no, I'm not ... 0.989713"},{"location":"transforms/universal/hap/python/#how-to-run","title":"How to run","text":"<p>Place your input Parquet file in the <code>test-data/input/</code> directory. A sample file, <code>test1.parquet</code>, is available in this directory. Once done, run the script.</p> <pre><code>python hap_local_python.py\n</code></pre> <p>You will obtain the output file <code>test1.parquet</code> in the output directory.</p>"},{"location":"transforms/universal/hap/python/#throughput","title":"Throughput","text":"<p>The table below shows the throughput (tokens per second) of the HAP transform module, which primarily includes sentence splitting, HAP annotation, and HAP score aggregation. We herein compare two models:</p> <ul> <li>4-layer lightweight toxicity classifier ibm-granite/granite-guardian-hap-38m</li> <li>12-layer toxicity classifier ibm-granite/granite-guardian-hap-125m</li> </ul> <p>We processed 6,000 documents (12 MB in Parquet file size) using the HAP transform module and reported the average CPU throughput over three trials.</p> Model used in HAP transform module throughput (tokens per second) granite-guardian-hap-38m 6.16 k granite-guardian-hap-125m 1.14 k"},{"location":"transforms/universal/hap/ray/","title":"Hate, Abuse, and Profanity (HAP) Annotation","text":""},{"location":"transforms/universal/hap/ray/#hap-transform-for-ray","title":"HAP Transform for Ray","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/hap/ray/#summary","title":"Summary","text":"<p>This project wraps the hap transform with a Ray runtime.</p>"},{"location":"transforms/universal/hap/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/hap/ray/#running","title":"Running","text":""},{"location":"transforms/universal/hap/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/universal/noop/","title":"NOOP Transform","text":"<p>The NOOP transforms serves as a simple exemplar to demonstrate the development of a simple 1:1 transform.  Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>spark - enables the running of a spark-based transformation in a Spark runtime. </li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/noop/kfp_ray/","title":"NOOP Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/noop/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/noop/kfp_ray/#pipeline-file-generation","title":"Pipeline file generation","text":"<p>In order to generate a pipeline python file run <pre><code>make workflow-generate\n</code></pre> This will use the pipeline_definitions.yaml to generate the python file of the pipeline. It uses the pipeline generator directory.</p>"},{"location":"transforms/universal/noop/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/noop/python/","title":"NOOP Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/noop/python/#summary","title":"Summary","text":"<p>This transform serves as a template for transform writers as it does not perform any transformations on the input (i.e., a no-operation transform). As such, it simply copies the input parquet files to the output directory. It shows the basics of creating a simple 1:1 table transform. It also implements a single configuration value to show how configuration of the transform is implemented.</p>"},{"location":"transforms/universal/noop/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding NOOPTransform  configuration for values are as follows:</p> <ul> <li>noop_sleep_sec - specifies the number of seconds to sleep during the call the  the <code>transform()</code> method of <code>NOOPTransformation</code>.  This may be useful for simulating real transform timings and as a way to limit I/O on an S3 endpoint..</li> <li>noop_pwd - specifies a dummy password not included in metadata. Provided as an example of metadata that we want to not include in logging.</li> </ul>"},{"location":"transforms/universal/noop/python/#running","title":"Running","text":""},{"location":"transforms/universal/noop/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the python launcher. <pre><code>  --noop_sleep_sec NOOP_SLEEP_SEC\n                        Sleep actor for a number of seconds while processing the data frame, before writing the file to COS\n  --noop_pwd NOOP_PWD   A dummy password which should be filtered out of the metadata\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/universal/noop/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/noop_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/noop_local.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then </p> <p><pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/noop/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/noop/ray/","title":"NOOP Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/noop/ray/#summary","title":"Summary","text":"<p>This project wraps the noop transform with a Ray runtime.</p>"},{"location":"transforms/universal/noop/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Noop configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/noop/ray/#running","title":"Running","text":""},{"location":"transforms/universal/noop/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/universal/noop/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/noop_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/noop_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/noop_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/noop/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/noop/spark/","title":"NOOP Spark Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/noop/spark/#summary","title":"Summary","text":"<p>This project wraps the noop transform with a Ray runtime.</p>"},{"location":"transforms/universal/noop/spark/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Noop configuration and command line options are the same as for the base python transform.</p>"},{"location":"transforms/universal/noop/spark/#running","title":"Running","text":""},{"location":"transforms/universal/noop/spark/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of spark launcher are available.</p>"},{"location":"transforms/universal/noop/spark/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/noop_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/noop_local_spark.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example, <pre><code>make run-cli-sample\n...\n</code></pre> Then <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/noop/spark/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/profiler/","title":"Profiler Transform","text":"<p>Profiler implement a word count. Typical implementation of the word count is done using map reduce. * It\u2019s O(N2) complexity * shuffling with lots of data movement</p> <p>Implementation here is using \u201cstreaming\u201d aggregation, based on central cache:</p> <ul> <li>At the heart of the implementation is a cache of partial word counts, implemented as a set of Ray actors and containing   word counts processed so far.</li> <li>Individual data processors are responsible for:<ul> <li>Reading data from data plane</li> <li>tokenizing documents (we use pluggable tokenizer)</li> <li>Coordinating with distributed cache to collect overall word counts</li> </ul> </li> </ul> <p>The complication of mapping this model to transform model is the fact that implementation requires an aggregators cache, that transform mode knows nothing about. The solution here is to use transform runtime to create cache and pass it as a parameter to transforms.</p>"},{"location":"transforms/universal/profiler/kfp_ray/","title":"Profiler Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/profiler/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/profiler/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/profiler/kfp_ray/src/","title":"Profiler Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/profiler/kfp_ray/src/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/profiler/kfp_ray/src/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/profiler/python/","title":"Profiler","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/profiler/python/#summary","title":"Summary","text":"<p>Profiler implement a word count. Typical implementation of the word count is done using map reduce. * It\u2019s O(N2) complexity * shuffling with lots of data movement</p> <p>Implementation here is using \u201cstreaming\u201d aggregation, based on central cache:</p> <ul> <li>At the heart of the implementation is a cache of partial word counts, implemented as a set of Ray actors and containing  word counts processed so far.</li> <li>Individual data processors are responsible for:</li> <li>Reading data from data plane</li> <li>tokenizing documents (we use pluggable tokenizer)</li> <li>Coordinating with distributed cache to collect overall word counts</li> </ul> <p>The complication of mapping this model to transform model is the fact that implementation requires an aggregators cache,  that transform mode knows nothing about. The solution here is to use transform runtime to create cache and pass it as a parameter to transforms.</p>"},{"location":"transforms/universal/profiler/python/#transform-runtime","title":"Transform runtime","text":"<p>Transform runtime is responsible for creation cache actors and sending their  handles to the transforms themselves Additionally it writes created word counts to the data storage (as .csv files) and enhances statistics information with the information about cache size and utilization</p>"},{"location":"transforms/universal/profiler/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ProfilerTransform configuration for values are as follows:</p> <ul> <li>doc_column - specifies name of the column containing documents</li> </ul>"},{"location":"transforms/universal/profiler/python/#running","title":"Running","text":""},{"location":"transforms/universal/profiler/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Python launcher (i.e. TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <p><pre><code>  --profiler_doc_column PROFILER_DOC_COLUMN\n                        key for accessing data\n ```\n\nThese correspond to the configuration keys described above.\n\n### Running the samples\nTo run the samples, use the following `make` targets\n\n* `run-cli-sample` - runs src/ededup_transform_ray.py using command line args\n* `run-local-sample` - runs src/ededup_local_ray.py\n* `run-s3-sample` - runs src/ededup_s3_ray.py\n    * Requires prior installation of minio, depending on your platform (e.g., from [here](https://min.io/docs/minio/macos/index.html)\n      and [here](https://min.io/docs/minio/linux/index.html)\n      and invocation of `make minio-start` to load data into local minio for S3 access.\n\nThese targets will activate the virtual environment and set up any configuration needed.\nUse the `-n` option of `make` to see the detail of what is done to run the sample.\n\nFor example,\n```shell\nmake run-cli-sample\n...\n</code></pre> Then <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/profiler/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/profiler/ray/","title":"Profiler","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/profiler/ray/#summary","title":"Summary","text":"<p>This project wraps the profiler transform with a Ray runtime.</p>"},{"location":"transforms/universal/profiler/ray/#transform-runtime","title":"Transform runtime","text":"<p>Transform runtime is responsible for creation cache actors and sending their  handles to the transforms themselves Additionally it writes created word counts to the data storage (as .csv files) and enhances statistics information with the information about cache size and utilization</p>"},{"location":"transforms/universal/profiler/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>In addition to the configuration parameters, defined here Ray version adds the following parameters:</p> <ul> <li>aggregator_cpu - specifies an amount of CPUs per aggregator actor</li> <li>num_aggregators - specifies number of aggregator actors</li> </ul>"},{"location":"transforms/universal/profiler/ray/#running","title":"Running","text":""},{"location":"transforms/universal/profiler/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <p><pre><code>  --profiler_aggregator_cpu PROFILER_AGGREGATOR_CPU\n                        number of CPUs per aggrigator\n  --profiler_num_aggregators PROFILER_NUM_AGGREGATORS\n                        number of agregator actors to use\n  --profiler_doc_column PROFILER_DOC_COLUMN\n                        key for accessing data\n ```\n\nThese correspond to the configuration keys described above.\n\n### Running the samples\nTo run the samples, use the following `make` targets\n\n* `run-cli-sample` - runs src/ededup_transform_ray.py using command line args\n* `run-local-sample` - runs src/ededup_local_ray.py\n* `run-s3-sample` - runs src/ededup_s3_ray.py\n    * Requires prior installation of minio, depending on your platform (e.g., from [here](https://min.io/docs/minio/macos/index.html)\n     and [here](https://min.io/docs/minio/linux/index.html) \n     and invocation of `make minio-start` to load data into local minio for S3 access.\n\nThese targets will activate the virtual environment and set up any configuration needed.\nUse the `-n` option of `make` to see the detail of what is done to run the sample.\n\nFor example, \n```shell\nmake run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/profiler/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/profiler/spark/","title":"Profiler Transform","text":"<p>Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image for the transformer in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/profiler/spark/#summary","title":"Summary","text":"<p>This project wraps the profiler transform with a Spark runtime.</p>"},{"location":"transforms/universal/profiler/spark/#transform-runtime","title":"Transform runtime","text":"<p>Transform runtime is responsible for creation cache actors and sending their handles to the transforms themselves Additionally it writes created word counts to the data storage (as .csv files) and enhances statistics information with the information about cache size and utilization</p>"},{"location":"transforms/universal/profiler/spark/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Spark version uses the same configuration parameters as the Python one</p>"},{"location":"transforms/universal/profiler/spark/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e. TransformLauncher), in addition to command line arguments provided by the launcher. the same arguments are available as for the python one</p>"},{"location":"transforms/universal/profiler/spark/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/ededup_transform_ray.py using command line args</li> <li><code>run-local-sample</code> - runs src/ededup_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/ededup_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here   and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example, <pre><code>make run-cli-sample\n...\n</code></pre> Then <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/profiler/spark/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/resize/","title":"Resize","text":"<p>The resize transforms allows to change the sizes of input files (both split larger ones and combine smaller) Per the set of transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation   implementation.</li> <li>ray - enables the running of the base python transformation   in a Ray runtime</li> <li>kfp - enables running the ray docker image   in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/resize/kfp_ray/","title":"Document Identification Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/resize/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/resize/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/resize/python/","title":"Resize  files","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/resize/python/#summary","title":"Summary","text":"<p>This is a simple transformer that is resizing the input tables to a specified size.  * resizing based on in-memory size of the tables. * resized based on the number of rows in the tables. </p> <p>Tables can be either split into smaller sizes or aggregated into larger sizes.</p>"},{"location":"transforms/universal/resize/python/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/resize/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding BlockListTransform configuration for values are as follows:</p> <ul> <li>max_rows_per_table - specifies max documents per table</li> <li>max_mbytes_per_table - specifies max size of table, according to the _size_type value.</li> <li>size_type - indicates how table size is measured. Can be one of<ul> <li>memory - table size is measure by the in-process memory used by the table</li> <li>disk - table size is estimated as the on-disk size of the parquet files.  This is an estimate only     as files are generally compressed on disk and so may not be exact due varying compression ratios.     This is the default.</li> </ul> </li> </ul> <p>Only one of the max_rows_per_table and max_mbytes_per_table may be used.</p>"},{"location":"transforms/universal/resize/python/#running","title":"Running","text":""},{"location":"transforms/universal/resize/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by the launcher and map to the configuration keys above.</p> <pre><code>  --resize_max_rows_per_table RESIZE_MAX_ROWS_PER_TABLE\n                        Max number of rows per table\n  --resize_max_mbytes_per_table RESIZE_MAX_MBYTES_PER_TABLE\n                        Max table size (MB). Size is measured according to the --resize_size_type parameter\n  --resize_size_type {disk,memory}\n                        Determines how memory is measured when using the --resize_max_mbytes_per_table option.\n                        'memory' measures the in-process memory footprint and \n                        'disk' makes an estimate of the resulting parquet file size.\n</code></pre>"},{"location":"transforms/universal/resize/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/resize/ray/","title":"Resize  files","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/resize/ray/#summary","title":"Summary","text":"<p>This is a simple transformer that is resizing the input tables to a specified size.  * resizing based on in-memory size of the tables. * resized based on the number of rows in the tables. </p>"},{"location":"transforms/universal/resize/ray/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/resize/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding BlockListTransform configuration for values are as follows:</p> <ul> <li>max_rows_per_table - specifies max documents per table</li> <li>max_mbytes_per_table - specifies max size of table, according to the _size_type value.</li> <li>size_type - indicates how table size is measured. Can be one of<ul> <li>memory - table size is measure by the in-process memory used by the table</li> <li>disk - table size is estimated as the on-disk size of the parquet files.  This is an estimate only     as files are generally compressed on disk and so may not be exact due varying compression ratios.     This is the default.</li> </ul> </li> </ul> <p>Only one of the max_rows_per_table and max_mbytes_per_table may be used.</p>"},{"location":"transforms/universal/resize/ray/#running","title":"Running","text":"<p>We also provide several demos of the transform usage for different data storage options, including local file system and s3.</p>"},{"location":"transforms/universal/resize/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by the launcher and map to the configuration keys above.</p> <pre><code>  --resize_max_rows_per_table RESIZE_MAX_ROWS_PER_TABLE\n                        Max number of rows per table\n  --resize_max_mbytes_per_table RESIZE_MAX_MBYTES_PER_TABLE\n                        Max table size (MB). Size is measured according to the --resize_size_type parameter\n  --resize_size_type {disk,memory}\n                        Determines how memory is measured when using the --resize_max_mbytes_per_table option.\n                        'memory' measures the in-process memory footprint and \n                        'disk' makes an estimate of the resulting parquet file size.\n</code></pre>"},{"location":"transforms/universal/resize/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/resize/spark/","title":"Resize  files","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/resize/spark/#summary","title":"Summary","text":"<p>This is a simple transformer that is resizing the input tables to a specified size.  * resizing based on in-memory size of the tables. * resized based on the number of rows in the tables. </p>"},{"location":"transforms/universal/resize/spark/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/resize/spark/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding BlockListTransform configuration for values are as follows:</p> <ul> <li>max_rows_per_table - specifies max documents per table</li> <li>max_mbytes_per_table - specifies max size of table, according to the _size_type value.</li> <li>size_type - indicates how table size is measured. Can be one of<ul> <li>memory - table size is measure by the in-process memory used by the table</li> <li>disk - table size is estimated as the on-disk size of the parquet files.  This is an estimate only     as files are generally compressed on disk and so may not be exact due varying compression ratios.     This is the default.</li> </ul> </li> </ul> <p>Only one of the max_rows_per_table and max_mbytes_per_table may be used.</p>"},{"location":"transforms/universal/resize/spark/#running","title":"Running","text":"<p>We also provide several demos of the transform usage for different data storage options, including local file system and s3.</p>"},{"location":"transforms/universal/resize/spark/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by the launcher and map to the configuration keys above.</p> <pre><code>  --resize_max_rows_per_table RESIZE_MAX_ROWS_PER_TABLE\n                        Max number of rows per table\n  --resize_max_mbytes_per_table RESIZE_MAX_MBYTES_PER_TABLE\n                        Max table size (MB). Size is measured according to the --resize_size_type parameter\n  --resize_size_type {disk,memory}\n                        Determines how memory is measured when using the --resize_max_mbytes_per_table option.\n                        'memory' measures the in-process memory footprint and \n                        'disk' makes an estimate of the resulting parquet file size.\n</code></pre>"},{"location":"transforms/universal/resize/spark/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/tokenization/","title":"Tokenization Transform","text":"<p>The tokenization transform annotates pyarrow tables and parquet files to add a column containing tokens for the document column.  Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the core python-based transformation  implementation.</li> <li>ray - enables the running of the python-based transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  the transform in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/universal/tokenization/kfp_ray/","title":"Tokenization Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/tokenization/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/tokenization/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/tokenization/python/","title":"Index","text":"<p> Distributed tokenization module for data sets using any Hugging Face compatible tokenizer.      </p>"},{"location":"transforms/universal/tokenization/python/#table-of-contents","title":"\ud83d\udcdd Table of Contents","text":"<ul> <li>Summary</li> <li>Running</li> <li>CLI Options</li> </ul>"},{"location":"transforms/universal/tokenization/python/#data-tokenization","title":"Data Tokenization","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/tokenization/python/#summary","title":"Summary","text":"<p>The data tokenization transform operates by converting a (non-empty) input table into an output table  using a pre-trained tokenizer. The input table is required to have a minimum of two columns,  named <code>document_id</code> and <code>contents</code> by default. However, alternate column names can be specified using  <code>--tkn_doc_id_column</code> for the document id and <code>--tkn_doc_content_column</code> for the document contents. It is essential for the values within the <code>document_id</code> column to be unique across the dataset,  while the <code>contents</code> column stores their respective document content. To execute example demonstrations within this directory,  a machine with <code>64GiB</code> of RAM is recommended.</p> <p>To specify a pre-trained tokenizer, utilize the <code>--tkn_tokenizer</code> parameter.  This parameter accepts the name of a tokenizer ready for download from Hugging Face,  such as <code>hf-internal-testing/llama-tokenizer, bigcode/starcoder</code>, or any other tokenizer compatible  with the Hugging Face AutoTokenizer library. Additionally, you can employ the <code>--tkn_tokenizer_args</code> parameter  to include extra arguments specific to the chosen tokenizer.  For instance, when loading a Hugging Face tokenizer like <code>bigcode/starcoder</code>, which necessitate an access token,  you can specify <code>use_auth_token=&lt;your token&gt;</code> in <code>--tkn_tokenizer</code>. </p> <p>The tokenization transformer utilizes the specified tokenizer to tokenize each row,  assuming each row represents a document, in the input table and save it to a corresponding row in the output table.  The output table generally consists of four columns: <code>tokens, document_id, document_length</code>, and <code>token_count</code>.</p> <p>The <code>tokens</code> stores the sequence of token IDs generated by the tokenizer during the document tokenization process.  The <code>document_id</code> (or the designated name specified in <code>--tkn_doc_id_column</code>) contains the document ID,  while <code>document_length</code> and <code>token_count</code> respectively record the length of the document and the total count of generated tokens. During tokenization, the tokenizer will disregard empty documents (rows) in the input table,  as well as documents that yield no tokens or encounter failure during tokenization.  The count of such documents will be stored in the <code>num_empty_rows</code> field of the <code>metadata</code> file.</p> <p>In certain cases, the tokenization process of some tokenizers may be sluggish,  particularly when handling lengthy documents containing millions of characters.  To address this, you can employ the <code>--tkn_chunk_size</code> parameter to define the length of chunks to tokenize at a given time. For English text (<code>en</code>), it is recommended to set the chunk size to <code>20,000</code>, roughly equivalent to <code>15</code> pages of text.  The tokenizer will then tokenize each chunk separately and combine their resulting token IDs. By default, the value of <code>--tkn_chunk_size</code> is <code>0</code>, indicating that each document is tokenized as a whole, regardless of its length.</p>"},{"location":"transforms/universal/tokenization/python/#running","title":"Running","text":""},{"location":"transforms/universal/tokenization/python/#cli-options","title":"CLI Options","text":"<p>The following command line arguments are available in addition to  the options provided by the python launcher and the python launcher. <pre><code>  --tkn_tokenizer TKN_TOKENIZER\n                        Tokenizer used for tokenization. It also can be a path to a pre-trained tokenizer. By defaut, `hf-internal-testing/llama-tokenizer` from HuggingFace is used\n  --tkn_tokenizer_args TKN_TOKENIZER_ARGS\n                        Arguments for tokenizer. For example, `cache_dir=/tmp/hf,use_auth_token=Your_HF_authentication_token` could be arguments for tokenizer `bigcode/starcoder` from HuggingFace\n  --tkn_doc_id_column TKN_DOC_ID_COLUMN\n                        Column contains document id which values should be unique across dataset\n  --tkn_doc_content_column TKN_DOC_CONTENT_COLUMN\n                        Column contains document content\n  --tkn_text_lang TKN_TEXT_LANG\n                        Specify language used in the text content for better text splitting if needed\n  --tkn_chunk_size TKN_CHUNK_SIZE\n                        Specify &gt;0 value to tokenize each row/doc in chunks of characters (rounded in words)\n</code></pre></p>"},{"location":"transforms/universal/tokenization/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/tokenization_transform_python.py using command line args</li> <li><code>run-local-sample</code> - runs src/tokenization_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/tokenization/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/tokenization/ray/","title":"Tokenization Transform for Ray","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/tokenization/ray/#summary","title":"Summary","text":"<p>This project wraps the tokenization transform with a Ray runtime.</p>"},{"location":"transforms/universal/tokenization/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Noop configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/tokenization/ray/#running","title":"Running","text":""},{"location":"transforms/universal/tokenization/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  ray launcher are available.</p>"},{"location":"transforms/universal/tokenization/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/tokenization_transform_ray.py using command line args</li> <li><code>run-local-sample</code> - runs src/tokenization_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/filter_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/tokenization/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"}]}