{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text from PDF and convert them into parquet files\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Exact Dedup**: Chunks with exact same content are filtered out\n",
    "- **Doc_ID generation**: Each chunk is assigned a uniq id, based on content and hash\n",
    "- **Fuzzy Dedup**: Eliminate chunks that are 'very similar' content\n",
    "- **Doc quality**: Scores the documents based on criteria like number of words, if it contains bad words ..etc\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "## Configuration\n",
    "class MyConfig:\n",
    "    pass \n",
    "\n",
    "MY_CONFIG = MyConfig ()\n",
    "\n",
    "## Input Data - configure this to the folder we want to process\n",
    "MY_CONFIG.INPUT_DATA_DIR = \"input\"\n",
    "MY_CONFIG.OUTPUT_FOLDER = \"output\"\n",
    "MY_CONFIG.OUTPUT_FOLDER_FINAL = os.path.join(MY_CONFIG.OUTPUT_FOLDER , \"output_final\")\n",
    "\n",
    "## Embedding model\n",
    "MY_CONFIG.EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "## RAY CONFIGURATION\n",
    "num_cpus_available =  os.cpu_count()\n",
    "# print (num_cpus_available)\n",
    "# MY_CONFIG.RAY_NUM_CPUS = num_cpus_available // 2  ## use half the available cores for processing\n",
    "MY_CONFIG.RAY_NUM_CPUS =  1\n",
    "# print (MY_CONFIG.RAY_NUM_CPUS)\n",
    "MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
    "# MY_CONFIG.RAY_RUNTIME_WORKERS = num_cpus_available // 3\n",
    "MY_CONFIG.RAY_RUNTIME_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc3f0e",
   "metadata": {},
   "source": [
    "### Download Data\n",
    "\n",
    "We will use [Walmart annual report PDFs](https://github.com/sujee/data/tree/main/data-prep-kit/walmart-reports-1) as our input data.\n",
    "\n",
    "Feel free to substitute your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c1ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "from utils import download_file\n",
    "\n",
    "## Download the data files\n",
    "shutil.os.makedirs(MY_CONFIG.INPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart-10K-Reports-Optimized_2023.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart-10K-Reports-Optimized_2023.pdf' ))\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart_2024.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart_2024.pdf' ))\n",
    "\n",
    "download_file (url = 'https://raw.githubusercontent.com/sujee/data/main/data-prep-kit/walmart-reports-1/Walmart_2024.pdf', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'Walmart_2024_copy.pdf' ))  # create a dupe file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Main repo root\n",
    "from utils import rootdir\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "STAGE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "<a id=\"pdf2parquet\"></a>\n",
    "\n",
    "## Step-2: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE = 1  ## DEBUG\n",
    "\n",
    "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_parquet_out\")\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS, \"memory\": MY_CONFIG.RAY_MEMORY_GB * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
    "# launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "<a id=\"chunking\"></a>\n",
    "\n",
    "##  Step-3: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96198fa6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE = 2  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_chunk_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_ray import DocChunkRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocChunkRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-4: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE  = 3  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_ededupe_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1c3c3",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d824ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## Step-5:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "STAGE  += 1\n",
    "# STAGE  = 4  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_doc_id_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d492c2b",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ade826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-6: Fuzzy Dedup\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "STAGE  += 1\n",
    "# STAGE  = 5  ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_fdedupe_out\")\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8cd11",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## Step-7: Document Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE  = 6 ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_doc_quality_out\")\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from doc_quality_transform import (\n",
    "    text_lang_cli_param,\n",
    "    doc_content_column_cli_param,\n",
    "    bad_word_filepath_cli_param,\n",
    ")\n",
    "from doc_quality_transform_ray import DocQualityRayTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_quality_basedir = os.path.join(rootdir, \"transforms\", \"language\", \"doc_quality\", \"python\")\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    # doc quality configuration\n",
    "    text_lang_cli_param: \"en\",\n",
    "    doc_content_column_cli_param: \"contents\",\n",
    "    bad_word_filepath_cli_param: os.path.join(doc_quality_basedir, \"ldnoobw\", \"en\"),\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocQualityRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7d855",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-8:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGE  += 1\n",
    "# STAGE  = 7 ## DEBUG\n",
    "\n",
    "input_folder = output_folder # previous output folder is the input folder for the current stage\n",
    "output_folder =  os.path.join(MY_CONFIG.OUTPUT_FOLDER, f\"{STAGE:02}_encoder_out\")\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_ray import TextEncoderRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TextEncoderRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-9: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"‚úÖ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
