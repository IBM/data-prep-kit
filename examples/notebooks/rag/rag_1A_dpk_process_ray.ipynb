{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Data Processing for RAG with Data Prep Kit (RAY)</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15976e3",
   "metadata": {},
   "source": [
    "## Before Running the notebook\n",
    "\n",
    "Please complete [setting up python dev environment](./setup-python-dev-env.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook will process PDF documents as part of RAG pipeline\n",
    "\n",
    "![](media/rag-overview-2.png)\n",
    "\n",
    "This notebook will perform steps 1, 2 and 3 in RAG pipeline.\n",
    "\n",
    "Here are the processing steps:\n",
    "\n",
    "- **pdf2parquet** : Extract text from PDF and convert them into parquet files\n",
    "- **Chunk documents**: Split the PDFs into 'meaningful sections' (paragraphs, sentences ..etc)\n",
    "- **Doc_ID generation**: Each chunk is assigned a uniq id, based on content and hash\n",
    "- **Exact Dedup**: Chunks with exact same content are filtered out\n",
    "- **Fuzzy Dedup**: Eliminate chunks that are 'very similar' content\n",
    "- **Doc quality**: Scores the documents based on criteria like number of words, if it contains bad words ..etc\n",
    "- **Text encoder**: Convert chunks into vectors using embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b10be1",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33345487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from my_config import MY_CONFIG\n",
    "\n",
    "## RAY CONFIGURATION\n",
    "num_cpus_available =  os.cpu_count()\n",
    "# print (num_cpus_available)\n",
    "# MY_CONFIG.RAY_NUM_CPUS = num_cpus_available // 2  ## use half the available cores for processing\n",
    "MY_CONFIG.RAY_NUM_CPUS =  1\n",
    "# print (MY_CONFIG.RAY_NUM_CPUS)\n",
    "MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
    "# MY_CONFIG.RAY_RUNTIME_WORKERS = num_cpus_available // 3\n",
    "MY_CONFIG.RAY_RUNTIME_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c58856",
   "metadata": {},
   "source": [
    "## Step-2:  Data\n",
    "\n",
    "We will use white papers  about LLMs.  \n",
    "\n",
    "- [Granite Code Models](https://arxiv.org/abs/2405.04324)\n",
    "- [Attention is all you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "You can of course substite your own data below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce5939",
   "metadata": {},
   "source": [
    "### 2.1 - Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfde6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input/attension.pdf (2.22 MB) downloaded successfully.\n",
      "\n",
      "input/granite.pdf (1.27 MB) downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "from utils import download_file\n",
    "\n",
    "## Download the data files\n",
    "shutil.os.makedirs(MY_CONFIG.INPUT_DATA_DIR, exist_ok=True)\n",
    "\n",
    "download_file (url = 'https://arxiv.org/pdf/1706.03762', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'attension.pdf' ))\n",
    "\n",
    "download_file (url = 'https://arxiv.org/pdf/2405.04324', local_file = os.path.join(MY_CONFIG.INPUT_DATA_DIR, 'granite.pdf' ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### 2.2  - Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared output directory\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
    "    raise Exception (f\"‚ùå Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
    "\n",
    "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
    "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_chunk_out')\n",
    "output_docid_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_docid_out')\n",
    "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_exact_dedupe_out')\n",
    "output_fuzzy_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '05_fuzzy_dedupe_out')\n",
    "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '06_embeddings_out')\n",
    "\n",
    "\n",
    "## clear output folder\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
    "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "print (\"‚úÖ Cleared output directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## Step-3: pdf2parquet -  Convert data from PDF to Parquet\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### 3.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-1: Processing input='input' --> output='output/01_parquet_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 1 \n",
    "\n",
    "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
    "output_folder =  output_parquet_dir\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### 3.2 -  Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:44:08 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'double_precision': 8}\n",
      "15:44:08 INFO - pipeline id pipeline_id\n",
      "15:44:08 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "15:44:08 INFO - number of workers 2 worker options {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1}\n",
      "15:44:08 INFO - actor creation delay 0\n",
      "15:44:08 INFO - job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:44:08 INFO - data factory data_ is using local data access: input_folder - input output_folder - output/01_parquet_out\n",
      "15:44:08 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:44:08 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "15:44:08 INFO - Running locally\n",
      "2024-10-08 15:44:15,462\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:44:19 INFO - orchestrator started at 2024-10-08 15:44:19\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:44:19 INFO - Number of files is 2, source profile {'max_file_size': 2.112621307373047, 'min_file_size': 1.2146415710449219, 'total_file_size': 3.3272628784179688}\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:44:19 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 14.478764344006777, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:44:19 INFO - Number of workers - 2 with {'num_cpus': 1, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(RayTransformFileProcessor pid=59105)\u001b[0m 15:44:22 INFO - Initializing models\n",
      "Fetching 10 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 171897.70it/s]\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:45:01 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=59104)\u001b[0m 15:44:22 INFO - Initializing models\n",
      "Fetching 10 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 179243.76it/s]\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:46:23 INFO - Completed processing 2 files in 1.359 min\n",
      "\u001b[36m(orchestrate pid=59074)\u001b[0m 15:46:23 INFO - done flushing in 0.002 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=59105)\u001b[0m /opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=59105)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "15:46:33 INFO - Completed execution in 2.412 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=59104)\u001b[0m /opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=59104)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:1 completed successfully\n",
      "CPU times: user 2.18 s, sys: 1.02 s, total: 3.21 s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS, \"memory\": MY_CONFIG.RAY_MEMORY_GB * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca790e0",
   "metadata": {},
   "source": [
    "### 3.3 - Inspect Generated output\n",
    "\n",
    "Here we should see one entry per input file processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe59563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dimensions (rows x columns)=  (2, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>contents</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_id</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>{\"_name\":\"\",\"type\":\"pdf-document\",\"description...</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>89c788c3-8f54-4809-8177-d90721f28587</td>\n",
       "      <td>pdf</td>\n",
       "      <td>8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...</td>\n",
       "      <td>135809</td>\n",
       "      <td>2024-10-08T15:45:38.327708</td>\n",
       "      <td>36.473885</td>\n",
       "      <td>attension.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                           contents  \\\n",
       "0    granite.pdf  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...   \n",
       "1  attension.pdf  {\"_name\":\"\",\"type\":\"pdf-document\",\"description...   \n",
       "\n",
       "   num_pages  num_tables  num_doc_elements  \\\n",
       "0         28          17               348   \n",
       "1         15           4               193   \n",
       "\n",
       "                            document_id  ext  \\\n",
       "0  b5367177-e1c1-4643-9281-942f41bc5c87  pdf   \n",
       "1  89c788c3-8f54-4809-8177-d90721f28587  pdf   \n",
       "\n",
       "                                                hash    size  \\\n",
       "0  4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "1  8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...  135809   \n",
       "\n",
       "                date_acquired  pdf_convert_time source_filename  \n",
       "0  2024-10-08T15:46:23.368528         81.505457     granite.pdf  \n",
       "1  2024-10-08T15:45:38.327708         36.473885   attension.pdf  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(5)\n",
    "\n",
    "## To display certain columns\n",
    "#parquet_df[['column1', 'column2', 'column3']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72274586",
   "metadata": {},
   "source": [
    "##  Step-4: Doc chunks\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96198fa6",
   "metadata": {},
   "source": [
    "### 4.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305f00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-2: Processing input='output/01_parquet_out' --> output='output/02_chunk_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE = 2\n",
    "\n",
    "input_folder = output_parquet_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_chunk_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f2cd1",
   "metadata": {},
   "source": [
    "### 4.2 -  Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b7b18d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:46:51 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'doc_id_column_name': 'document_id', 'dl_min_chunk_len': None, 'output_chunk_column_name': 'contents', 'output_source_doc_id_column_name': 'source_document_id', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox', 'chunk_size_tokens': 128, 'chunk_overlap_tokens': 30}\n",
      "15:46:51 INFO - pipeline id pipeline_id\n",
      "15:46:51 INFO - code location None\n",
      "15:46:51 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "15:46:51 INFO - actor creation delay 0\n",
      "15:46:51 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:46:51 INFO - data factory data_ is using local data access: input_folder - output/01_parquet_out output_folder - output/02_chunk_out\n",
      "15:46:51 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:46:51 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "15:46:51 INFO - Running locally\n",
      "2024-10-08 15:46:53,970\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:46:56 INFO - orchestrator started at 2024-10-08 15:46:56\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:46:56 INFO - Number of files is 2, source profile {'max_file_size': 0.12769794464111328, 'min_file_size': 0.03536033630371094, 'total_file_size': 0.16305828094482422}\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:46:56 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.517623901367188, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:46:56 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:46:59 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:47:00 INFO - Completed processing 2 files in 0.003 min\n",
      "\u001b[36m(orchestrate pid=59954)\u001b[0m 15:47:00 INFO - done flushing in 0.001 sec\n",
      "15:47:10 INFO - Completed execution in 0.305 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:2 completed successfully\n",
      "CPU times: user 1.24 s, sys: 480 ms, total: 1.72 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Import doc_json_chunk transform configuration\n",
    "from doc_chunk_transform_ray import DocChunkRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc_chunk arguments\n",
    "    # ...\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocChunkRayTransformConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213afdf6",
   "metadata": {},
   "source": [
    "### 4.3 - Inspect Generated output\n",
    "\n",
    "We would see documents are split into many chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8138d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed : 2\n",
      "Chunks created : 211\n",
      "Input data dimensions (rows x columns)=  (2, 12)\n",
      "Output data dimensions (rows x columns)=  (211, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>pdf</td>\n",
       "      <td>8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...</td>\n",
       "      <td>135809</td>\n",
       "      <td>2024-10-08T15:45:38.327708</td>\n",
       "      <td>36.473885</td>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>89c788c3-8f54-4809-8177-d90721f28587</td>\n",
       "      <td>Attention Is All You Need\\nAidan N. Gomez ‚àó ‚Ä† ...</td>\n",
       "      <td>$.main-text[8]</td>\n",
       "      <td>1</td>\n",
       "      <td>[234.82455444, 475.27728271, 339.99435425, 508...</td>\n",
       "      <td>3bdf2978bed016a6996cf37ffdd1fb96248cac96fccf70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>pdf</td>\n",
       "      <td>8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...</td>\n",
       "      <td>135809</td>\n",
       "      <td>2024-10-08T15:45:38.327708</td>\n",
       "      <td>36.473885</td>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>89c788c3-8f54-4809-8177-d90721f28587</td>\n",
       "      <td>2 Background\\nEnd-to-end memory networks are b...</td>\n",
       "      <td>$.main-text[25]</td>\n",
       "      <td>2</td>\n",
       "      <td>[107.37045288, 218.29309082, 505.6534729, 249....</td>\n",
       "      <td>aec7ea8282cbc4a7d6410e8d29df05025e75c084bf5098...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>2 Data Collection\\nIn this section, we describ...</td>\n",
       "      <td>$.main-text[39]</td>\n",
       "      <td>4</td>\n",
       "      <td>[107.18983459, 582.13323975, 504.2984314, 625....</td>\n",
       "      <td>c99319052c80981fe2c47fef9fc6110348b53f6faf3389...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename  num_pages  num_tables  num_doc_elements  ext  \\\n",
       "128  attension.pdf         15           4               193  pdf   \n",
       "138  attension.pdf         15           4               193  pdf   \n",
       "17     granite.pdf         28          17               348  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "128  8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...  135809   \n",
       "138  8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...  135809   \n",
       "17   4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "\n",
       "                  date_acquired  pdf_convert_time source_filename  \\\n",
       "128  2024-10-08T15:45:38.327708         36.473885   attension.pdf   \n",
       "138  2024-10-08T15:45:38.327708         36.473885   attension.pdf   \n",
       "17   2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "\n",
       "                       source_document_id  \\\n",
       "128  89c788c3-8f54-4809-8177-d90721f28587   \n",
       "138  89c788c3-8f54-4809-8177-d90721f28587   \n",
       "17   b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "\n",
       "                                              contents     doc_jsonpath  \\\n",
       "128  Attention Is All You Need\\nAidan N. Gomez ‚àó ‚Ä† ...   $.main-text[8]   \n",
       "138  2 Background\\nEnd-to-end memory networks are b...  $.main-text[25]   \n",
       "17   2 Data Collection\\nIn this section, we describ...  $.main-text[39]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "128            1  [234.82455444, 475.27728271, 339.99435425, 508...   \n",
       "138            2  [107.37045288, 218.29309082, 505.6534729, 249....   \n",
       "17             4  [107.18983459, 582.13323975, 504.2984314, 625....   \n",
       "\n",
       "                                           document_id  \n",
       "128  3bdf2978bed016a6996cf37ffdd1fb96248cac96fccf70...  \n",
       "138  aec7ea8282cbc4a7d6410e8d29df05025e75c084bf5098...  \n",
       "17   c99319052c80981fe2c47fef9fc6110348b53f6faf3389...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
    "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8894d88",
   "metadata": {},
   "source": [
    "## Step-5:  DOC ID generation\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e88f76",
   "metadata": {},
   "source": [
    "### 5.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7debd243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-3: Processing input='output/02_chunk_out' --> output='output/03_docid_out'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STAGE  = 3\n",
    "\n",
    "input_folder = output_chunk_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_docid_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cadc2f3",
   "metadata": {},
   "source": [
    "### 5.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b0eade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:11 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'chunk_hash', 'int_column': 'chunk_id', 'start_id': 0}\n",
      "15:47:11 INFO - pipeline id pipeline_id\n",
      "15:47:11 INFO - code location None\n",
      "15:47:11 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "15:47:11 INFO - actor creation delay 0\n",
      "15:47:11 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:47:11 INFO - data factory data_ is using local data access: input_folder - output/02_chunk_out output_folder - output/03_docid_out\n",
      "15:47:11 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:47:11 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "15:47:11 INFO - Running locally\n",
      "2024-10-08 15:47:13,710\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:14 INFO - orchestrator started at 2024-10-08 15:47:14\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:14 INFO - Number of files is 2, source profile {'max_file_size': 0.0639944076538086, 'min_file_size': 0.028045654296875, 'total_file_size': 0.0920400619506836}\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:14 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.240727234631777, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:14 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:16 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:16 INFO - Completed processing 2 files in 0.004 min\n",
      "\u001b[36m(orchestrate pid=60017)\u001b[0m 15:47:16 INFO - done flushing in 0.001 sec\n",
      "15:47:26 INFO - Completed execution in 0.244 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:3 completed successfully\n",
      "CPU times: user 70.9 ms, sys: 118 ms, total: 189 ms\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformRuntimeConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"chunk_hash\",\n",
    "    \"doc_id_int_column\": \"chunk_id\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformRuntimeConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5c6e4",
   "metadata": {},
   "source": [
    "### 5.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d941b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (211, 16)\n",
      "Output data dimensions (rows x columns)=  (211, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>pdf</td>\n",
       "      <td>8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...</td>\n",
       "      <td>135809</td>\n",
       "      <td>2024-10-08T15:45:38.327708</td>\n",
       "      <td>36.473885</td>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>89c788c3-8f54-4809-8177-d90721f28587</td>\n",
       "      <td>6.3 English Constituency Parsing\\nVinyals &amp; Ka...</td>\n",
       "      <td>$.tables[3]</td>\n",
       "      <td>10</td>\n",
       "      <td>[144.17385864, 555.05541992, 467.28027344, 699...</td>\n",
       "      <td>2daf778770c8a3d86ef693fc90ab3505d1856ec4ad486e...</td>\n",
       "      <td>2daf778770c8a3d86ef693fc90ab3505d1856ec4ad486e...</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>6.1.5 RepoBench, CrossCodeEval: Repository-Lev...</td>\n",
       "      <td>$.main-text[163]</td>\n",
       "      <td>13</td>\n",
       "      <td>[107.2381134, 160.20976257, 505.74502563, 236....</td>\n",
       "      <td>9e47ead24fc356ae5aae90637990bcafb81064d2367e1a...</td>\n",
       "      <td>9e47ead24fc356ae5aae90637990bcafb81064d2367e1a...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>5 Instruction Tuning\\nFinetuning code LLMs on ...</td>\n",
       "      <td>$.main-text[91]</td>\n",
       "      <td>7</td>\n",
       "      <td>[107.37421417, 126.16381836, 504.37268066, 214...</td>\n",
       "      <td>39f55cbc4100ba434b64586c17c9ab8d8841131c9214da...</td>\n",
       "      <td>39f55cbc4100ba434b64586c17c9ab8d8841131c9214da...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename  num_pages  num_tables  num_doc_elements  ext  \\\n",
       "198  attension.pdf         15           4               193  pdf   \n",
       "74     granite.pdf         28          17               348  pdf   \n",
       "41     granite.pdf         28          17               348  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "198  8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...  135809   \n",
       "74   4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "41   4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "\n",
       "                  date_acquired  pdf_convert_time source_filename  \\\n",
       "198  2024-10-08T15:45:38.327708         36.473885   attension.pdf   \n",
       "74   2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "41   2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "\n",
       "                       source_document_id  \\\n",
       "198  89c788c3-8f54-4809-8177-d90721f28587   \n",
       "74   b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "41   b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "198  6.3 English Constituency Parsing\\nVinyals & Ka...       $.tables[3]   \n",
       "74   6.1.5 RepoBench, CrossCodeEval: Repository-Lev...  $.main-text[163]   \n",
       "41   5 Instruction Tuning\\nFinetuning code LLMs on ...   $.main-text[91]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "198           10  [144.17385864, 555.05541992, 467.28027344, 699...   \n",
       "74            13  [107.2381134, 160.20976257, 505.74502563, 236....   \n",
       "41             7  [107.37421417, 126.16381836, 504.37268066, 214...   \n",
       "\n",
       "                                           document_id  \\\n",
       "198  2daf778770c8a3d86ef693fc90ab3505d1856ec4ad486e...   \n",
       "74   9e47ead24fc356ae5aae90637990bcafb81064d2367e1a...   \n",
       "41   39f55cbc4100ba434b64586c17c9ab8d8841131c9214da...   \n",
       "\n",
       "                                            chunk_hash  chunk_id  \n",
       "198  2daf778770c8a3d86ef693fc90ab3505d1856ec4ad486e...       198  \n",
       "74   9e47ead24fc356ae5aae90637990bcafb81064d2367e1a...        74  \n",
       "41   39f55cbc4100ba434b64586c17c9ab8d8841131c9214da...        41  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "## Step-6: Exact Dedup\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### 6.1 -  Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c7a1b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-4: Processing input='output/03_docid_out' --> output='output/04_exact_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 4\n",
    "\n",
    "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_exact_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### 6.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:27 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'chunk_hash', 'use_snapshot': False, 'snapshot_directory': None, 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "15:47:27 INFO - pipeline id pipeline_id\n",
      "15:47:27 INFO - code location None\n",
      "15:47:27 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "15:47:27 INFO - actor creation delay 0\n",
      "15:47:27 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:47:27 INFO - data factory data_ is using local data access: input_folder - output/03_docid_out output_folder - output/04_exact_dedupe_out\n",
      "15:47:27 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:47:27 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "15:47:27 INFO - Running locally\n",
      "2024-10-08 15:47:29,753\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:30 INFO - orchestrator started at 2024-10-08 15:47:30\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:30 INFO - Number of files is 2, source profile {'max_file_size': 0.06943893432617188, 'min_file_size': 0.03227996826171875, 'total_file_size': 0.10171890258789062}\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:30 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.185150146484375, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:30 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:31 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:32 INFO - Completed processing 2 files in 0.004 min\n",
      "\u001b[36m(orchestrate pid=60090)\u001b[0m 15:47:32 INFO - done flushing in 0.001 sec\n",
      "15:47:42 INFO - Completed execution in 0.242 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:4 completed successfully\n",
      "CPU times: user 67 ms, sys: 122 ms, total: 189 ms\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformRuntimeConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "    \"ededup_doc_id_column\": \"chunk_hash\",\n",
    "    \n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(EdedupRayTransformRuntimeConfiguration())\n",
    "# launch\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1c3c3",
   "metadata": {},
   "source": [
    "### 6.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d824ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (211, 18)\n",
      "Output data dimensions (rows x columns)=  (211, 19)\n",
      "Input chunks before exact dedupe : 211\n",
      "Output chunks after exact dedupe : 211\n",
      "Duplicate chunks removed :   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>3 Model Architecture\\nWe train a series of cod...</td>\n",
       "      <td>$.main-text[56]</td>\n",
       "      <td>5</td>\n",
       "      <td>[106.90156555, 463.37445068, 505.64907837, 507...</td>\n",
       "      <td>0aa927498f188fd1a97c481bee4a5581c197d44f37e5fa...</td>\n",
       "      <td>0aa927498f188fd1a97c481bee4a5581c197d44f37e5fa...</td>\n",
       "      <td>23</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>pdf</td>\n",
       "      <td>8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...</td>\n",
       "      <td>135809</td>\n",
       "      <td>2024-10-08T15:45:38.327708</td>\n",
       "      <td>36.473885</td>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>89c788c3-8f54-4809-8177-d90721f28587</td>\n",
       "      <td>Attention Visualizations Input-Input Layer5\\nF...</td>\n",
       "      <td>$.main-text[181]</td>\n",
       "      <td>13</td>\n",
       "      <td>[107.35285187, 436.88726807, 504.46768188, 479...</td>\n",
       "      <td>0a545be8b488243477a72f2a991f3151d05af088a652a4...</td>\n",
       "      <td>0a545be8b488243477a72f2a991f3151d05af088a652a4...</td>\n",
       "      <td>208</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>pdf</td>\n",
       "      <td>8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...</td>\n",
       "      <td>135809</td>\n",
       "      <td>2024-10-08T15:45:38.327708</td>\n",
       "      <td>36.473885</td>\n",
       "      <td>attension.pdf</td>\n",
       "      <td>89c788c3-8f54-4809-8177-d90721f28587</td>\n",
       "      <td>4 Why Self-Attention\\nAs noted in Table 1, a s...</td>\n",
       "      <td>$.main-text[83]</td>\n",
       "      <td>6</td>\n",
       "      <td>[107.27950287, 69.51050568, 504.59405518, 101....</td>\n",
       "      <td>f3e8a0d1cf72ab4028684f84672386fe4adbad4cdc5d0a...</td>\n",
       "      <td>f3e8a0d1cf72ab4028684f84672386fe4adbad4cdc5d0a...</td>\n",
       "      <td>170</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename  num_pages  num_tables  num_doc_elements  ext  \\\n",
       "23     granite.pdf         28          17               348  pdf   \n",
       "208  attension.pdf         15           4               193  pdf   \n",
       "170  attension.pdf         15           4               193  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "23   4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "208  8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...  135809   \n",
       "170  8b3ff3ebfc838c36fa10e6bbf90c1a5d2781f2d2583883...  135809   \n",
       "\n",
       "                  date_acquired  pdf_convert_time source_filename  \\\n",
       "23   2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "208  2024-10-08T15:45:38.327708         36.473885   attension.pdf   \n",
       "170  2024-10-08T15:45:38.327708         36.473885   attension.pdf   \n",
       "\n",
       "                       source_document_id  \\\n",
       "23   b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "208  89c788c3-8f54-4809-8177-d90721f28587   \n",
       "170  89c788c3-8f54-4809-8177-d90721f28587   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "23   3 Model Architecture\\nWe train a series of cod...   $.main-text[56]   \n",
       "208  Attention Visualizations Input-Input Layer5\\nF...  $.main-text[181]   \n",
       "170  4 Why Self-Attention\\nAs noted in Table 1, a s...   $.main-text[83]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "23             5  [106.90156555, 463.37445068, 505.64907837, 507...   \n",
       "208           13  [107.35285187, 436.88726807, 504.46768188, 479...   \n",
       "170            6  [107.27950287, 69.51050568, 504.59405518, 101....   \n",
       "\n",
       "                                           document_id  \\\n",
       "23   0aa927498f188fd1a97c481bee4a5581c197d44f37e5fa...   \n",
       "208  0a545be8b488243477a72f2a991f3151d05af088a652a4...   \n",
       "170  f3e8a0d1cf72ab4028684f84672386fe4adbad4cdc5d0a...   \n",
       "\n",
       "                                            chunk_hash  chunk_id removed  \n",
       "23   0aa927498f188fd1a97c481bee4a5581c197d44f37e5fa...        23      []  \n",
       "208  0a545be8b488243477a72f2a991f3151d05af088a652a4...       208      []  \n",
       "170  f3e8a0d1cf72ab4028684f84672386fe4adbad4cdc5d0a...       170      []  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
    "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
    "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## Step-7: Fuzzy Dedup\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### 7.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-5: Processing input='output/04_exact_dedupe_out' --> output='output/05_fuzzy_dedupe_out'\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "STAGE  = 5\n",
    "\n",
    "input_folder = output_exact_dedupe_dir # previous output folder is the input folder for the current stage\n",
    "output_folder =  output_fuzzy_dedupe_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### 7.2 - Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:47:43 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'chunk_id', 'cluster_column': 'chunk_hash', 'bucket_cpu': 0.3, 'mhash_cpu': 0.3, 'doc_cpu': 0.3, 'num_doc_actors': 1, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 1, 'num_permutations': 64, 'threshold': 0.7, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 1}}\n",
      "15:47:43 INFO - pipeline id pipeline_id\n",
      "15:47:43 INFO - code location None\n",
      "15:47:43 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "15:47:43 INFO - actor creation delay 0\n",
      "15:47:43 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:47:43 INFO - data factory data_ is using local data access: input_folder - output/04_exact_dedupe_out output_folder - output/05_fuzzy_dedupe_out\n",
      "15:47:43 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:47:43 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "15:47:43 INFO - Running locally\n",
      "2024-10-08 15:47:44,801\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - orchestrator started at 2024-10-08 15:47:46\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - Number of files is 2, source profile {'max_file_size': 0.06979846954345703, 'min_file_size': 0.032639503479003906, 'total_file_size': 0.10243797302246094}\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.11649932898581, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - Fuzzy: num buckets 8, bucket length 8\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:46 INFO - Table preprocessing uses 1 readers\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:47 INFO - created 1 table processor actors\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:48 INFO - Completed 1 files in 0.021 min\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:48 INFO - Completed 1 files (50.0%)  in 0.021 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:50 INFO - Completed processing 2 files in 0.056 min\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:50 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:51 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:51 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:52 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:52 INFO - created 1 document actors\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:53 INFO - created 1 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:53 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:53 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=60169)\u001b[0m 15:47:53 INFO - processing buckets 0 long, 1686 short\n",
      "\u001b[36m(BucketsHash pid=60169)\u001b[0m 15:47:53 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:54 INFO - Done processing buckets in 0.013 min\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:54 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=60215)\u001b[0m 15:47:54 INFO - Waiting bucket processing completion. Submitted requests 17\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:55 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:47:56 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:48:02 INFO - Completed processing 2 files in 0.105 min\n",
      "\u001b[36m(orchestrate pid=60160)\u001b[0m 15:48:02 INFO - done flushing in 0.002 sec\n",
      "15:48:12 INFO - Completed execution in 0.485 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:5 completed successfully\n",
      "CPU times: user 106 ms, sys: 147 ms, total: 253 ms\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"chunk_id\",\n",
    "    \"fdedup_cluster_column\": \"chunk_hash\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.3,\n",
    "    \"fdedup_doc_cpu\": 0.3,\n",
    "    \"fdedup_mhash_cpu\": 0.3,\n",
    "    \"fdedup_num_doc_actors\": 1,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 1,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.7, # between 0.0 to 1.0 ; smaller values tend to be more lenient in finding near dupes; close to 1.0 is more strict\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8cd11",
   "metadata": {},
   "source": [
    "### 7.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e899ad60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (211, 19)\n",
      "Output data dimensions (rows x columns)=  (211, 19)\n",
      "Duplicate chunks removed  by fuzzy-dedupe:   0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>chunk_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>6.5 Math Reasoning\\nTable 15 summarizes the re...</td>\n",
       "      <td>$.main-text[227]</td>\n",
       "      <td>19</td>\n",
       "      <td>[106.84754181, 127.60254669, 504.37875366, 171...</td>\n",
       "      <td>a5543da64e4d56508b18d26addd28131c63921a3a0f7a1...</td>\n",
       "      <td>105</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>6.7 Model Robustness\\nStarCoderBase-3B, Docstr...</td>\n",
       "      <td>$.tables[16]</td>\n",
       "      <td>21</td>\n",
       "      <td>[162.45884705, 471.86456299, 449.44735718, 668...</td>\n",
       "      <td>9d850c36997f8f8ebb4bed26cad319cd84c40a8c294660...</td>\n",
       "      <td>113</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>2.3 HAP, PII, Malware Filtering\\nTo reduce the...</td>\n",
       "      <td>$.main-text[45]</td>\n",
       "      <td>4</td>\n",
       "      <td>[107.02816772, 100.26654053, 505.24621582, 189...</td>\n",
       "      <td>3610c8f2db4587033885befe968d9ccbb40f7d5c97f21c...</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename  num_pages  num_tables  num_doc_elements  ext  \\\n",
       "105  granite.pdf         28          17               348  pdf   \n",
       "113  granite.pdf         28          17               348  pdf   \n",
       "20   granite.pdf         28          17               348  pdf   \n",
       "\n",
       "                                                  hash    size  \\\n",
       "105  4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "113  4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "20   4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "\n",
       "                  date_acquired  pdf_convert_time source_filename  \\\n",
       "105  2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "113  2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "20   2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "\n",
       "                       source_document_id  \\\n",
       "105  b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "113  b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "20   b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "\n",
       "                                              contents      doc_jsonpath  \\\n",
       "105  6.5 Math Reasoning\\nTable 15 summarizes the re...  $.main-text[227]   \n",
       "113  6.7 Model Robustness\\nStarCoderBase-3B, Docstr...      $.tables[16]   \n",
       "20   2.3 HAP, PII, Malware Filtering\\nTo reduce the...   $.main-text[45]   \n",
       "\n",
       "     page_number                                               bbox  \\\n",
       "105           19  [106.84754181, 127.60254669, 504.37875366, 171...   \n",
       "113           21  [162.45884705, 471.86456299, 449.44735718, 668...   \n",
       "20             4  [107.02816772, 100.26654053, 505.24621582, 189...   \n",
       "\n",
       "                                           document_id  chunk_id removed  \\\n",
       "105  a5543da64e4d56508b18d26addd28131c63921a3a0f7a1...       105      []   \n",
       "113  9d850c36997f8f8ebb4bed26cad319cd84c40a8c294660...       113      []   \n",
       "20   3610c8f2db4587033885befe968d9ccbb40f7d5c97f21c...        20      []   \n",
       "\n",
       "     chunk_hash  \n",
       "105          -1  \n",
       "113          -1  \n",
       "20           -1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## Step-8:   Text encoding\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbbeaff",
   "metadata": {},
   "source": [
    "### 8.1 - Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉüèº STAGE-6: Processing input='output/05_fuzzy_dedupe_out' --> output='output/06_embeddings_out'\n"
     ]
    }
   ],
   "source": [
    "STAGE  = 6\n",
    "\n",
    "input_folder = output_fuzzy_dedupe_dir\n",
    "output_folder =  output_embeddings_dir\n",
    "\n",
    "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
    "\n",
    "print (f\"üèÉüèº STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a88f8",
   "metadata": {},
   "source": [
    "### 8.2 - Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:48:15 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "15:48:15 INFO - pipeline id pipeline_id\n",
      "15:48:15 INFO - code location None\n",
      "15:48:15 INFO - number of workers 2 worker options {'num_cpus': 1, 'max_restarts': -1}\n",
      "15:48:15 INFO - actor creation delay 0\n",
      "15:48:15 INFO - job details {'job category': 'preprocessing', 'job name': 'text_encoder', 'job type': 'ray', 'job id': 'job_id'}\n",
      "15:48:15 INFO - data factory data_ is using local data access: input_folder - output/05_fuzzy_dedupe_out output_folder - output/06_embeddings_out\n",
      "15:48:15 INFO - data factory data_ max_files -1, n_sample -1\n",
      "15:48:15 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "15:48:15 INFO - Running locally\n",
      "2024-10-08 15:48:17,403\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:20 INFO - orchestrator started at 2024-10-08 15:48:20\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:20 INFO - Number of files is 2, source profile {'max_file_size': 0.06540584564208984, 'min_file_size': 0.02941417694091797, 'total_file_size': 0.09482002258300781}\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:20 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 16.571041870862246, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:20 INFO - Number of workers - 2 with {'num_cpus': 1, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:25 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:30 INFO - Completed processing 2 files in 0.088 min\n",
      "\u001b[36m(orchestrate pid=60271)\u001b[0m 15:48:30 INFO - done flushing in 0.001 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=60308)\u001b[0m /opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=60308)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "15:48:40 INFO - Completed execution in 0.42 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=60304)\u001b[0m /opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=60304)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Stage:6 completed successfully\n",
      "CPU times: user 341 ms, sys: 231 ms, total: 572 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from text_encoder_transform_ray import TextEncoderRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
    "    # text_encoder\n",
    "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TextEncoderRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "\n",
    "return_code = launcher.launch()\n",
    "\n",
    "if return_code == 0:\n",
    "    print (f\"‚úÖ Stage:{STAGE} completed successfully\")\n",
    "else:\n",
    "    raise Exception (\"‚ùå Ray job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b734852c",
   "metadata": {},
   "source": [
    "### 8.3 - Inspect Generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b1c1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data dimensions (rows x columns)=  (211, 19)\n",
      "Output data dimensions (rows x columns)=  (211, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>source_filename</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>6.2 Code Explanation and Fixing\\nWhile most of...</td>\n",
       "      <td>$.main-text[173]</td>\n",
       "      <td>14</td>\n",
       "      <td>[107.09112549, 340.27554321, 505.74041748, 427...</td>\n",
       "      <td>2c34041020831bf879b275591bf69d51d3a3935ac9eae1...</td>\n",
       "      <td>78</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.013920594, -0.021673767, 0.06490408, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>6.4 Code Reasoning, Understanding and Executio...</td>\n",
       "      <td>$.main-text[209]</td>\n",
       "      <td>18</td>\n",
       "      <td>[106.99939728, 242.3865509, 505.73922729, 373....</td>\n",
       "      <td>c1c7b810f73360048f595898c49a356490fe9708659863...</td>\n",
       "      <td>99</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[-0.05579955, -0.078981526, -0.016305814, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>348</td>\n",
       "      <td>pdf</td>\n",
       "      <td>4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...</td>\n",
       "      <td>655037</td>\n",
       "      <td>2024-10-08T15:46:23.368528</td>\n",
       "      <td>81.505457</td>\n",
       "      <td>granite.pdf</td>\n",
       "      <td>b5367177-e1c1-4643-9281-942f41bc5c87</td>\n",
       "      <td>6.3 Code Editing and Translation\\nTable 11: Pa...</td>\n",
       "      <td>$.main-text[188]</td>\n",
       "      <td>16</td>\n",
       "      <td>[196.31352234, 697.56256104, 414.67791748, 708...</td>\n",
       "      <td>ab5dee480c14bc13a764485e07b33035e6e052d138c231...</td>\n",
       "      <td>86</td>\n",
       "      <td>[]</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0.029248616, -0.007842009, -0.0911203, -0.037...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename  num_pages  num_tables  num_doc_elements  ext  \\\n",
       "78  granite.pdf         28          17               348  pdf   \n",
       "99  granite.pdf         28          17               348  pdf   \n",
       "86  granite.pdf         28          17               348  pdf   \n",
       "\n",
       "                                                 hash    size  \\\n",
       "78  4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "99  4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "86  4b4ee627dd6a21c1eb56205875a3970c3ad975864f3dcf...  655037   \n",
       "\n",
       "                 date_acquired  pdf_convert_time source_filename  \\\n",
       "78  2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "99  2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "86  2024-10-08T15:46:23.368528         81.505457     granite.pdf   \n",
       "\n",
       "                      source_document_id  \\\n",
       "78  b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "99  b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "86  b5367177-e1c1-4643-9281-942f41bc5c87   \n",
       "\n",
       "                                             contents      doc_jsonpath  \\\n",
       "78  6.2 Code Explanation and Fixing\\nWhile most of...  $.main-text[173]   \n",
       "99  6.4 Code Reasoning, Understanding and Executio...  $.main-text[209]   \n",
       "86  6.3 Code Editing and Translation\\nTable 11: Pa...  $.main-text[188]   \n",
       "\n",
       "    page_number                                               bbox  \\\n",
       "78           14  [107.09112549, 340.27554321, 505.74041748, 427...   \n",
       "99           18  [106.99939728, 242.3865509, 505.73922729, 373....   \n",
       "86           16  [196.31352234, 697.56256104, 414.67791748, 708...   \n",
       "\n",
       "                                          document_id  chunk_id removed  \\\n",
       "78  2c34041020831bf879b275591bf69d51d3a3935ac9eae1...        78      []   \n",
       "99  c1c7b810f73360048f595898c49a356490fe9708659863...        99      []   \n",
       "86  ab5dee480c14bc13a764485e07b33035e6e052d138c231...        86      []   \n",
       "\n",
       "    chunk_hash                                         embeddings  \n",
       "78          -1  [-0.013920594, -0.021673767, 0.06490408, -0.03...  \n",
       "99          -1  [-0.05579955, -0.078981526, -0.016305814, 0.05...  \n",
       "86          -1  [0.029248616, -0.007842009, -0.0911203, -0.037...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import read_parquet_files_as_df\n",
    "\n",
    "output_df = read_parquet_files_as_df(output_folder)\n",
    "\n",
    "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.sample(min(3, output_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e12630-be6b-4188-a925-77117155617b",
   "metadata": {},
   "source": [
    "## Step-9: Copy output to final output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Copied output from 'output/06_embeddings_out' --> 'output/output_final'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
    "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
    "\n",
    "print (f\"‚úÖ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
