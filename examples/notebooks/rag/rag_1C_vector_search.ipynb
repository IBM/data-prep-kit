{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Utils to do Vector Search on Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig:\n",
    "    pass\n",
    "MY_CONFIG = MyConfig()\n",
    "\n",
    "MY_CONFIG.EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MY_CONFIG.EMBEDDING_LENGTH = 384\n",
    "\n",
    "MY_CONFIG.DB_URI = './rag_1_dpk.db'  # For embedded instance\n",
    "#MY_CONFIG.DB_URI = 'http://localhost:19530'  # For Docker instance\n",
    "MY_CONFIG.COLLECTION_NAME = 'dpk_walmart_docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance: ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embeddings\n",
    "\n",
    "Two choices here. \n",
    "\n",
    "1. use sentence transformers directly\n",
    "2. use Milvus model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Option 1 - use sentence transformers directly\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = embedding_model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 2 - Milvus model\n",
    "from pymilvus import model\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "\n",
    "# embedding_fn = model.DefaultEmbeddingFunction()\n",
    "\n",
    "## initialize the SentenceTransformerEmbeddingFunction\n",
    "embedding_fn = model.dense.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL,\n",
    "    device='cpu' # this will work on all devices (KIS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence transformer : embeddings len = 384\n",
      "sentence transformer : embeddings[:5] =  [ 0.02468893  0.10352128  0.02752643 -0.08551716 -0.01412826]\n",
      "milvus model wrapper : embeddings len = 384\n",
      "milvus model wrapper  : embeddings[:5] =  [ 0.02468893  0.10352128  0.02752643 -0.08551716 -0.01412826]\n"
     ]
    }
   ],
   "source": [
    "# Test Embeddings\n",
    "text = 'Paris 2024 Olympics'\n",
    "embeddings = get_embeddings(text)\n",
    "print ('sentence transformer : embeddings len =', len(embeddings))\n",
    "print ('sentence transformer : embeddings[:5] = ', embeddings[:5])\n",
    "\n",
    "embeddings = embedding_fn([text])\n",
    "print ('milvus model wrapper : embeddings len =', len(embeddings[0]))\n",
    "print ('milvus model wrapper  : embeddings[:5] = ', embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do A  Vector Search\n",
    "\n",
    "We will do this to verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "## helper function to perform vector search\n",
    "def  do_vector_search (query):\n",
    "    query_vectors = [get_embeddings(query)]  # Option 1 - using sentence transformers\n",
    "    # query_vectors = embedding_fn([query])  # using Milvus model \n",
    "\n",
    "    results = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,  # target collection\n",
    "        data=query_vectors,  # query vectors\n",
    "        limit=5,  # number of returned entities\n",
    "        output_fields=[\"filename\", \"page_number\", \"text\"],  # specifies fields to be returned\n",
    "    )\n",
    "    return results\n",
    "## ----\n",
    "\n",
    "def  print_search_results (results):\n",
    "    # pprint (results)\n",
    "    print ('num results : ', len(results[0]))\n",
    "\n",
    "    for i, r in enumerate (results[0]):\n",
    "        #pprint(r, indent=4)\n",
    "        print (f'------ result {i+1} --------')\n",
    "        print ('search score:', r['distance'])\n",
    "        print ('filename:', r['entity']['filename'])\n",
    "        print ('page number:', r['entity']['page_number'])\n",
    "        print ('text:\\n', r['entity']['text'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "------ result 1 --------\n",
      "search score: 0.5978392958641052\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 99\n",
      "text:\n",
      " Stock Performance Chart\n",
      "Walmart Inc., 2019 = $100.00. Walmart Inc., 2020 = $120.27. Walmart Inc., 2021 = $148.41. Walmart Inc., 2022 = $148.47. Walmart Inc., 2023 = $153.58. Walmart Inc., 2024 = $177.30. S&P 500 Index, 2019 = 100.00. S&P 500 Index, 2020 = 121.68. S&P 500 Index, 2021 = 142.67. S&P 500 Index, 2022 = 175.90. S&P 500 Index, 2023 = 161.45. S&P 500 Index, 2024 = 195.06. S&P 500 Consumer   Discretionary, 2019 = . S&P 500 Consumer   Discretionary, 2020 = . S&P 500 Consumer   Discretionary, 2021 = . S&P 500 Consumer   Discretionary, 2022 = . S&P 500 Consumer   Discretionary, 2023 = . S&P 500 Consumer   Discretionary, 2024 = . Discretionary   Distribution &   RiliId, 2019 = 100.00. Discretionary   Distribution &   RiliId, 2020 = 117.54. Discretionary   Distribution &   RiliId, 2021 = 166.19. Discretionary   Distribution &   RiliId, 2022 = 180.56. Discretionary   Distribution &   RiliId, 2023 = 147.66. Discretionary   Distribution &   RiliId, 2024 = 190.67\n",
      "\n",
      "------ result 2 --------\n",
      "search score: 0.5875853896141052\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "Operating cash flow $36B\n",
      "\n",
      "------ result 3 --------\n",
      "search score: 0.5865607857704163\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "through, up to and including 2030. Additional qualifying information can be found by visiting http://corporate.walmart.com/purpose/esgreport.\n",
      "\n",
      "------ result 4 --------\n",
      "search score: 0.5840539932250977\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "Revenues\n",
      "\n",
      "------ result 5 --------\n",
      "search score: 0.5462992191314697\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "1 Our global advertising business is recorded in either net sales or as a reduction to cost of sales, depending on the nature of the advertising arrangement. 2 1B tonnes CO 2 e emissions reduced, avoided, or sequestered reported by suppliers cumulatively since 2017 through Project Gigaton. Calculated in accordance with Walmart's \"Project Gigaton Accounting Methodology.\" 3 This result also includes emissions impacts that may only be realized in 2024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What was Walmart's revenue in 2023?\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "------ result 1 --------\n",
      "search score: 0.5755810141563416\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "through, up to and including 2030. Additional qualifying information can be found by visiting http://corporate.walmart.com/purpose/esgreport.\n",
      "\n",
      "------ result 2 --------\n",
      "search score: 0.502342700958252\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "1B Tonnes\n",
      "\n",
      "------ result 3 --------\n",
      "search score: 0.5014065504074097\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 99\n",
      "text:\n",
      " Stock Performance Chart\n",
      "Walmart Inc., 2019 = $100.00. Walmart Inc., 2020 = $120.27. Walmart Inc., 2021 = $148.41. Walmart Inc., 2022 = $148.47. Walmart Inc., 2023 = $153.58. Walmart Inc., 2024 = $177.30. S&P 500 Index, 2019 = 100.00. S&P 500 Index, 2020 = 121.68. S&P 500 Index, 2021 = 142.67. S&P 500 Index, 2022 = 175.90. S&P 500 Index, 2023 = 161.45. S&P 500 Index, 2024 = 195.06. S&P 500 Consumer   Discretionary, 2019 = . S&P 500 Consumer   Discretionary, 2020 = . S&P 500 Consumer   Discretionary, 2021 = . S&P 500 Consumer   Discretionary, 2022 = . S&P 500 Consumer   Discretionary, 2023 = . S&P 500 Consumer   Discretionary, 2024 = . Discretionary   Distribution &   RiliId, 2019 = 100.00. Discretionary   Distribution &   RiliId, 2020 = 117.54. Discretionary   Distribution &   RiliId, 2021 = 166.19. Discretionary   Distribution &   RiliId, 2022 = 180.56. Discretionary   Distribution &   RiliId, 2023 = 147.66. Discretionary   Distribution &   RiliId, 2024 = 190.67\n",
      "\n",
      "------ result 4 --------\n",
      "search score: 0.49448615312576294\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "+20%\n",
      "\n",
      "------ result 5 --------\n",
      "search score: 0.49202316999435425\n",
      "filename: Walmart_2024_copy.pdf\n",
      "page number: 2\n",
      "text:\n",
      " \"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "+6%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How many distribution facilities does Walmart have?\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# milvus_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
