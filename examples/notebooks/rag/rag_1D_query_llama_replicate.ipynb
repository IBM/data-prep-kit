{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data using LLM\n",
    "\n",
    "Here is the overall RAG pipeline.   In this notebook, we will do steps (5), (6), (7), (8), (9)\n",
    "- Importing data is already done in this notebook [rag_1_B_load_data.ipynb](rag_1_B_load_data.ipynb)\n",
    "- üëâ Step 5: Calculate embedding for user query\n",
    "- üëâ Step 6 & 7: Send the query to vector db to retrieve relevant documents\n",
    "- üëâ Step 8 & 9: Send the query and relevant documents (returned above step) to LLM and get answers to our query\n",
    "\n",
    "![image missing](../media/rag-overview-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig:\n",
    "    pass\n",
    "MY_CONFIG = MyConfig()\n",
    "\n",
    "MY_CONFIG.EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MY_CONFIG.EMBEDDING_LENGTH = 384\n",
    "\n",
    "MY_CONFIG.DB_URI = './rag_1_dpk.db'  # For embedded instance\n",
    "#MY_CONFIG.DB_URI = 'http://localhost:19530'  # For Docker instance\n",
    "MY_CONFIG.COLLECTION_NAME = 'dpk_walmart_docs'\n",
    "\n",
    "MY_CONFIG.LLM_MODEL = \"meta/meta-llama-3-8b-instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Create a .env file with the following properties.  You can use [env.txt](../env.txt) as starting point\n",
    "\n",
    "---\n",
    "\n",
    "```text\n",
    "REPLICATE_API_TOKEN=YOUR_TOKEN_GOES_HERE\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ config REPLICATE_API_TOKEN found\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "MY_CONFIG.REPLICATE_API_TOKEN = config.get('REPLICATE_API_TOKEN')\n",
    "\n",
    "if  MY_CONFIG.REPLICATE_API_TOKEN:\n",
    "    print (\"‚úÖ config REPLICATE_API_TOKEN found\")\n",
    "else:\n",
    "    raise Exception (\"'‚ùå REPLICATE_API_TOKEN' is not set.  Please set it above to continue...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Milvus instance: ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"‚úÖ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-: Setup Embeddings\n",
    "\n",
    "Use the same embeddings we used to index our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/sujee/apps/anaconda3/envs/data-prep-kit-2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings len = 384\n",
      "embeddings[:5] =  [ 0.02468893  0.10352128  0.02752643 -0.08551716 -0.01412826]\n"
     ]
    }
   ],
   "source": [
    "# Test embeddings\n",
    "embeddings = get_embeddings('Paris 2024 Olympics')\n",
    "print ('embeddings len =', len(embeddings))\n",
    "print ('embeddings[:5] = ', embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents using vector / sementic search\n",
    "\n",
    "def fetch_relevant_documents (query : str) :\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "        data = [get_embeddings(query)], # Use the `emb_text` function to convert the question to an embedding vector\n",
    "        limit=3,  # Return top 3 results\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "    # print (search_res)\n",
    "\n",
    "    retrieved_docs_with_distances = [\n",
    "        {'text': res[\"entity\"][\"text\"], 'distance' : res[\"distance\"]} for res in search_res[0]\n",
    "    ]\n",
    "    return retrieved_docs_with_distances\n",
    "## --- end ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'distance': 0.5978392958641052,\n",
      "        'text': 'Stock Performance Chart\\n'\n",
      "                'Walmart Inc., 2019 = $100.00. Walmart Inc., 2020 = $120.27. '\n",
      "                'Walmart Inc., 2021 = $148.41. Walmart Inc., 2022 = $148.47. '\n",
      "                'Walmart Inc., 2023 = $153.58. Walmart Inc., 2024 = $177.30. '\n",
      "                'S&P 500 Index, 2019 = 100.00. S&P 500 Index, 2020 = 121.68. '\n",
      "                'S&P 500 Index, 2021 = 142.67. S&P 500 Index, 2022 = 175.90. '\n",
      "                'S&P 500 Index, 2023 = 161.45. S&P 500 Index, 2024 = 195.06. '\n",
      "                'S&P 500 Consumer   Discretionary, 2019 = . S&P 500 Consumer   '\n",
      "                'Discretionary, 2020 = . S&P 500 Consumer   Discretionary, '\n",
      "                '2021 = . S&P 500 Consumer   Discretionary, 2022 = . S&P 500 '\n",
      "                'Consumer   Discretionary, 2023 = . S&P 500 Consumer   '\n",
      "                'Discretionary, 2024 = . Discretionary   Distribution &   '\n",
      "                'RiliId, 2019 = 100.00. Discretionary   Distribution &   '\n",
      "                'RiliId, 2020 = 117.54. Discretionary   Distribution &   '\n",
      "                'RiliId, 2021 = 166.19. Discretionary   Distribution &   '\n",
      "                'RiliId, 2022 = 180.56. Discretionary   Distribution &   '\n",
      "                'RiliId, 2023 = 147.66. Discretionary   Distribution &   '\n",
      "                'RiliId, 2024 = 190.67'},\n",
      "    {   'distance': 0.5875853896141052,\n",
      "        'text': '\"At Walmart, we\\'re a people-led, tech-powered omnichannel '\n",
      "                'retailer dedicated\\n'\n",
      "                'Operating cash flow $36B'},\n",
      "    {   'distance': 0.5865607857704163,\n",
      "        'text': '\"At Walmart, we\\'re a people-led, tech-powered omnichannel '\n",
      "                'retailer dedicated\\n'\n",
      "                'through, up to and including 2030. Additional qualifying '\n",
      "                'information can be found by visiting '\n",
      "                'http://corporate.walmart.com/purpose/esgreport.'}]\n"
     ]
    }
   ],
   "source": [
    "# test relevant vector search\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "question = \"What was Walmart's revenue in 2023?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "pprint.pprint(relevant_docs, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM\n",
    "\n",
    "### LLM Choices at Replicate\n",
    "\n",
    "- llama 3.1 : Latest\n",
    "    - **meta/meta-llama-3.1-405b-instruct** : Meta's flagship 405 billion parameter language model, fine-tuned for chat completions\n",
    "- Base version of llama-3 from meta\n",
    "    - [meta/meta-llama-3-8b](https://replicate.com/meta/meta-llama-3-8b) : Base version of Llama 3, an 8 billion parameter language model from Meta.\n",
    "    - **meta/meta-llama-3-70b** : 70 billion\n",
    "- Instruct versions of llama-3 from meta, fine tuned for chat completions\n",
    "    - **meta/meta-llama-3-8b-instruct** : An 8 billion parameter language model from Meta, \n",
    "    - **meta/meta-llama-3-70b-instruct** : 70 billion\n",
    "\n",
    "References \n",
    "\n",
    "- https://docs.llamaindex.ai/en/stable/examples/llm/llama_2/?h=replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = MY_CONFIG.REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    print ('============ context (this is the context supplied to LLM) ============')\n",
    "    print (context)\n",
    "    print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    print ('============ here is the answer from LLM... STREAMING... =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    print ('============ context (this is the context supplied to LLM) ============')\n",
    "    print (context)\n",
    "    print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "\n",
    "    print ('============ here is the answer from LLM... STREAMING... =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 0,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      "Stock Performance Chart\n",
      "Walmart Inc., 2019 = $100.00. Walmart Inc., 2020 = $120.27. Walmart Inc., 2021 = $148.41. Walmart Inc., 2022 = $148.47. Walmart Inc., 2023 = $153.58. Walmart Inc., 2024 = $177.30. S&P 500 Index, 2019 = 100.00. S&P 500 Index, 2020 = 121.68. S&P 500 Index, 2021 = 142.67. S&P 500 Index, 2022 = 175.90. S&P 500 Index, 2023 = 161.45. S&P 500 Index, 2024 = 195.06. S&P 500 Consumer   Discretionary, 2019 = . S&P 500 Consumer   Discretionary, 2020 = . S&P 500 Consumer   Discretionary, 2021 = . S&P 500 Consumer   Discretionary, 2022 = . S&P 500 Consumer   Discretionary, 2023 = . S&P 500 Consumer   Discretionary, 2024 = . Discretionary   Distribution &   RiliId, 2019 = 100.00. Discretionary   Distribution &   RiliId, 2020 = 117.54. Discretionary   Distribution &   RiliId, 2021 = 166.19. Discretionary   Distribution &   RiliId, 2022 = 180.56. Discretionary   Distribution &   RiliId, 2023 = 147.66. Discretionary   Distribution &   RiliId, 2024 = 190.67\n",
      "\"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "Operating cash flow $36B\n",
      "\"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "through, up to and including 2030. Additional qualifying information can be found by visiting http://corporate.walmart.com/purpose/esgreport.\n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "The provided context does not mention Walmart's revenue in 2023. However, it does provide the stock performance chart for Walmart Inc. in 2023, which shows that the stock price was $153.58.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 254 ms, sys: 17.3 ms, total: 271 ms\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"What was Walmart's revenue in 2023?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      "\"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "through, up to and including 2030. Additional qualifying information can be found by visiting http://corporate.walmart.com/purpose/esgreport.\n",
      "Stock Performance Chart\n",
      "Walmart Inc., 2019 = $100.00. Walmart Inc., 2020 = $120.27. Walmart Inc., 2021 = $148.41. Walmart Inc., 2022 = $148.47. Walmart Inc., 2023 = $153.58. Walmart Inc., 2024 = $177.30. S&P 500 Index, 2019 = 100.00. S&P 500 Index, 2020 = 121.68. S&P 500 Index, 2021 = 142.67. S&P 500 Index, 2022 = 175.90. S&P 500 Index, 2023 = 161.45. S&P 500 Index, 2024 = 195.06. S&P 500 Consumer   Discretionary, 2019 = . S&P 500 Consumer   Discretionary, 2020 = . S&P 500 Consumer   Discretionary, 2021 = . S&P 500 Consumer   Discretionary, 2022 = . S&P 500 Consumer   Discretionary, 2023 = . S&P 500 Consumer   Discretionary, 2024 = . Discretionary   Distribution &   RiliId, 2019 = 100.00. Discretionary   Distribution &   RiliId, 2020 = 117.54. Discretionary   Distribution &   RiliId, 2021 = 166.19. Discretionary   Distribution &   RiliId, 2022 = 180.56. Discretionary   Distribution &   RiliId, 2023 = 147.66. Discretionary   Distribution &   RiliId, 2024 = 190.67\n",
      "\"At Walmart, we're a people-led, tech-powered omnichannel retailer dedicated\n",
      "+6%\n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "I apologize, but the provided context does not mention the number of distribution centers Walmart has. The context appears to be discussing Walmart's stock performance and its commitment to being a people-led, tech-powered omnichannel retailer. It does not provide information about the number of distribution centers.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 214 ms, sys: 4.25 ms, total: 218 ms\n",
      "Wall time: 928 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"How many distribution centers does Walmart have?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ context (this is the context supplied to LLM) ============\n",
      "                                                 -                                                                        \n",
      "3/29/2024 10:28:40 AM\n",
      "*E@4< '6C7@C>2?46  92CE\n",
      " &$' ) *&% &    0  )  ,$,# + -  +&+ # ) +,)% \n",
      " &7-  59.:&0*287 &2) %36/.2,  &4.8&0  *+.(.8 \n",
      ":D42= 062CD  ?565 !2?F2CJ   \n",
      "============ end  context ============\n",
      "============ here is the answer from LLM... STREAMING... =====\n",
      "I'm happy to help! However, I must point out that the provided context does not contain any information about the moon landing. The text appears to be a jumbled mix of characters and symbols, and does not provide any relevant information about the moon landing or any other historical event.\n",
      "\n",
      "If you could provide a different context or question, I would be happy to try and assist you to the best of my abilities.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 268 ms, sys: 12.4 ms, total: 280 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"When was the moon landing?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
