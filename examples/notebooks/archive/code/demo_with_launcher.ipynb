{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Demo: Data Prep Kit</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Welcome to the demo notebook! Inside, you will find an end-to-end sample data pipeline designed for processing code datasets, beginning with GitHub repositories (.zip files) and culminating in processed data. This notebook provides the following transforms for processing the data. \n",
    "\n",
    "- [Ingest2parquet](#item1)\n",
    "- [Exact Dedup](#item2)\n",
    "- [Doc_ID generation](#item3)\n",
    "- [Fuzzy Dedup](#item4)\n",
    "- [Programming Language Select](#item5)\n",
    "- [Code quality](#item6)\n",
    "- [Filtering](#item7)\n",
    "- [Tokenization](#item8)\n",
    "\n",
    "### Getting started\n",
    "\n",
    "If you want to try this pipeline on your data, you need to download your github repositories, as .zip files. Please refer to steps below for the same. One can also try it on sample data by downloading a few repos of interest.\n",
    "\n",
    "Here's how to download a GitHub repository in ZIP format:\n",
    "\n",
    "1. Go to the desired repository on GitHub.\n",
    "2. Click the \"Code\" button near the top right corner of the repository.\n",
    "3. Click the \"Download ZIP\" button.\n",
    "\n",
    "This will download a ZIP archive of the entire repository to your computer.\n",
    "\n",
    "Follow these steps and download some repositories from github into a folder. Now your data is ready.\n",
    "\n",
    "The folder containing this data would serve as the input to the pipeline. Assign the path of this data folder to the variable `zip_input_folder` in the below cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d11e5ce1-23cc-445a-b1fa-4b8129aa645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture logpip --no-stderr\n",
    "!pip install data-prep-toolkit-transforms-ray==0.2.1.dev1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# We can set input paths here\n",
    "zip_input_folder = \"input_data\"\n",
    "\n",
    "if not os.path.exists(zip_input_folder):\n",
    "    print (\"NO INPUT DATA\")\n",
    "    print (\"Please set `zip_input_folder` variable to path containing data\")\n",
    "\n",
    "# make sure the paths are correct\n",
    "data_base_path = \"test-data\"\n",
    "\n",
    "parquet_data_output = os.path.join(data_base_path, \"parquet_input\")\n",
    "\n",
    "ededup_out =  os.path.join(data_base_path, \"ededup_out\")\n",
    "\n",
    "doc_id_out =  os.path.join(data_base_path, \"doc_id_out\")\n",
    "fdedup_out = os.path.join(data_base_path, \"fdedup_out\")\n",
    "\n",
    "lang_out =  os.path.join(data_base_path,\"lang_out\")\n",
    "cq_out = os.path.join(data_base_path,\"cq_out\")\n",
    "\n",
    "filter_out = os.path.join(data_base_path ,\"filter_out\")\n",
    "tokensization_out = os.path.join(data_base_path ,\"tokenization_out\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\"> 1. Convert data to parquet using ingest2parquet [<-](#top)<a class=\"anchor\" id=\"item1\"></a>\n",
    "_zip_ to _parquet_ </span>\n",
    "\n",
    "Raw code data files which are in zip format are converted to parquet files, where each row of the parquet file corresponds to a separate code file. Apart from the contents of the code file, every row also contains a unique document id, file URL, name of the repository, source of the data, date of acquisition and license of the repository. For every code file, a language field is also added, which is detected using the filename\n",
    "extensions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this stage input folder contains the zip files, each zip file contains a github repo.\n",
    "\n",
    "input_folder = zip_input_folder\n",
    "output_folder =  parquet_data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:52:23 INFO - Running locally\n",
      "10:52:23 INFO - data factory code2parquet_ is using local configuration without input/output path\n",
      "10:52:23 INFO - data factory code2parquet_ max_files -1, n_sample -1\n",
      "10:52:23 INFO - data factory code2parquet_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:52:23 INFO - data factory data_ is using local data access: input_folder - input_data output_folder - test-data/parquet_input\n",
      "10:52:23 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:52:23 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.zip'], files to checkpoint ['.parquet']\n",
      "10:52:23 INFO - pipeline id pipeline_id\n",
      "10:52:23 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "10:52:23 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1}\n",
      "10:52:23 INFO - actor creation delay 0\n",
      "10:52:23 INFO - job details {'job category': 'preprocessing', 'job name': 'code2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:52:25,864\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:26 INFO - orchestrator started at 2024-08-22 10:52:26\n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:26 INFO - Number of files is 2, source profile {'max_file_size': 4.850915908813477, 'min_file_size': 0.2978849411010742, 'total_file_size': 5.148800849914551}\n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:26 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 13.397023010998964, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:26 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:26 INFO - Completed 0 files (0.0%)  in 3.45309575398763e-06 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=88515)\u001b[0m 10:52:27 WARNING - file environments-master/cfortunes/diebenkorn_notes.dat is empty. content , skipping\n",
      "\u001b[36m(RayTransformFileProcessor pid=88515)\u001b[0m 10:52:27 WARNING - file environments-master/cfortunes/obliquestrategies.dat is empty. content , skipping\n",
      "\u001b[36m(RayTransformFileProcessor pid=88515)\u001b[0m 10:52:27 WARNING - file environments-master/commands/grel is empty. content , skipping\n",
      "\u001b[36m(RayTransformFileProcessor pid=88515)\u001b[0m 10:52:27 WARNING - file environments-master/commands/ldid is empty. content , skipping\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m 10:52:27 WARNING - Exception File is not a zip file processing file /Users/touma/data-prep-lab/examples/notebooks/code/input_data/.ipynb_checkpoints/test-checkpoint.zip: Traceback (most recent call last):\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/runtime/transform_file_processor.py\", line 62, in process_file\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m     out_files, stats = self.transform.transform_binary(file_name=f_name, byte_array=filedata)\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/code2parquet_transform.py\", line 118, in transform_binary\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m     with zipfile.ZipFile(io.BytesIO(bytes(byte_array))) as opened_zip:\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m   File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/zipfile.py\", line 1302, in __init__\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m     self._RealGetContents()\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m   File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/zipfile.py\", line 1369, in _RealGetContents\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m     raise BadZipFile(\"File is not a zip file\")\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m zipfile.BadZipFile: File is not a zip file\n",
      "\u001b[36m(RayTransformFileProcessor pid=88516)\u001b[0m \n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:27 INFO - Completed processing 2 files in 0.010891234874725342 min\n",
      "\u001b[36m(orchestrate pid=88509)\u001b[0m 10:52:27 INFO - done flushing in 0.0013649463653564453 sec\n",
      "10:52:37 INFO - Completed execution in 0.22322853803634643 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from code2parquet_transform import (\n",
    "    detect_programming_lang_cli_key,\n",
    "    supported_langs_file_cli_key,\n",
    ")\n",
    "from code2parquet_transform_ray import CodeToParquetRayConfiguration\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "\n",
    "\n",
    "# create parameters\n",
    "supported_languages_file = os.path.abspath(\n",
    "    \"../../../transforms/code/code2parquet/python/test-data/languages/lang_extensions.json\"\n",
    ")\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8, \"memory\": 2 * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    supported_langs_file_cli_key: supported_languages_file,\n",
    "    detect_programming_lang_cli_key: True,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.zip']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(CodeToParquetRayConfiguration())\n",
    "# launch\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "##  <span style=\"color: green\">   2. Exact Dedup [<-](#top)<a class=\"anchor\" id=\"item2\"></a> </span>\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a0e1f6-ff53-40aa-96b1-096ade4bd1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/parquet_input\n",
      "test-data/ededup_out\n"
     ]
    }
   ],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = parquet_data_output\n",
    "output_folder = ededup_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:52:38 INFO - Running locally\n",
      "10:52:38 INFO - exact dedup params are {'doc_column': 'contents', 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "10:52:38 INFO - data factory data_ is using local data access: input_folder - test-data/parquet_input output_folder - test-data/ededup_out\n",
      "10:52:38 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:52:38 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:52:38 INFO - pipeline id pipeline_id\n",
      "10:52:38 INFO - code location None\n",
      "10:52:38 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:52:38 INFO - actor creation delay 0\n",
      "10:52:38 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:52:40,592\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:41 INFO - orchestrator started at 2024-08-22 10:52:41\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:41 INFO - Number of files is 1, source profile {'max_file_size': 0.04346275329589844, 'min_file_size': 0.04346275329589844, 'total_file_size': 0.04346275329589844}\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:41 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 13.453863525763154, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:41 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:41 INFO - Completed 0 files (0.0%)  in 2.5351842244466145e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:43 INFO - Completed processing 1 files in 0.03372826973597209 min\n",
      "\u001b[36m(orchestrate pid=88563)\u001b[0m 10:52:43 INFO - done flushing in 0.0025000572204589844 sec\n",
      "10:52:53 INFO - Completed execution in 0.2433228333791097 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "ededup_launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "# launch\n",
    "ededup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  3. DOC ID generation [<-](#top)<a class=\"anchor\" id=\"item3\"></a> </span>\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/ededup_out\n",
      "test-data/doc_id_out\n"
     ]
    }
   ],
   "source": [
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "input_folder = ededup_out\n",
    "output_folder = doc_id_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:52:54 INFO - Running locally\n",
      "10:52:54 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "10:52:54 INFO - data factory data_ is using local data access: input_folder - test-data/ededup_out output_folder - test-data/doc_id_out\n",
      "10:52:54 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:52:54 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:52:54 INFO - pipeline id pipeline_id\n",
      "10:52:54 INFO - code location None\n",
      "10:52:54 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:52:54 INFO - actor creation delay 0\n",
      "10:52:54 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:52:56,551\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:57 INFO - orchestrator started at 2024-08-22 10:52:57\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:57 INFO - Number of files is 1, source profile {'max_file_size': 0.04342079162597656, 'min_file_size': 0.04342079162597656, 'total_file_size': 0.04342079162597656}\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:57 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 13.392010498791933, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:57 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:57 INFO - Completed 0 files (0.0%)  in 2.102057139078776e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:58 INFO - Completed processing 1 files in 0.013987716039021809 min\n",
      "\u001b[36m(orchestrate pid=88632)\u001b[0m 10:52:58 INFO - done flushing in 0.0014891624450683594 sec\n",
      "10:53:08 INFO - Completed execution in 0.22342359622319538 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## 4. <span style=\"color: green\">  Fuzzy Dedup [<-](#top)<a class=\"anchor\" id=\"item4\"></a> </span>\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/doc_id_out\n",
      "test-data/fdedup_out\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "input_folder = doc_id_out\n",
    "output_folder = fdedup_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:53:20 INFO - Running locally\n",
      "10:53:20 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 0.8}}\n",
      "10:53:20 INFO - data factory data_ is using local data access: input_folder - test-data/doc_id_out output_folder - test-data/fdedup_out\n",
      "10:53:20 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:53:20 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:53:20 INFO - pipeline id pipeline_id\n",
      "10:53:20 INFO - code location None\n",
      "10:53:20 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:53:20 INFO - actor creation delay 0\n",
      "10:53:20 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:53:21,947\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - orchestrator started at 2024-08-22 10:53:22\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - Number of files is 1, source profile {'max_file_size': 0.04804039001464844, 'min_file_size': 0.04804039001464844, 'total_file_size': 0.04804039001464844}\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 13.416908264160156, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - Table preprocessing uses 2 readers\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - created 2 table processor actors\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:22 INFO - Completed 0 files (0.0%)  in 3.135204315185547e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:26 INFO - Completed processing 1 files in 0.0659868836402893 min\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:26 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:27 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:27 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:28 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:28 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:28 INFO - created 2 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:28 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:28 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=88740)\u001b[0m 10:53:28 INFO - processing buckets 0 long, 255 short\n",
      "\u001b[36m(BucketsHash pid=88740)\u001b[0m 10:53:28 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:29 INFO - Done processing buckets in 0.01056810220082601 min\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:29 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=88777)\u001b[0m 10:53:29 INFO - Waiting bucket processing completion. Submitted requests 3\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:31 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:31 INFO - Completed 0 files (0.0%)  in 3.449122111002604e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:34 INFO - Completed processing 1 files in 0.04709231456120809 min\n",
      "\u001b[36m(orchestrate pid=88730)\u001b[0m 10:53:34 INFO - done flushing in 0.003609895706176758 sec\n",
      "10:53:44 INFO - Completed execution in 0.4046771128972371 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "fdedup_launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "fdedup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d761-62a7-4a12-ad23-b2c268ad8ed2",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  5. Programming language annotation [<-](#top)<a class=\"anchor\" id=\"item5\"></a> </span>\n",
    "\n",
    "The raw data may contains many programming languages. Of this, we would wish to retain a prioritised list of selected programming languages. This component takes a file which has new line separated names of languages we need to select. It annotates the data a new column with boolean values. This column can be used by filter component to select the required languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db05e1e-4c62-4367-93ca-b2ddff95e4b4",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8ec4fb6-fa62-45d1-9aa1-596d7182b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder = fdedup_out\n",
    "output_folder = lang_out \n",
    "selected_languages_file = \"./test-data/allowed-code-languages.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e7e5a-064f-4dca-a017-4211f7a3e980",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48dbb2a3-a6f4-4a3d-bb2f-8491fd063611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:53:45 INFO - Running locally\n",
      "10:53:45 INFO - data factory proglang_select_ is using local configuration without input/output path\n",
      "10:53:45 INFO - data factory proglang_select_ max_files -1, n_sample -1\n",
      "10:53:45 INFO - data factory proglang_select_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:53:45 INFO - data factory data_ is using local data access: input_folder - test-data/fdedup_out output_folder - test-data/lang_out\n",
      "10:53:45 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:53:45 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:53:45 INFO - pipeline id pipeline_id\n",
      "10:53:45 INFO - code location None\n",
      "10:53:45 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:53:45 INFO - actor creation delay 0\n",
      "10:53:45 INFO - job details {'job category': 'preprocessing', 'job name': 'proglang_select', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:53:47,639\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 INFO - orchestrator started at 2024-08-22 10:53:48\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 INFO - Number of files is 1, source profile {'max_file_size': 0.044445037841796875, 'min_file_size': 0.044445037841796875, 'total_file_size': 0.044445037841796875}\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 13.360301208682358, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 INFO - Getting supported languages from file ./test-data/allowed-code-languages.txt\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 ERROR - Error reading file ./test-data/allowed-code-languages.txt: [Errno 2] No such file or directory: './test-data/allowed-code-languages.txt'\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing_ray/runtime/ray/transform_orchestrator.py\", line 83, in orchestrate\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m     \"transform_params\": runtime.get_transform_config(\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/proglang_select_transform_ray.py\", line 70, in get_transform_config\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m     lang_list = _get_supported_languages(\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/proglang_select_transform.py\", line 33, in _get_supported_languages\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m     lang_list, _ = data_access.get_file(lang_file)\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/data_access/data_access_local.py\", line 367, in get_file\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m     raise e\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/data_access/data_access_local.py\", line 361, in get_file\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m     with open(path, \"rb\") as f:\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m          ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: './test-data/allowed-code-languages.txt'\n",
      "\u001b[36m(orchestrate pid=88862)\u001b[0m 10:53:48 ERROR - Exception during execution [Errno 2] No such file or directory: './test-data/allowed-code-languages.txt': None\n",
      "10:53:58 INFO - Completed execution in 0.21568763256072998 min, execution result 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from proglang_select_transform_ray import ProgLangSelectRayConfiguration\n",
    "from proglang_select_transform import (\n",
    "    lang_allowed_langs_file_key,\n",
    "    lang_lang_column_key,\n",
    "    lang_output_column_key,\n",
    ")\n",
    "\n",
    "# create parameters\n",
    "language_column_name = \"programming_language\"\n",
    "annotated_column_name = \"lang_selected\"\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langselect_config = {\n",
    "    lang_allowed_langs_file_key: selected_languages_file,\n",
    "    lang_lang_column_key: language_column_name,\n",
    "    lang_output_column_key: annotated_column_name,\n",
    "}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 1,\n",
    "    # language selection specific parameters\n",
    "    **langselect_config,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(ProgLangSelectRayConfiguration())\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  6. Code Quality [<-](#top)<a class=\"anchor\" id=\"item6\"></a> </span>\n",
    "\n",
    "We experiment with various code quality metrics but finally retain\n",
    "the four code quality metrics used by (Li et al., 2023) to balance the tradeoff between\n",
    "code quality versus data volume. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/lang_out\n",
      "test-data/cq_out\n"
     ]
    }
   ],
   "source": [
    "input_folder = lang_out\n",
    "output_folder = cq_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:54:05 INFO - Running locally\n",
      "10:54:05 INFO - data factory data_ is using local data access: input_folder - test-data/lang_out output_folder - test-data/cq_out\n",
      "10:54:05 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:54:05 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:54:05 INFO - pipeline id pipeline_id\n",
      "10:54:05 INFO - code location None\n",
      "10:54:05 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:54:05 INFO - actor creation delay 0\n",
      "10:54:05 INFO - job details {'job category': 'preprocessing', 'job name': 'code_quality', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:54:07,325\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=88945)\u001b[0m 10:54:09 INFO - orchestrator started at 2024-08-22 10:54:09\n",
      "\u001b[36m(orchestrate pid=88945)\u001b[0m 10:54:09 ERROR - No input files to process - exiting\n",
      "10:54:19 INFO - Completed execution in 0.22560758193333943 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from code_quality_transform_ray import CodeQualityRayTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "language_column_name = \"programming_language\"\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    # code quality configuration\n",
    "    \"cq_contents_column_name\": \"contents\",\n",
    "    \"cq_language_column_name\": language_column_name,\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(CodeQualityRayTransformConfiguration())\n",
    "# launch\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9839-13b4-4d32-89c5-38f59c5a89f0",
   "metadata": {},
   "source": [
    "## 7. <span style=\"color: green\">   Filtering [<-](#top)<a class=\"anchor\" id=\"item7\"></a> </span>\n",
    "\n",
    "Filter out documents that do not meet the quality threshold for each annotation. The thresholds are computed based on a distributional\n",
    "analysis as well as manual inspection of samples maintaining the balance between data quality and data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c54d69-8aee-4f0f-b74c-35dc0609270f",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7991811-b19e-43b5-89ac-b24060c0ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = cq_out\n",
    "output_folder = filter_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460e05c-aeee-4b53-9dd5-8dfa1afc0ece",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61dea2b0-0e54-4912-8620-886e2b8420ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:54:21 INFO - Running locally\n",
      "10:54:21 INFO - data factory data_ is using local data access: input_folder - test-data/cq_out output_folder - test-data/filter_out\n",
      "10:54:21 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:54:21 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:54:21 INFO - pipeline id pipeline_id\n",
      "10:54:21 INFO - code location None\n",
      "10:54:21 INFO - number of workers 5 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:54:21 INFO - actor creation delay 0\n",
      "10:54:21 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:54:23,149\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=89002)\u001b[0m 10:54:23 INFO - orchestrator started at 2024-08-22 10:54:23\n",
      "\u001b[36m(orchestrate pid=89002)\u001b[0m 10:54:23 ERROR - No input files to process - exiting\n",
      "10:54:33 INFO - Completed execution in 0.20987151861190795 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from data_processing.data_access import DataAccessLocal\n",
    "from filter_transform import (\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "from filter_transform_ray import FilterRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"total_num_lines > 10 AND total_num_lines < 90\",\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "}\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "launcher_params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(launcher_params | filter_params)\n",
    "# Create the longer to launch with the blocklist transform.\n",
    "launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## 8. <span style=\"color: green\">  Tokenization [<-](#top)<a class=\"anchor\" id=\"item8\"></a> </span>\n",
    "\n",
    "The data tokenization transform maps a (non-empty) input table to an output table using a pre-trained tokenizer. The input table must contain at least two columns, by default named document_id and contents. The tokenization transform utilizes the pre-trained tokenizer to tokenize each row (assuming a document) in the input table to each row in the output folder.\n",
    "\n",
    "A pre-trained tokenizer must be specified through the --tkn_tokenizer parameter, which can be the name of a ready-for-download tokenizer from HuggingFace such as hf-internal-testing/llama-tokenizer, bigcode/starcoder or any others that can loaded by the Huggingface AutoTokenizer library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = filter_out\n",
    "output_folder = tokensization_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:54:35 INFO - Running locally\n",
      "10:54:35 INFO - data factory data_ is using local data access: input_folder - test-data/filter_out output_folder - test-data/tokenization_out\n",
      "10:54:35 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:54:35 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:54:35 INFO - pipeline id pipeline_id\n",
      "10:54:35 INFO - code location None\n",
      "10:54:35 INFO - number of workers 5 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:54:35 INFO - actor creation delay 0\n",
      "10:54:35 INFO - job details {'job category': 'preprocessing', 'job name': 'Tokenization', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-22 10:54:37,293\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=89059)\u001b[0m 10:54:38 INFO - orchestrator started at 2024-08-22 10:54:38\n",
      "\u001b[36m(orchestrate pid=89059)\u001b[0m 10:54:38 ERROR - No input files to process - exiting\n",
      "10:54:48 INFO - Completed execution in 0.22238094806671144 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenization_transform_ray import TokenizationRayConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TokenizationRayConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e86b0-4d1e-455a-a44a-d09107a9dae4",
   "metadata": {},
   "source": [
    "## Repo Level Ordering Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c54cc37-7c60-43f7-9193-ce919995dddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: python  [-h] [--run_locally RUN_LOCALLY] [--repo_lvl_stage_one_only]\n",
      "               [--repo_lvl_grouping_column REPO_LVL_GROUPING_COLUMN]\n",
      "               [--repo_lvl_language_column REPO_LVL_LANGUAGE_COLUMN]\n",
      "               [--repo_lvl_store_type REPO_LVL_STORE_TYPE]\n",
      "               [--repo_lvl_store_backend_dir REPO_LVL_STORE_BACKEND_DIR]\n",
      "               [--repo_lvl_store_ray_cpus REPO_LVL_STORE_RAY_CPUS]\n",
      "               [--repo_lvl_store_ray_nworkers REPO_LVL_STORE_RAY_NWORKERS]\n",
      "               [--repo_lvl_sorting_enabled REPO_LVL_SORTING_ENABLED]\n",
      "               [--repo_lvl_sorting_algo REPO_LVL_SORTING_ALGO]\n",
      "               [--repo_lvl_output_by_langs REPO_LVL_OUTPUT_BY_LANGS]\n",
      "               [--repo_lvl_combine_rows REPO_LVL_COMBINE_ROWS]\n",
      "               [--data_s3_cred DATA_S3_CRED] [--data_s3_config DATA_S3_CONFIG]\n",
      "               [--data_local_config DATA_LOCAL_CONFIG]\n",
      "               [--data_max_files DATA_MAX_FILES]\n",
      "               [--data_checkpointing DATA_CHECKPOINTING]\n",
      "               [--data_files_to_checkpoint DATA_FILES_TO_CHECKPOINT]\n",
      "               [--data_data_sets DATA_DATA_SETS]\n",
      "               [--data_files_to_use DATA_FILES_TO_USE]\n",
      "               [--data_num_samples DATA_NUM_SAMPLES]\n",
      "               [--runtime_num_workers RUNTIME_NUM_WORKERS]\n",
      "               [--runtime_worker_options RUNTIME_WORKER_OPTIONS]\n",
      "               [--runtime_creation_delay RUNTIME_CREATION_DELAY]\n",
      "               [--runtime_pipeline_id RUNTIME_PIPELINE_ID]\n",
      "               [--runtime_job_id RUNTIME_JOB_ID]\n",
      "               [--runtime_code_location RUNTIME_CODE_LOCATION]\n",
      "python : error: argument --repo_lvl_output_by_langs: expected one argument\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:1902\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     namespace, args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:2114\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args\u001b[0;34m(self, arg_strings, namespace)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     \u001b[38;5;66;03m# consume the next optional and any arguments for it\u001b[39;00m\n\u001b[0;32m-> 2114\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mconsume_optional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;66;03m# consume any positionals following the last Optional\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:2044\u001b[0m, in \u001b[0;36mArgumentParser._parse_known_args.<locals>.consume_optional\u001b[0;34m(start_index)\u001b[0m\n\u001b[1;32m   2043\u001b[0m selected_patterns \u001b[38;5;241m=\u001b[39m arg_strings_pattern[start:]\n\u001b[0;32m-> 2044\u001b[0m arg_count \u001b[38;5;241m=\u001b[39m \u001b[43mmatch_argument\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_patterns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2045\u001b[0m stop \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m arg_count\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:2208\u001b[0m, in \u001b[0;36mArgumentParser._match_argument\u001b[0;34m(self, action, arg_strings_pattern)\u001b[0m\n\u001b[1;32m   2205\u001b[0m         msg \u001b[38;5;241m=\u001b[39m ngettext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m argument\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2206\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m arguments\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2207\u001b[0m                        action\u001b[38;5;241m.\u001b[39mnargs) \u001b[38;5;241m%\u001b[39m action\u001b[38;5;241m.\u001b[39mnargs\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArgumentError(action, msg)\n\u001b[1;32m   2210\u001b[0m \u001b[38;5;66;03m# return the number of arguments matched\u001b[39;00m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument --repo_lvl_output_by_langs: expected one argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[20], line 50\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Launch the ray actor(s) to process the input\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mlauncher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing_ray/runtime/ray/transform_launcher.py:119\u001b[0m, in \u001b[0;36mRayTransformLauncher.launch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mExecute method orchestrates driver invocation\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m:return: launch result\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    120\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit_for_execution()\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing_ray/runtime/ray/transform_launcher.py:68\u001b[0m, in \u001b[0;36mRayTransformLauncher.__get_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_config\u001b[38;5;241m.\u001b[39madd_input_params(parser\u001b[38;5;241m=\u001b[39mparser)\n\u001b[0;32m---> 68\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_locally \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mrun_locally\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:1869\u001b[0m, in \u001b[0;36mArgumentParser.parse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1869\u001b[0m     args, argv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_known_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1870\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m argv:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:1904\u001b[0m, in \u001b[0;36mArgumentParser.parse_known_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ArgumentError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 1904\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:2630\u001b[0m, in \u001b[0;36mArgumentParser.error\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2629\u001b[0m args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprog\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprog, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m: message}\n\u001b[0;32m-> 2630\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%(prog)s\u001b[39;49;00m\u001b[38;5;124;43m: error: \u001b[39;49m\u001b[38;5;132;43;01m%(message)s\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/argparse.py:2617\u001b[0m, in \u001b[0;36mArgumentParser.exit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_message(message, _sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m-> 2617\u001b[0m \u001b[43m_sys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2145\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2143\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2145\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1454\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1345\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1342\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1192\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1185\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1190\u001b[0m ):\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1082\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1080\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1081\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1082\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1083\u001b[0m )\n\u001b[1;32m   1085\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1086\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/IPython/core/ultratb.py:1150\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1150\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "from repo_level_order_transform import RepoLevelOrderRayTransformConfiguration\n",
    "\n",
    "input_folder = \"../../../transforms/code/repo_level_ordering/ray/test-data/input\"\n",
    "output_folder = \"./output\"\n",
    "\n",
    "import tempfile\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "    # create parameters\n",
    "    local_conf = {\n",
    "        \"input_folder\": input_folder,\n",
    "        \"output_folder\": output_folder,\n",
    "     }\n",
    "\n",
    "    worker_options = {\"num_cpus\": 0.8}\n",
    "    code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "    params = {\n",
    "        # where to run\n",
    "        \"run_locally\": True,\n",
    "        # Data access. Only required parameters are specified\n",
    "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "        # orchestrator\n",
    "        \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "        \"runtime_num_workers\": 2,\n",
    "        \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "        \"runtime_job_id\": \"job_id\",\n",
    "        \"runtime_creation_delay\": 0,\n",
    "        \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "    }\n",
    "\n",
    "\n",
    "    repo_level_params = {\n",
    "        \"repo_lvl_sorting_algo\": \"SORT_SEMANTIC_NORMALISED\",\n",
    "        \"repo_lvl_store_type\": \"local\",\n",
    "        \"repo_lvl_store_backend_dir\": tmpdirname,\n",
    "    }\n",
    "\n",
    "    repo_level_flags = [\n",
    "        \"repo_lvl_output_by_langs\",\n",
    "        \"repo_lvl_combine_rows\",\n",
    "        \"repo_lvl_sorting_enabled\",\n",
    "    ]\n",
    "\n",
    "    d = ParamsUtils.dict_to_req(d=params | repo_level_params)\n",
    "    sys.argv = d + [f\"--{flag}\" for flag in repo_level_flags]\n",
    "    # create launcher\n",
    "    launcher = RayTransformLauncher(RepoLevelOrderRayTransformConfiguration())\n",
    "    # Launch the ray actor(s) to process the input\n",
    "    launcher.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaee7b6-40a5-4422-9dee-e9421138c005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
